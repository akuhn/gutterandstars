\documentclass[10pt]{book}

%%%
\usepackage{xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\etc}{\emph{etc.}\xspace}
\newcommand{\adhoc}{\emph{ad hoc}\xspace}
%%%
\usepackage{amsmath, amsthm, amssymb} 
\usepackage[pdftex,colorlinks=true,pdfstartview=FitV,
 linkcolor=black,citecolor=black,urlcolor=black]{hyperref}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\figureautorefname}{Figure}
%%%
\usepackage[draft]{graphicx}

\newcommand{\moarsauce}{}%\vspace{-0.07in}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Markup macros for proof-reading
\usepackage[normalem]{ulem} % for \sout
\usepackage{xcolor}
\newcommand{\ra}{$\rightarrow$}
\newcommand{\ugh}[1]{\textcolor{red}{\uwave{#1}}} % please rephrase
\newcommand{\ins}[1]{\textcolor{blue}{\uline{#1}}} % please insert
\newcommand{\del}[1]{\textcolor{red}{\sout{#1}}} % please delete
\newcommand{\chg}[2]{\textcolor{red}{\sout{#1}}{\ra}\textcolor{blue}{\uline{#2}}} % please change
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Put edit comments in a really ugly standout display
\usepackage{ifthen}
\usepackage{amssymb}
\newboolean{showcomments}
\setboolean{showcomments}{false} % toggle to show or hide comments
\ifthenelse{\boolean{showcomments}}
  {\newcommand{\note}[2]{
	% \fbox{\bfseries\sffamily\scriptsize#1}
	\fcolorbox{gray}{yellow}{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
    % \marginpar{\fbox{\bfseries\sffamily#1}}
   }
   \newcommand{\version}{\emph{\scriptsize$-$Id$-$}}
  }
  {\newcommand{\note}[2]{}
   \newcommand{\version}{}
  }

\newcommand{\dm}[1]{\note{dominique}{#1}}
\newcommand{\AK}[1]{\note{akuhn}{#1}}
\newcommand{\on}[1]{\note{oscar}{#1}}

\newcommand{\todo}[1]{\note{todo}{#1}}
\newcommand{\rev}[2]{\note{Rev #1}{#2}}
%\newcommand{\done}[1]{\note{done}{#1}}
\newcommand{\done}[1]{}

\def\figureautorefname{Figure}%
\def\tableautorefname{Table}%
\def\partautorefname{Part}%
\def\appendixautorefname{Appendix}%
\def\chapterautorefname{Chapter}%
\def\sectionautorefname{Section}%

%%%
% --> for chapter on Hapax work
\newcommand\concept[1]{#1\xspace}
\newcommand\red{\concept{Red}}
\newcommand\blue{\concept{Blue}}
\newcommand\green{\concept{Green}}
\newcommand\orange{\concept{Orange}}
\newcommand\cyan{\concept{Cyan}}
\newcommand\yellow{\concept{Yellow}}
\newcommand\pink{\concept{Pink}}
\newcommand\magenta{\concept{Magenta}}
\newcommand\black{\concept{Black}}
\newcommand\darkgreen{\concept{DarkGreen}}
\newcommand\navy{\concept{Navy}}
%%%
% --> for chapter on LogLR work
\newcommand{\loglr}{log-likelihood ratio\xspace}
\newcommand{\loglrs}{log-likelihood ratios\xspace}
%%%
% --> for chapter on Chronia
\newcommand{\omap}{\emph{Ownership Map}\xspace}
\newcommand{\id}[1]{{\sf\small{#1}}}
%%%
% --> for chapter on Bug Assignment
\newcommand{\VOC}{vocabulary\xspace}
\newcommand{\VOCs}{vocabularies\xspace}
\newcommand{\diff}{\emph{diff}\xspace}
\newcommand{\diffs}{\emph{diffs}\xspace}
\newcommand{\rlog}{\emph{log}\xspace}
\newcommand{\rlogs}{\emph{logs}\xspace}
\newcommand{\TAM}{term-author-matrix\xspace}
\newcommand{\TAMS}{term-author-matrices\xspace}
\newcommand{\BR}{bug report\xspace}
\newcommand{\BRs}{bug reports\xspace}
\newcommand{\EC}{Eclipse\xspace}
\newcommand{\VCS}{version control system\xspace}
\newcommand{\VCSs}{version control systems\xspace}
\newcommand{\trainingset}{training set\xspace}
\newcommand{\validationset}{validation set\xspace}
\newcommand{\TOOL}{\textsc{Devlect}\xspace}
%%%
% --> for Code Search
\newcommand{\findQ}{\footnote{\underline{Find quote!}} }
\newcommand{\keyw}[1]{\textbf{#1}}
\newcommand{\proj}[1]{#1}
\newcommand{\codeList}[1]{\begin{center}
\texttt{#1}
\end{center}}
\newcommand{\Jbd}{\emph{JBender}\xspace}
\newcommand{\Rbd}{\emph{RBender}\xspace}
\newcommand{\gcs}{\proj{Google Code Search}\xspace}
\newcommand{\krugle}{\proj{Krugle}\xspace}
\newcommand{\koders}{\proj{Koders}\xspace}
\newcommand{\sourcerer}{\proj{Sourcerer}\xspace}
\newcommand{\conjurer}{\proj{Code Conjurer}\xspace}
\newcommand{\merobase}{\proj{Merobase}\xspace}
%%% 
% --> for chapter on user study
\newcommand{\ZoomableUML}{Zoomable UML}
\newcommand{\ConceptMap}{Concept Map}
\newcommand{\FacettedSearch}{Faceted Search}
\newcommand{\RichIntellisense}{Rich Intellisense}
\newcommand{\CloudREPL}{Interactive Snippets} % Dropping the jargon acronym
%%%
% -> for chapter on Codemap Userstudy
\newcommand{\SOCA}{Software Cartography\xspace}
\newcommand{\LSI}{Latent Semantic Indexing\xspace}
\newcommand{\MDS}{Multi\-dimensional Scaling\xspace}
\newcommand{\Codemap}{\textsc{Code\-map}\xspace}
\newcommand{\mds}{multi\-dimensional scaling\xspace}
\newcommand{\eclipse}{Eclipse\xspace}
%%%
% -> for chapter on Codemap itself
\newcommand{\secref}[1]{\autoref{sec:#1}}
\newcommand{\figref}[1]{\autoref{fig:#1}}
\newcommand{\tabref}[1]{\autoref{tab:#1}}
\newcommand{\soca}{software cartography\xspace}
\newcommand{\emSOCA}{\emph{Software Cartography}\xspace}
\newcommand{\lsi}{latent semantic indexing\xspace}
\newcommand{\CODEMAP}{\textsc{CodeMap}\xspace}
%%%%%%%%%%%%%%%%%%%%
 

\begin{document}

\title{Halfway between the Gutter and the Stars}
\author{Adrian Kuhn}
\date{November 2010}
\maketitle

\tableofcontents

\chapter{Introduction}

Other than common believe, software engineers do not spend most time writing code. An approximate 50--80\% of their time is spend on code orientation, ie navigation and understanding \cite{many}. User studies have found that developers use a wide set of cognitive clues for code orientation \cite{many}. Tool support for orientation, however, is typically limited to text file processing or hyperlinks at best. In our research, we explore how to support code orientation by naming, spatial and social clues in development tools.

\begin{itemize}
\item Lexical clues
\item Structural clues
\item Spatial clues
\item Temporal clues
\item Episodic clues
\item Social clues
\end{itemize}

In this work, we present the following tools that explore the use of orientation clues for tool building. Each of these contributions has been published as one or more peer-review paper at an international conference or in an international journal. 

\begin{itemize}
\item Software Clustering \cite{Kuhn07a,Kuhn05a,Lung05a}
\item  Feature Classification \cite{Kuhn06c,Kuhn05b}
\item  Software Summarization \cite{Kuhn09a}
\item  Spatial Representation \cite{Kuhn10c,Kuhn10b,Duca06c,Kuhn08a}
\item  Ownership Map \cite{Girb05a}
\item  Bug-Report Triage \cite{Matt09a}
\item  Credibility in Code Search \cite{Gysi10b}
\end{itemize}

%%%

Acquiring knowledge about a software system is one of the main activities in software reengineering, it is estimated that up to 60 percent of software maintenance is spent on comprehension \cite{Abra04a}. This is because a lot of knowledge about the software system and its associated business domain is not captured in an explicit form. Most approaches that have been developed focus on program structure \cite{Duca05b} or on external documentation \cite{Maar91a,Anto02b}. However, there is another fundamental source of information: the developer knowledge contained in identifier names and source code comments.

{\small\begin{quotation}\emph{The informal linguistic information that the software engineer deals with is not simply supplemental information that can
be ignored because automated tools do not use it. Rather, this information is fundamental. [\ldots] If we are to use this informal information in design recovery tools, we must propose a form for it, suggest how that form relates to the formal information captured in program source code or in formal specifications, and propose a set of operations on these structures that implements the design recovery process} \cite{Bigg89c}.
\end{quotation}}

Languages are a means of communication, and programming languages are no different. Source code contains two levels of communication: human-machine communication through program instructions, and human to human communications through names of identifiers and comments. Let us consider a small code example:

When we strip away all identifiers and comments, from the machine point of view the functionality remains the same, but for a human reader the meaning is obfuscated and almost impossible to figure out. In our example, retaining formal information only yields:

When we keep only the informal information, the purpose of the code is still recognizable. In our example, retaining only the naming yields:

is morning hours minutes seconds is date hours minutes
seconds invalid time value hours 12 minutes 60 seconds 60

%%%%%%%%%%%%

\chapter{Approches for Code Orientation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work, Semantic Clustering}\label{sec:soa}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The use of information retrieval techniques for reverse engineering dates back to the late eighties. Frakes and Nejmeh proposed to apply them on source code as if it would be a natural language text corpus \cite{Frak87a}. They applied an IR system based on keyword matching, which allowed to perform simple searches using wildcards and set expressions.

Antoniol \etal have published a series of papers on recovering code to documentation traceability \cite{Anto00c,Anto02b}. They use information retrieval as well, however with another approach. They rely on external documentation as text corpus, then they query the documentation with identifiers taken from source code to get matching external documents.

Maletic and Marcus were the first to propose using LSI to analyze software systems. In a first work they categorized the source files of the Mosaic web browser and presented in several follow-ups other applications of LSI in software analysis \cite{Male00a}. Their work is a precursor of our work, as they proved that LSI is usable technique to compare software source documents. They apply a minimal-spanning-tree clustering and report on class names and average similarity of selected clusters. We broaden the approach by providing a visual notation that gives an overview of all the clusters and their relationships, and we provide the automatic labeling that takes the entire vocabulary into account and not only the class names.

LSI was also used in other related areas: Marcus and Maletic used LSI to detect high-level conceptual clones, that is they go beyond just string based clone detection using the LSI capability to spot similar terms \cite{Marc01a}. They
select a known implementation of an abstract datatype, and manually investigate all similar source documents to find high-level concept clones. The same authors also used LSI to recover links between external documentation and source code by querying the source code with queries from documentation \cite{Marc03b}.

Kawaguchi \etal used LSI to categorize software systems in open-source software repositories \cite{Kawa04a}. Their approach uses the same techniques as ours, but with a different set up and other objectives. They present a tool that categorizes software projects in a source repository farm, that is they use entire software systems as the documents of their LSI space. They use clustering to provide an overlapping categorizations of software, whereas we use clustering to partition the software into distinct topics. They use a visualization of they results with the objective to navigate among categorizations and projects, similar to the Softwarenaut tool \cite{Lung06a}, whereas we use visualizations to present an overview, including all documents and the complete partition, at one glance.

Marcus \etal employed LSI to detect concepts in the code \cite{Marc04a}. They used the LSI as a search engine and searched in the code the concepts formulated as queries. Their work is about concept location of externally defined concepts, whereas we derive our concepts from the vocabulary usage on the source-code level. Their  article also gives a good overview of the related work. Marcus \etal also use LSI to compute the cohesion of a class based on the semantic similarity of its methods \cite{Marc05a}. In our work, we extend this approach and illustrate on the correlation matrix both, the semantic similarity within a cluster and the semantic similarity between clusters.

De Lucia \etal introduce strategies to improve LSI-based traceability detection \cite{Luci04a}. They use three techniques of link classification: taking the top-\emph{n} search results, using a fix threshold or a variable threshold. Furthermore they create separate LSI spaces for different document categories and observe better results that way, with best results on pure natural language spaces. Lormans and Deursen present two additional links classification strategies \cite{Lorm06a}, and discuss open research questions in traceability link recovery.

Di Lucca \etal also focus on external documentation, doing automatic assignment of maintenance requests to teams \cite{Lucc02b}. They compare approaches based on pattern matching and clustering to information retrieval techniques, of which clustering performs better.

Huffman-Hayes \etal compare the results of several information retrieval techniques in recovering links between document and source code to the results of a senior engineer \cite{Huff06a}. The results suggest that automatic recovery performs better than human analysis, both in terms of precision and recall and with comparable signal-to-noise ratio. In accordance with these findings, we automate the ``Read all the Code in One Hour'' pattern using information retrieval techniques.

\v{C}ubrani\'{c} \etal build a searchable database with artifacts related to a software system, both source code and external documentation \cite{Cubr03a}. They use a structured meta model, which relates bug reports, news messages, external documentation and source files to each other. Their goal is the support software engineers, especially those new to a project, with a searchable database of what they call ``group memory''. To search the database they use information retrieval, however they do not apply LSI and use a plain vector space model only. They implemented their approach in an eclipse plug-in called Hipikat.

\section{Related work for Software Summarization}\label{relwork}

The present work is related to Jonathan Feinberg's comparison of inaugural addresses \cite{Feinberg09blog}. Feinberg analysed the inaugural address of Mr. President Barack Obama and his predecessors in office. For each inaugural addresses he provides a pair of \textsc{Wordle}\footnote{\url{http://www.wordle.net}} word clouds. One cloud consists of words that are specific to the address, and the other cloud consists of words that are missing in the address. Font size is used to represent frequency of a word and saturation to represent the log-likelihood ratio. The color blue is used in the left cloud to represent likely terms, and red is used in the right cloud to represent unlikely terms.

Anslow \etal \cite{Anslow08OOPSLA} visualized the evolution of words in class names in Java version 1.1 and Java version 6.0. They illustrated the history in a combined word cloud that contains terms from both versions. Each word is printed twice, font size represents word frequency and color the corpus. As such they compared word counts, which assumes normal distribution and is thus not as sound as using log-likelihood ratios.
 
Linstead \etal \cite{Linstead09SUITE} analysed the vocabulary of 12,151 open source projects from Sourceforge and Apache. They provide strong evidence of power-law behavior for word distribution across program entities. In addition, they analyse the vocabulary of structural entities (class, interface, method, field) and report the top-10 most frequent terms, as well as the top-10 unique terms for each structural category. As such, they compare word counts which assumes normal distribution and is thus not as sound as using log-likelihood ratios. In \autoref{example5} we report on the likelihood of words for the same structural categories, although our analysis is limited to a smaller corpus, \ie the Java API only.

Kawaguchi \etal \cite{Kawa04a} presented \textsc{MUDABlue}, a tool that provides labels for projects. They used Sourceforge as normative corpus and applied Latent Semantic Indexing (LSI) at the level of projects.
For each project they grouped together all source files into one retrieval document, and applied LSI to retrieve the semantic similarity among both terms and document.
They categorized projects and described the categories with the most similar terms. However, the probabilistic model of LSI does not match observed data: LSI assumes that words and documents form a joint Gaussian model, while a Poisson distribution has been observed \cite{Hofmann99PLSA}.

Lexical information of source code has been further proven useful for various tasks in software engineering (\eg \cite{Anto02a,Marc05a,Posh09a}). Many of these approaches apply Latent Semantic Indexing and inverse-document frequency weighting, which are well-accpeted techniques in Information Retrieval but are according to Dunning ``only justified on very sketchy grounds \cite{Dunning}.''

Baldi \etal \cite{Bald08a} present a theory of aspects (the programming language feature) as latent topics. They apply Latent Dirichlet Analysis (LDA) to detect topic distributions that are possible candidates for aspect-oriented programming. They present the retrieved topics as a list of the 5 most likely words. The model of LDA assumes that each topic is associated with a multinomial distribution over words, and each document is associated with a multinomial distribution over topics; their approach is thus sound.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work for Chronia}\label{sec:soa}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Analyzing the way developers interact with the system has only attracted few research. A visualization similar to the \omap is used to visualize how authors change a wiki page \cite{Vieg04a}.

Xiaomin Wu \etal visualize \cite{Wu04b} the change log information to provide an overview of the active places in the system as well as of the authors activity. They display measurements like the number of times an author changed a file, or the date of the last commitment.

Measurements and visualization have long been used to analyze how software systems evolve.

Ball and Eick \cite{Ball96a} developed multiple visualizations for showing changes that appear in the source code. For example, they show what is the percentage of bug fixes and feature addition in files, or which lines were changed recently.

Eick \etal proposed multiple visualizations to show changes using colors and third dimension \cite{Eick02a}.

Chuah and Eick proposed a three visualizations for comparing and correlating different evolution information like the number of lines added, the errors recorded between versions, number of people working etc. \cite{Chua98a}.

Rysselberghe and Demeyer use a scatter plot visualization of the changes  to provide an overview of the evolution of systems and to detect patterns of change\cite{Ryss04a}.

Jingwei Wu \etal use the spectrograph metaphor to visualize how changes occur in software systems \cite{Wu04a}. They used colors to denote the age of changes on different parts of the systems.

Jazayeri analyzes the stability of the architecture \cite{Jaza02a} by using colors to depict the changes. From the visualization he concluded that old parts tend to stabilize over time.

Lanza and Ducasse visualize the evolution of classes in the Evolution Matrix \cite{Lanz02a}. Each class version is represented using a rectangle. The size of the rectangle is given by different measurements applied on the class version. From the visualization different evolution patterns can be detected such as continuous growth, growing and shrinking phases etc.

Another relevant reverse engineering domain is the analysis of the co-change history.

Gall \etal aimed to detect logical coupling between parts of the system \cite{Gall98a} by identifying the parts of the system which change together. They used this information to define a coupling measurement based on the fact that the more times two modules were changed at the same time, the more they were coupled.

Zimmerman \etal aimed to provide mechanism to warn developers about the correlation of changes between functions. The authors placed their analysis at the level of entities in the meta-model (\eg methods) \cite{Zimm04a}. The same authors defined a measurement of coupling based on co-changes \cite{Zimm03a}.

Hassan \etal analyzed the  types of data that are good predictors of change propagation, and came to the conclusion that historical co-change is a better mechanism than structural dependencies like call-graph \cite{Hass04a}.

\section{Related Work for Bugreport Assignment}\label{sec:relatedwork}

There was previous work done on mining developer expertise from source code. However, if such an expertise model was applied to assist in the triage of bug reports, it required an explicit linkage between bug reports and source files~\cite{Anvik07}. Most existing recommendation systems for the assignment of developers to bug reports use other expertise models, as \eg models based on previous reports. Thus, the main differences between our approach and other recommendation systems are: First, our system is trained on the source code repository rather than on previous bug reports. Second, for the recommendation of a developer suitable to handle a bug report, we do not need a linkage between the bug tracking system and the software repository. As a collorary, our approach is a) not dependent on the quality of previous bug reports, and b) can also be used to assign developers that have not worked on any bug report previously. The main requirement for a developer to be assignable is that he has a contribution history of approx. half a year or more.

Mockus and Herbsleb \cite{Mock02b} compute the experience of a developer as a function of the number of changes he has made to a software system so far. Additionally, they compute recent experience by weighting recent changes more than older ones. The experience is then used to model the expertise of a developer. Furthermore, they examine the time that a developer needs to find additional people to work on a given modification request. Based on the results, they report that finding experts is a difficult task.

Fritz \etal \cite{Frit07a} report on an empirical study that investigates whether a programmer's activity indicates knowledge of code. They found that the frequency and recency of interaction  indicates the parts of the code for which the developer is an expert. They also report on a number of indicators that may improve the expertise model, such as authorship, role of elements, and the task being performed. In our work, we use the vocabulary of frequently and recently changed code to build an expertise model of developers. By using the vocabulary of software changes, lexical information about the role of elements and the kind of tasks are included in our expertise model.

Other approaches working with the \VOC of \BRs and/or source code include: Estimating the person-hour spent on fixing a bug \cite{Weis07a}. \todo{more}

Siy \etal \cite{Siy08a} present a way to summarize developer work history in terms of the files they have modified over time by segmenting the CVS change data of individual Eclipse developers. They show that the files modified by developers tend to change significantly over time. Although, most of the developers tend to work within the same directories. This accords with our observation regarding the decay of vocabulary.

Gousios \etal \cite{Gous08a} present an approach for evaluating developer contributions to the software development process based on data acquired from software repositories and collaboration infrastructures. However, their expertise model does not include the vocabulary of software changes and is thus not queryable using the content of bug reports.

Alonso \etal \cite{Alon08a} describe an approach using classification of the file paths of contributed source code files to derive the expertise of developers. 

Schuler and Zimmerman \cite{Schu08a} introduce the concept of usage expertise, which manifests itself whenever developers are using functionality, \eg by calling API methods. They present preliminary
results for the Eclipse project indicating that usage expertise is a promising complement to implementation expertise (such as our expertise model). Given these results, we consider as future work to extend our expertise model with usage expertise as well.

A valuable source of developer expertise are mailing lists. It has been shown that the frequency with which software entities (functions, methods, classes, etc) are mentioned in the mail correlated with the number of times these entities are included in changes to the software \cite{Patt08a}. It has been shown that approx.\ 70\% of the vocabulary used in source code changes is found in mailing lists as well~ \cite{Bays07a}, we can thus estimate the impact of including mailinglists on our results.  

Given these results, we consider as future work to including vocabulary from mailing lists in our expertise model. We expect that this will considerable improve our results, since Information Retrieval approaches are known to perform the better the more data is available \cite{TheTextminigHandbookOrOther}.

 While research in the mining of software repositories has frequently ignored commits that include a large number of files; 
Hindle \etal \cite{Hind08b} perform a case study that includes the manual classification of large commits. They show that large commits tend to be perfective while small commits are more likely to be corrective. Commits are not normalized in our expertise model, thus the size of a commit may affect our model. However, since large commits are rare and small commits are common, we can expect our expertise model to equally take perfective and corrective contributions into account.
However, it would be interesting for future work to introduce weightings for commit sizes. 

Hill \etal \cite{Hill08a} present an automatic mining technique to expand abbreviations in source code.  Automatically generated abbreviation expansions can be used to enhance software maintenance tools that utilize natural language information, such as our approach. If the same abbreviations are used in both souce code and bug reports then our approach is not affected by this issue, nevertheless we plan to include  automatic abbreviation expansion as future work.

Currently our expertise model is limited to the granularity of commited software changes, a more fine grained acquisition of the model could be achieved by using a change-aware development environment \cite{Omor08a,Robb08a} that records developer vocabulary as the software is written.  

Bettenburg \etal \cite{Bett08c} present an approach to split bug reports into natural text parts and structured parts, \ie source code fragments or stack traces. Our approach treats both the same, since counting word frequencies is applicable for natural-language text as well as source code in the same way.

Recommending experts to assign developers to bug reports is a common application of developer expertise models.
Anvik \etal \cite{Anvi06a} build developers' expertise from previous \BRs and try to assign current reports based on this expertise. They label the reports. If a report cannot be labeled, it is not considered for training. Additionally, reports involving developers with a too low bug fixing frequency or involving developers not working on the project anymore are filtered. They then assign \BRs from a period of lower than half a year. To find possible experts for their recall calculation, they look for the developers who fixed the bug in the source code (by looking for the corresponding bug ID in the change comments). They reach precision levels of 57\% and 64\% on the Eclipse and Firefox development projects respectively. However, they only achieve around 6\% precision on the Gnu C Compiler project. The highest recall they achieve is on average 10\% (Eclipse), 3\% (Firefox) and 8\% (gcc). Please note that the recall results are not directly comparable to ours, since they use different configurations of bug-related persons to compute recall.

Cubranic and Murphy \cite{Cubr04b} propose to use machine learning techniques to assist in bug triage. Their prototype uses supervised Bayesian learning to train a classifier with the textual content of resolved bug reports. This is then used to classify newly incoming bug reports. They can correctly predict 30\% of the report assignments, considering \EC as a case study.

Canfora and Cerulo \cite{Canf05a} propose an Information Retrieval technique to assign developers to bug reports and to predict the files impacted by the bug's fix. They use the lexical content of \BRs to index source files as well as developers. They do not use vocabulary found in source files, rather they assign to source files the vocabulary of related bug reports. The same is done for developers. For the assignments of developers, they achieve 30\%--50\% top-1 recall for KDE and 10\% to 20\% top-1 recall for the Mozilla case study.

G\^irba, Kuhn \etal\cite{Girb05c} assume that a developer changing a line knows about the line.

Similar to our work, Di Lucca \etal use Information Retrieval approaches to classify maintenance requests \cite{Lucc02b}. They train a classifier on previously assigned bug reports, which is then used to classify incoming bug reports. They evaluate different classifiers, one of them being a term-documents matrix using cosine similarity. However, this matrix is used to model the vocabulary of previous bug reports and not the vocabulary of developers.

Anvik and Murphy evaluate approaches that mine implementation expertise from a software repository or from \BRs~\cite{Anvik07}. Both approaches are used to recommend experts for a \BR. For the approach that gathers information from the software repository, a linkage between similar reports and source code elements is required. For the approach that mines the reports itself, amongst others, the commenters of the reports (if they are developers) are estimated as possible experts. Both approaches disregard inactive developers. Both recommendation sets are then compared to human generated expert sets for the \BR.
%\todo{write more, write results, compare to our results}

Minto and Murphy's Emergent Expertise Locator (EEL)~\cite{Minto07} recommends a ranked list of experts for a set of files of interest. The expertise is calculated based on how many times which files have been changed together and how many times which author has changed what file. They validate their approach by comparing recommended experts for files changed for a bug fix with the developers commenting on the bug report, assuming that they ``either have expertise in this area or gain expertise through the discussion"~\cite{Minto07}.
%\todo{I guess the next cite is not really related...}\cite{Sand04xMSR}.

%\todo{Explain why we didn't use ten-fold cross validation which is normally commonly used (see also \cite{Anvi06a}) }

We are not the first to apply an author-topic model \cite{Linstead07, Bald08a}. Linstead \etal showed promising results in applying author-topic models on Eclipse and Baldi \etal applied topic models to mine aspects.
%They extract the words contained in the source code files to generate a word-document matrix. An additional author-document matrix is generated from a bug database where information about developers changing specific source files can be found. 

Podgurski \etal classified failure reports~\cite{Podgurski03}. They cluster profiles of failed executions in order to classify new failures and to diagnose common defect sources.

Lexical information of source code has previously been proven useful for other tasks in software engineering, such as: identifying high-level conceptual clones \cite{Marc01a}, recovering traceability links between external documentation and source code \cite{Anto02a}, automatically categorizing software projects in open-source repositories \cite{Kawa04a}, visualizing conceptual correlations among software artifacts \cite{Kuhn07a,Kuhn08b}, and defining cohesion and coupling metrics \cite{Marc05a,Posh09a}.

% =====================================================================
\section{Background \& Related Work of Code Search}
\label{sec:relwork}

Since the rise of internet-scale code search engines, searching for reusable source code has quickly become a fundamental activity for developers \cite{Kuhn09b}. However, in order to establish search-driven software reuse as a best practice, the cost and time of deciding whether to integrate a search result must be minimized. The decision whether to reuse a search result or not should be quickly taken without the need for careful (and thus time-consuming) examination of the search results.

Trustability is a big issue for reusing code. When a developer reuses code from an external sources he has to trust the work of external developers that are unknown to him. This is not to be confused with trustworthy computing, where clients are concerned with security and reliability of a computation service.

For a result to actually be helpful and serve the purpose originally pursued with the search it is not enough to just match the entered keywords.
It is essential that the developer know at least the license under which certain source code was published, otherwise he will not be able to use it legally. Furthermore, it is very helpful to know from which project a search result is taken when assessing its quality.
User studies have shown that developers rely on both technical and human clues to assess the trustability of search results \cite{Gall09a}. For example developers will prefer results from well-known open source projects over results rom less popular projects.

The issue of providing meta-information alongside search results and thereby increasing trustabilty has not been widely studied and we are trying to address this with our work.

In recent years special search engines for source code have appeared, namely  \textsc{\gcs}\footnote{\url{http://www.google.com/codesearch}}, \textsc{\krugle}\footnote{\url{http://www.krugle.org}} and  \textsc{\koders}\footnote{\url{http://www.koders.com}}. They all focus on full-text search over a huge code base, but lack detailed information about the project. Search results typically provide a path to the version control repository and little meta-information on the actual open source project; often, even such basic information as the name and homepage of the project are missing.

\textsc{\sourcerer}\footnote{\url{http://sourcerer.ics.uci.edu}} by Bajracharya \etal \cite{Bajr06a} and \textsc{\merobase}\footnote{\url{http://www.merobase.org}} by Hummel \etal \cite{Humm08a} are research projects with an internet-scale code search-engine. Both provide the developer with license information and project name. Merobase also provides a set of metrics such as cyclomatic- and Halstead complexity.  An improved version of \sourcerer with trustability data is in development, though it has not yet been published\footnote{Personal communication with Sushil Bajracharya.}. 

In addition to the web user interface, both Sourcerer and Merobase are also accessible through Eclipse plug-ins that allow the developer to write unit tests. These are then used as a special form of query to search for matching classes/ methods, \ie classes that pass the tests\cite{Humm08a}. Using unit tests as form of formulating queries is a way of increase technical trustability: Unit-tested search results are of course more trustable, however at the cost of a more time consuming query formulation (\ie additionally writing the unit tests). The kind of results returned are also limited to clearly-defined and testable features.  A combination of technical trustability factors (\eg unit tests) and human trustability factors might be promising future work.

In addition to the web user interface, both Sourcerer and Merobase are also accessible through Eclipse plug-ins that allow the developer to write unit tests that are then used to query the search engines for matching classes \cite{Lemo07a,Humm08a}. Using unit tests for query formulation is interesting with regard to trustability. Unit-tested search results are of course more trustable, however at the cost of a more time consuming query formulation (\ie additionally writing the unit tests). \on{This is still confusing. It is not clear what writing unit tests have to do with query formulation.} The kind of results returned are also limited to clearly-defined and testable features.  A combination of technical trustability factors (\eg unit tests) and human trustability factors might be promising future work.

We are not the first to use collaborative filtering in code search. Ichii \etal used collaborative filtering to recommend relevant components to users \cite{Ichi09a}. Their system uses browsing history to recommend components to the user. The aim was to help users make cost-benefit decisions about whether or not those components are worth integrating. Our contribution beyond the state-of-the-art is our focus on human factors and the role of cross-project contributors.

The \Jbd prototype, that we present in \autoref{sec:approach}, uses data collected from the Ohloh website to compute the trustability metric that we propose in \autoref{sec:metric}.

% ===================================================================================
\section{RELATED WORK}
\label{sec:related}

Using \mds to visualize information based on the metaphor of cartographic maps is by no means a novel idea. \emph{Topic maps}, as they are called, have a longstanding tradition in information visualization \cite{Ware04a}. The work in this paper was originally inspired by Michael Hermann's and Heiri Leuthold's work on the political landscapes of Switzerland \cite{Herm03a}. 

In the same way, stable layouts have a long history in information visualization, as a starting point see \eg the recent work by Frishman and Tal on online dynamic graph drawing \cite{Fris08a}. They present an online graph drawing approach, which is similar to the online pipeline presented in this work. Please refer to \autoref{sec:other} for a comparison of graph drawing and \MDS. 

ThemeScape is the best-known example of a text visualization tool that uses the metaphor of cartographic maps. 
Topics extracted from documents are organized into a visualization where visual distance correlates to topical distance and surface height corresponds to topical frequency \cite{Wise95b}. The visualization is part of a larger toolset that uses a variety of algorithms to cluster terms in documents. For laying out small document sets MDS is used; for larger document sets a proprietary algorithm, called ``Anchored Least Stress'', is used. The digital elevation model is constructed by successively layering the contributions of the contributing topical terms, similar to our approach.

In the software visualization literature however, topic maps are rarely used.
Except for the use of graph splatting in RE Toolkit by Telea \etal \cite{Tele03a}, we are unaware of their prior application in software visualization. And even in the case of the RE toolkit, the maps are not used to produce consistent layouts for thematic maps, or to visualize the evolution of a software system. 


\subsection{Desiderata for Spatial Representation of Software}

Robert DeLine's work on software navigation \cite{Deli05b,Deli06a} closely relates to \SOCA. His work is based on the observation that developers are consistently lost in code \cite{Deli05a} and that using textual landmarks only places a large burden on cognitive memory. He concludes the need for new visualization techniques that allow developers to use their spatial memory while navigating source code.

DeLine proposes four desiderata \cite{Deli05b} that should be satisfied by spatial software navigation: 1)~the display should show the entire program and be continuous, 2)~the display should contain visual landmarks such that developers can find parts of the program perceptually rather than relying on names, 3)~the display should remain visually stable during navigation [and evolution], and 4)~the display should be capable of showing global program information overlays other than navigation.

An ad-hoc algorithm that satisfies the first and fourth properties is presented in the same work. As distance metric between software entities (here, methods) an arbitrary chosen score is used.

Our work satisfies all above desiderata, and completes them with a fifth desideratum that visual distance should have a meaningful interpretation. The scope of \SOCA is broader than just navigation, it is also intended for reverse engineering and code comprehension in general. We can thus generalize the five desiderata for spatial representation of software as follows:

\begin{enumerate}
\item The visualization should show the entire program and be continuous.
\item The visualization should contain visualization landmarks that allow the developers to find parts of the system perceptually, rather than relying on name or other cognitive causes. 
\item The visualization should remain visually stable as the system evolves. 
\item The visualization should should be capable of showing global information overlays.
\item On the visualization, distance should have a meaningful interpretation. 
\end{enumerate}

\subsection{Other Layout Approaches used in Software Visualization}

Most software visualization layouts are based on one or multiple of the following approaches: UML diagrams, force-based graph drawing, tree-map layouts, and polymetric views.

% -----------------------------------------------------------------------------------
\emph{UML diagrams} generally employ no particular layout and do not continuously use the visualization pane. The UML standard itself does not cover the layout of diagrams. Typically a UML tool will apply an unstable graph drawing layout (\eg based on visual optimization such a reducing the number of edge crossings) when asked to automatically layout a diagram. However, this does not imply that the layout of UML diagrams is meaningless. UML diagrams are carefully created by architects, at least those made during the design process, so their layout do have a lot of meaning. If you change such a diagram and re-show it to its owner, the owner will almost suddenly complain, since he invested time in drawing the diagram a certain way! Alas, this layout process requires manual effort.

Gudenberg \etal have proposed an evolutionary approach to layout UML diagrams in which a fitness function is used to optimize various metrics (such as number of edge crossings) \cite{Gude06a}. Although the resulting layout does not reflect a distance metric, in principle the technique could be adapted to do so. Andriyevksa \etal have conducted user studies to assess the effect that different UML layout schemes have on software comprehension \cite{Andr05a}.
They report that the layout scheme that groups architecturally related classes together yields best results. They conclude that it is more important that a layout scheme convey % NB: "convey", not "conveys" (subjunctive)
a meaningful grouping of entities, rather than being aesthetically appealing. Byelas and Telea highlight related elements in a UML
% Google says 51,700 x an UML and 315,000 a UML %
diagram using a custom ``area of interest'' algorithm that connects all related elements with a blob of the same color, taking special care to minimize the number of crossings \cite{Byel06a}.
The impact of layout on their approach is not discussed.

% -----------------------------------------------------------------------------------
\emph{Graph drawing} refers to a number of techniques to layout two- and three-dimensional graphs for the purpose of information visualization \cite{Ware04a,Kauf01b}. Noack \etal offer a good starting point for applying graph drawing to software visualization \cite{Noac05a}. Jucknath-John \etal present a technique to achieve stable graph layouts over the evolution of the displayed software system \cite{Juck06a}, thus achieving consistent layout, while sidestepping the issue of reflecting meaningful position or distance metrics.

Unlike MDS, graph drawing does not map an $n$-dimensional space to two dimensions, rather it is concerned with the placement of vertices and edges such that visual properties of the output are optimized. For example, algorithms minimize the number of edge crossings or try to avoid that nodes overlap each other. Even though, the standard force-based layouts can consider edge weights (which can be seen as a distance metric), edges with the same weight may have different length on the visualization pane depending on the connectedness of the graph at that position. Furthermore, the resulting placement is not continuos. The void between vertices is not continuous spectrum of metric locations, as is the case with an MDS layout.

\emph{Graph splatting} is a variation of graph drawing, which produced visualizations that are very similar to thematic maps \cite{Lier03a}. Graph splatting represents the layout of graph drawing algorithms as a continuous scalar field. Graph splatting combines the layout of graph drawing with the rendering of thematic maps. Each vertex contributes to the field with a Gaussian shaped basis function. The elevation of the field thus represents the density of the graph layout at that position. Telea \etal apply Graph splatting in their RE toolkit to visualize software systems \cite{Tele03a}. However, they are not concerned with stable layouts. Each run of their tool may yield a different layout.

% -----------------------------------------------------------------------------------
\emph{Treemaps} represent tree-structured information using nested rectangles \cite{Ware04a}.
Though treemaps make continuous use of the visualization pane, the interpretation of position and distance is implementation dependent. Classical treemap implementations are known to produce very narrow and thus distorted rectangles. Balzer \etal proposed a modification of the classical treemap layout using Voronoi tessellation \cite{Balz05a}. Their approach creates aesthetically more appealing treemaps, reducing the number of narrow tessels. There are some treemap variations (\eg the strip layout or the squarified layout) that can, and do, order the nodes depending on a metric. However, nodes are typically ordered on a local level only, not taking into account the global co-location of bordering leaf nodes contained in nodes that touch at a higher level. Many treemaps found in software visualization literature are even applied with arbitrary order of nodes, such as alphanumeric order of class names. 

\emph{Polymetric views} visualize software systems by mapping different software metrics on the visual properties of box-and-arrow diagrams \cite{Lanz03d,Lanz06a}. Many polymetric views are ordered by the 
value of a given software metric, so that relevant items appear first (whatever first means, given the 
layout). Such an order is more meaningful then alphabetic (or worse, hash-key ordering), but on the other hand only as stable as the used metric. The System Complexity view is by far the most popular polymetric view, and is often used as a base layout where our requirements for stability and consistence apply (see \eg \cite{Gree06a}). The layout of System Complexity uses graph drawing on inheritance relations, and orders the top-level classes as well as each layer of subclasses by class names. Such a layout does not meet our desiderate for a stable and consistent layout.
   
% -----------------------------------------------------------------------------------
\subsection{ More Cartography Metaphors in Software Visualization}\label{sec:other}

A number of tools have adopted metaphors from cartography in recent years to visualize software.
Usually these approaches are integrated in a tool with in an interactive, explorative interface and often feature three-dimensional visualizations. None of these approaches satisfies DeLine's desiderata.

MetricView is an exploratory environment featuring UML diagram visualizations \cite{Term05a}. The third dimension is used to extend UML with polymetric views \cite{Lanz03d}.
The diagrams use arbitrary layout, so do not reflect meaningful distance or position.

White Coats is an explorative environment also based on the notion of polymetric views \cite{Mesn05b}. The visualizations are three-dimensional with position and visual-distance of entities given by selected metrics. However they do not incorporate the notion of a consistent layout.

CGA Call Graph Analyser is an explorative environment that visualizes a combination of function call graph and nested modules structure \cite{Bohn07a}. The tool employs a 2$\frac{1}{2}$-dimensional approach. To our best knowledge, their visualizations use an arbitrary layout.

CodeCity is an explorative environment building on the city metaphor \cite{Wett07b}. CodeCity employs the nesting level of packages for their city's elevation model, and uses a modified tree layout to position the entities, \ie packages and classes. Within a package, elements are ordered by size of the element's visual representation. Hence, when changing the metrics mapped on width and height, the overall layout of the city changes, and thus, the consistent layout breaks.

VERSO is an explorative environment that is also based on the city metaphor \cite{Lang05a}. Similar to CodeCity, VERSO employs a treemap layout to position their elements. Within a package, elements are either ordered by their color or by first appearance in the system's history. As the leaf elements have all the same base size, changing this setting does not change the overall layout. Hence, they provide consistent layout, however within the spatial limitations of the classical treemap layout. 

% Related work above
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%

\chapter{Clustering Software by Lexical Clues}

Many of the existing approaches in Software Comprehension focus on program program structure or external documentation. However, by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked. To understand software as a whole, we need to enrich software analysis with the developer knowledge hidden in the code naming. This paper proposes the use of information retrieval to exploit linguistic information found in source code, such as identifier names and comments. We introduce \emph{Semantic Clustering}, a technique based on Latent Semantic Indexing and clustering to group source artifacts that use similar vocabulary. We call these groups \emph{semantic clusters} and we interpret them as \emph{linguistic topics} that reveal the intention of the code. We compare the topics to each other, identify links between them, provide automatically retrieved labels, and use a visualization to illustrate how they are distributed over the system. Our approach is language independent as it works at the level of identifier names. To validate our approach we applied it on several case studies, two of which we present in this paper.

In this paper, we use information retrieval techniques to \emph{derive topics from the vocabulary usage at the source code level}. Apart from external documentation, the location and use of source-code identifiers is the most frequently consulted source of information in software maintenance \cite{Kosk04a}. The objective of our work is to analyze software without taking into account any external documentation. In particular we aim at:

\begin{itemize}
  \item \textbf{Providing a first impression of an unfamiliar software system}. A common pattern when encountering an unknown or not well known software for the first time is ``Read all the Code in One Hour'' \cite{Deme02a}. Our objective is to support this task, and to provide a map with a survey of the system's most important topics and their location.
  \item \textbf{Revealing the developer knowledge hidden in identifiers.} In practice, it is not external documentation, but identifer names and comments where developers put their knowledge about a system. Thus, our objective is not to locate externally defined domain concepts, but rather to derive topics from the actual use of vocabulary in source code.
  \item \textbf{Enriching Software Analysis with informal information.} When analyzing formal information (\eg structure and behavior) we get only half of the picture: a crucial source of information is missing, namely, the semantics contained in the vocabulary of source code. Our objective is to reveal components or aspects when, for example, planning a large-scale refactoring. Therefore, we analyze how the code naming compares to the code structure: What is the distribution of linguistic topics over a system's modularization? Are the topics well-encapsulated by the modules or do they cross-cut the structure?
\end{itemize}


Our approach is based on Latent Semantic Indexing (LSI), an information retrieval technique that locates linguistic topics in a set of documents \cite{Deer90a,Marc04a}. We apply LSI to compute the linguistic similarity between source artifacts (\eg packages, classes or methods) and cluster them according to their similarity. This clustering partitions the system into linguistic topics that represent groups of documents using similar vocabulary. To identify how the clusters are related to each other, we use a correlation matrix \cite{Ling73a}. We employ LSI again to automatically label the clusters with their most relevant terms. And finally, to complete the picture, we use a map visualization to analyze the distribution of the concepts over the system's structure.

We implemented this approach in a tool called Hapax\footnote{The name is derived from the term \emph{hapax legomenon}, that refers to a word occurring only once a given body of text.}, which is built on top of the Moose reengineering environment \cite{Duca05a,Nier05c}, and we apply the tool on several case studies, two of which are presented in this work: JEdit\footnote{http://www.jedit.org/} and JBoss\footnote{http://www.JBoss.org/}.

This paper is based on our previous work, in which we first proposed semantic clustering \cite{Kuhn05a}. The main contributions of the current paper are:
\begin{itemize}

\item \emph{Topic distribution analysis.} In our previous work we introduced semantic clustering to detect linguistic topics given by parts of the system that use similar vocabulary. We complement the approach with the analysis of how topics are distributed over the system using a Distribution Map \cite{Duca06c}.

\item \emph{Improvement of the labeling algorithm.} One important feature of semantic clustering is the automatic labeling -- \ie given a cluster we retrieve the most relevant labels for it. We propose an improved algorithm that takes also the similarity to the whole system into account.

\item \emph{Case studies.} In our previous work, we showed the results of the clustering and labeling on different levels of abstraction on three case studies. In this paper we report on other two case studies.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Latent Semantic Indexing}\label{sec:LSI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As with most information retrieval techniques, Latent Semantic Indexing (LSI) is based on the vector space model approach. This approach models documents as bag-of-words and arranges them in a term-document matrix $A$, such that $a_{i,j}$ equals the number of times term $t_i$ occurs in document $d_j$.

LSI has been developed to overcome problems with synonymy and polysemy that occurred in prior vectorial approaches, and thus improves the basic vector space model by replacing the original term-document matrix with an approximation. This is done using singular value decomposition (SVD), a principal components analysis (PCA) technique originally used in signal processing to reduce noise while preserving the original signal. Assuming that the original term-document matrix is noisy (the aforementioned synonymy and polysemy), the approximation is interpreted as a noise reduced -- and thus better -- model of the text corpus.

As an example, a typical search engine covers a text corpus with millions of web pages, containing some ten thousands of terms, which is reduced to a vector space with 200-500 dimensions only. In Software Analysis, the number of documents is much smaller and we typically reduce the text corpus to 20-50 dimensions.

Even though search engines are the most common uses of LSI \cite{Berr94a}, there is a wide range of applications, such as automatic essay grading \cite{Folt99a}, automatic assignment of reviewers to submitted conference papers \cite{Duma92a},  cross-language search engines, thesauri, spell checkers and many more.
In the field of software engineering LSI has been successfully applied to categorized source files \cite{Male00a} and open-source projects \cite{Kawa04a}, detect high-level conceptual clones \cite{Marc01a}, recover links between external documentation and source code \cite{Luci04a,Marc05a} and to compute the class cohesion \cite{Marc05a}. Furthermore LSI has proved useful in psychology to simulate language understanding of the human brain, including processes such as the language acquisition of children and other high-level comprehension phenomena \cite{Land97a}.

\autoref{fig:lsi} schematically represents the LSI process. The document collection is modeled as a vector space. Each document is represented by the vector of its term occurrences, where terms are words appearing in the document. The term-document-matrix $A$ is a sparse matrix and represents the document vectors on the rows. This matrix is of size $n \times m$, where $m$ is the number of documents and $n$ the total number of terms over all documents. Each entry $a_{i,j}$ is the frequency of term $t_i$ in document $d_j$. A geometric interpretation of the term-document-matrix is as a set of document vectors occupying a vector space spanned by the terms. The similarity between documents is typically defined as the cosine or inner product between the corresponding vectors. Two documents are considered similar if their corresponding vectors point in the same direction.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=.8\columnwidth]{lsi}
\caption{LSI takes as input a set of documents and the terms occurrences, and returns as output a vector space containing all the terms and all the documents. The similarity between two items (\ie terms or documents) is given by the angle between their corresponding vectors.}
\label{fig:lsi}
\end{center}
\end{figure}

LSI starts with a raw term-document-matrix, weighted by a weighting function to balance out very rare and very common terms. SVD is used to break down the vector space model into less dimensions. This algorithm preserves as much information as possible about the relative distances between the document vectors, while collapsing them into a much smaller set of dimensions.

SVD decomposes matrix $A$ into its singular values and its singular vectors, and yields -- when truncated at the $k$ largest singular values -- an approximation $A'$ of $A$ with rank $k$. Furthermore, not only the low-rank term-document matrix $A'$ can be computed but also a term-term matrix and a document-document matrix. Thus, LSI allows us to compute term-document, term-term and document-document similarities.

As the rank is the number of linear-independent rows and columns of a matrix, the vector space spanned by $A'$ is of dimension $k$ only and much less complex than the initial space. When used for information retrieval, $k$ is typically about 200-500, while $n$ and $m$ may go into millions. When used to analyze software on the other hand, $k$ is typically about $20-50$ with vocabulary and documents in the range of thousands only. And since $A'$ is the best approximation of $A$ under the least-square-error criterion, the similarity between documents is preserved, while in the same time mapping semantically related terms on one axis of the reduced vector space and thus taking into account synonymy and polysemy. In other words, the initial term-document-matrix $A$ is a table with term occurrences and by breaking it down to much less dimension the latent meaning \emph{must} appear in $A'$ since there is now much less space to encode the same information. Meaningless occurrence data is transformed into meaningful concept information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semantic Clustering: Grouping Source Documents}\label{sec:sekla}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The result of applying LSI is a vector space, based on which we can compute the similarity between both documents or terms. We use this similarity measurement to identify topics in the source code.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=.8\columnwidth]{clustering}
\caption{Semantic clustering of software source code (\eg classes, methods).}
\label{fig:clustering}
\end{center}
\end{figure}

\autoref{fig:clustering} illustrates the first three steps of the approach: preprocessing, applying LSI, and clustering. Furthermore we retrieve the most relevant terms for each cluster and visualize the clustering on a 2D-map, thus in short the approach is:

\begin{enumerate}
  \item \emph{Preprocessing the software system.} In \autoref{sec:parsing}, we show how we break the system into documents and how we build a term-document-matrix that contains the vocabulary usage of the system.
  \item \emph{Applying Latent Semantic Indexing.} In \autoref{sec:lsi} we use LSI to compute the similarities between source code documents and illustrate the result in a correlation matrix \cite{Ling73a}.
  \item \emph{Identifying topics.} In \autoref{sec:clustering} we cluster the documents based on their similarity, and we rearrange the correlation matrix. In \autoref{sec:wittgenstein} we discuss that each cluster is a \emph{linguistic topic}.
  \item \emph{Describing the topics with labels.} In \autoref{sec:labeling} we use LSI again to retrieve for each cluster the top-$n$ most relevant terms.
  \item \emph{Comparing the topics to the structure.} In \autoref{sec:distribution} we illustrate the distribution of topics over the system on a Distribution Map \cite{Duca06c}.
\end{enumerate}

We want to emphasize that the primary contribution of our work is semantic clustering and the labeling. The visualization we describe are just used as a means to convey the results and are not original contributions of this paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On the Relationship between Concepts and Semantic Clustering}\label{sec:wittgenstein}

In this paper we are interested in locating concepts in the source code. However, our concern is not to locate the implementation of externally defined domain concepts, but rather to derive the implemented topics from the vocabulary of source code. We tackle the problem of concept location on the very level of source code, where we apply information retrieval to analyze the use of words in source code documents. The clusters retrieved by our approach are not necessarily domain or application concepts, but rather code-oriented topics.

Other than with domain or application concepts in Software Analysis \cite{Bigg93a}, it is not uncommon in information retrieval to derive \emph{linguistic topics} from the distribution of words over a set of documents. This is in accordance with Wittgenstein who states that \emph{``Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache---the meaning of a word is given by its use in language''} \cite{Witt53a}. Unlike in classical philosophy, as for example Plato, there is no external definition of a word's meaning, but rather the meaning of a word is given by the relations it bears with other terms and sentences being used in the same context. Certainly, there is a congruence between the external definition of a word and its real meaning, since our external definitions are human-made as well, and thus, also part of the language game, but this congruence is never completely accurate.

In accordance with \cite{Bigg89c} we call our clusters \emph{linguistic topics} since they are derived from language use. Certainly, some linguistic topics do map to the domain and others do map to application concepts, however, this mapping is never complete. There is no guarantee that semantic clustering locates all or even any externally defined domain concept. But nonetheless, our case studies show that semantic clustering is able to capture important domain and application concepts of a software system. Which does not come as a surprise, since it is well known that identifer names and comments are one of the most prominent places where developers put their knowledge about a system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preprocessing the Software System}\label{sec:parsing}

When we apply LSI to a software system we partition its source code into documents and we use the vocabulary found therein as terms. The system can be split into documents at any level of granularity, such as packages or classes and methods. Other slicing solutions are possible as well, for example execution traces \cite{Kuhn05b}, or we can even use entire projects as documents and analyze a complete source repository \cite{Kawa04a}.

To build the term-document-matrix, we extract the vocabulary from the source code: we use both identifier names and the content of comments. Natural language text in comments is broken into words, whereas compound identifier names are split into parts. As most modern naming conventions use camel case, splitting identifiers is straightforward: for example \emph{FooBar} becomes \emph{foo} and \emph{bar}.

We exclude common stopwords from the vocabulary, as they do not help to discriminate documents. In addition, if the first comment of a class contains a copyright disclaimer, we exclude it as well. To reduce words to their morphological root we apply a stemming algorithm: for example \emph{entity} and \emph{entities} both become \emph{entiti} \cite{Port80a}. And finally, the term-document matrix is weighted with \emph{tf-idf} to balance out the influence of very rare and very common terms \cite{Duma91a}.

When preprocessing object-oriented software systems we take the inheritance relationship into account as well. For example, when applying our approach on the level of classes, each class inherits some of the vocabulary of its superclass. If a method is defined only in the superclass we add its vocabulary to the current class. Per level of inheritance a weighting factor of $w = 0.5$ applies to the term occurrences, to balance out between the abstractness of high level definitions and concrete implementations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Using Latent Semantic Indexing to Build the Similarity Index}
\label{sec:lsi}

We use LSI to extract linguistic information from the source code, which results in an LSI-index with similarities between source documents (\ie packages, classes or methods). Based on the index we can determine the similarity between source code documents. Documents are more similar if they cover the same topic, terms are more similar if they denote related topics.

In the vector space model there is a vector for each document. For example, if we use methods as documents, there is a vector for each method and the cosine between these vectors denotes the semantic similarity between the methods. In general cosine values are in the $[-1,1]$ range, however when using an LSI-index the cosine between its element never strays much below zero. This is since the LSI-index is derived from a term-document matrix that contains positive occurrence data only.

\emph{First matrix in \autoref{fig:comaFourStep}.} To visualize similarities between documents we map them to gray values: the darker, the more similar. The similarities between elements are arranged in a square matrix called \emph{correlation matrix} or \emph{dot plot}. Correlation matrix is a common visualization tool to analyze patterns in a set of entities \cite{Ling73a}. Each dot $a_{i,j}$ denotes the similarity between element $d_i$ and element $d_j$. Put in other words, the elements are arranged on the diagonal and the dots in the off-diagonal show the relationship between them.

\begin{figure}[h]
  \includegraphics{clusteringEx}
  \caption{From left to right: unordered correlation matrix, then sorted by similarity, then grouped by clusters, and finally including semantic links.}\label{fig:comaFourStep}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clustering: Ordering the Correlation Matrix}\label{sec:clustering}

%do not change the first sentence of this section, it is a hidden reference to the fist sentence from William  Gibsons novel Necromancer: ``The sky above the port was the color of television, tuned to a dead channel''.

Without proper ordering the correlation matrix looks like television tuned to a dead channel. An unordered matrix does not reveal any patterns. An arbitrary ordering, such as for example the names of the elements, is generally as useful as random ordering \cite{Bert73a}. Therefore, we cluster the matrix to put similar elements near each other and dissimilar elements far apart of each other.

A clustering algorithm groups similar elements together and aggregates them into clusters \cite{Jain99a}. Hierarchical clustering creates a tree of nested clusters, called \emph{dendrogram}, which has two features: breaking the tree at a given threshold groups the elements into clusters, and traversing the tree imposes a sort order upon its leaves. We use these two features to rearrange the matrix and to group the dots into rectangular areas.

\emph{Second and third matrix in \autoref{fig:comaFourStep}.} Each rectangle on the diagonal represents a semantic cluster: the size is given by the number of classes that belong to a topic, the color refers to the \emph{semantic cohesion} \cite{Marc05a} (\ie the average similarity among its classes\footnote{Based on the similarity ${\rm sim}(a,b)$ between elements, we define the similarity between cluster $A$ and cluster $B$ as $\frac{1}{|B| \times |A|}\sum \sum {\rm sim}(a_m,b_n)$ with $a \in A$ and $b \in B$ and in the same way the similarity between an element $a_0$ and a cluster $B$ as $\frac{1}{|B|}\sum {\rm sim}(a_0,b_n)$ with $B \in B$.
}). The color in the off-diagonal is the darker the more similar to clusters are, if it is white they are not similar at all. The position on the diagonal is ordered to make sure that similar topics are placed together.

The clustering takes the focus of the visualization from similarity between elements to similarity between clusters. The tradeoff is, as with any abstraction, that some valuable detail information is lost. Our experiments showed that one-to-many relationships between an element and an entire cluster are valuable patterns.

\emph{Fourth matrix in \autoref{fig:comaFourStep}.} If the similarity between an element $d_n$ from a cluster and another cluster differs more than a fix threshold from the average similarity between the two clusters, we plot $d_n$ on top of the clustered matrix either as a bright line if $d_n$ is less similar than average, or as a dark line if $d_n$ is more similar than average. We call such a one-to-many relationship a \emph{semantic link}, as it reveals an element that links from its own topic to another topic.

\autoref{fig:hotspot} illustrates an example of a semantic link. The relationship between cluster A and cluster B is represented by the rectangle found at the intersection of the two clusters. The semantic link can be identified by a horizontal (or vertical) line that is darker than the rest of the rectangle. In our example, we find such a semantic link between two clusters. Please note that the presence of a link does not mean that the element in A should belong to B, it only means that the element in A is more similar to B then the other elements in A.

\begin{figure}[h]
    \centering
  \includegraphics{hotspot}
  \caption{Illustration of a semantic link. A semantic link is a one-to-many relation: A document in cluster A that is more similar than its siblings in A to cluster B.}\label{fig:hotspot}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Labeling the Clusters}\label{sec:labeling}

With the clustering we partitioned the source documents by their vocabulary, but we do not know about the actual vocabulary that connects these documents together. In other words, what are the most important terms for each cluster? In this section we use LSI to retrieve for each cluster the top-$n$ list with its most relevant terms. We use these lists to label the topics.

The labeling works as follows. As we already have an LSI-index at hand, we use it as a search engine \cite{Berr94a}. We reverse the usual search process where a search query of terms is used to find documents, and instead, we use the documents in a cluster as search query to find the most similar terms. To label a cluster, we take the top-$n$ most similar terms, using a top-7 list provides a useful labeling for most case studies.

To compute the relevance of a term, we compare the similarity to the current cluster with the similarity to all other clusters. This raises better results than just retrieving the top most similar terms \cite{Kuhn05a}. Common terms, as for example the names of custom data structures and utility classes, are often highly similar to many clusters. Thus, these terms would pollute the top-$n$ lists with non-discriminating labels if using plain similarity only, whereas subtracting the average lowers the ranking of such common terms. The following formula ranks high those terms that are very similar to the current cluster but not common to all other clusters.

$$\mathrm{rel}(t_0,A_0) = \mathrm{sim}(t_0,A_0) - \frac{1}{|\mathcal{A}|} \sum_{A_n \in \mathcal{A}}{\mathrm{sim}(t_0,A_n)}$$

Term $t_0$ is relevant to the current cluster $A_0$, if it has a high similarity to the current cluster $A_0$ but not to the remaining clusters $A_n \in \mathcal{A}$. Given the similarity between a term $t$ and a cluster $A$ as $\mathrm{sim}(t,A)$, we define the relevance of term $t_0$ according to cluster $A_0$ as given above.

Another solution to avoid such terms, is to include the common terms into a custom stopword list. However this cannot be done without prior knowledge about the system, which is more work and contradicts our objectives. Furthermore, this ranking formula is much smoother than the strict opt-out logic of a stopword list.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analyzing the Distribution of Semantic Clusters}\label{sec:distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The semantic clusters help us grasp the topics implemented in the source code. However, the clustering does not take the structure of the system into account. As such, an important question is: How are these topics distributed over the system?

To answer this question, we use a Distribution Map \cite{Tuft01a,Duca06c}. A Distribution Map visualizes the distribution of properties over system parts \ie a set of entities. In this paper, we visualize packages and their classes, and color these classes according to the semantic cluster to which they belong.

For example, in \autoref{fig:distMap} we show an example of a Distribution Map representing 5 packages, 37 classes and 4 semantic clusters. Each package is represented by a rectangle, which includes classes represented as small squares. Each class is colored by the semantic cluster to which it belongs.

\begin{figure}[h]
    \centering
  \includegraphics{MetricExamples}\\
  \caption{Example of a Distribution Map.}\label{fig:distMap}
\end{figure}

Using the Distribution Map visualization we correlate linguistic information with structural information. The semantic partition of a system, as obtained by semantic clustering, does generally not correspond one-on-one to its structural modularization. In most systems we find both, topics that correspond to the structure as well as topics that cross-cut it. Applying this visualization on several case studies, we identified the following patterns:

\begin{itemize}
  \item \emph{Well-encapsulated topic} -- if a topic corresponds to system parts, we call this a \emph{well-encapsulated topic}. Such a topic is spread over one or multiple parts and includes almost all source code within those parts. If a well-encapsulated topic covers only one part we speak of a \emph{solitary topic}.

  \item \emph{Cross-Cutting topic} -- if a topic is orthogonal to system parts, we call this a \emph{cross-cutting topic}. Such a topic spreads across multiple parts, but includes only one or very few elements within each parts. As linguistic information and structure are independent of each other, cross-cutting identifiers or names do not constitute a design flaw. Whether a cross-cutting topic has to be considered a design smell or not depends on the particular circumstances. Consider for example the popular three-tier architecture: It separates \emph{accessing, processing \emph{and} presenting data} into three layers; where application specific topics -- such as \eg \emph{accounts, transactions \emph{or} customers} -- are deliberately designated to cross-cut the layers. That is, it emphasizes on the separation of those three topics and deliberately designates the others as cross-cutting concerns.

 \item \emph{Octopus topic} -- if a topic dominates one part, as a solitary does, but also spreads across other parts, as a cross-cutter does, we call this an \emph{octopus topic}. Consider for example a framework or a library: there is a core part with the implementation and scattered across other parts there is source code that plug into the core, and hence use the same vocabulary as the core.

  \item \emph{Black Sheep topic} -- if there is a topic that consists only of one or a few separate source documents, we call this a \emph{black sheep}. Each black sheep deserves closer inspection, as these documents are sometimes a severe design smell. Yet as often, a black sheep is just an unrelated helper class and thus not similar enough to any other topic of the system.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Case studies}\label{sec:validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To show evidence of the usefulness of our approach for software comprehension, in this section we apply it on two case studies. Due to space limitations, only the first case study is presented in full length.

First, we exemplify each step of the approach and discuss its findings in the case of JEdit, a text editor written in Java. This case study is presented in full length. Secondly, we present JBoss, an application-server written in Java, which includes interesting anomalies in its vocabulary.

\begin{figure}[h]
\centering
{\scriptsize
\begin{tabular}{l|llrrrrrr}
\hline
\textbf{Case Study}&\textbf{language}&\textbf{type}&\textbf{docs}&\textbf{terms}
&\textbf{parts}&\textbf{links}&\textbf{rank}&\textbf{sim}\\
\hline
Ant & Java & \emph{Classes} & 665 & 1787 & 9 & -- & 17 & 0.4\\
Azureus & Java & \emph{Classes}       & 2184 & 1980 & 14 & -- & 22 & 0.4\\
JEdit & Java & \emph{Classes}       & 394  & 1603 & 9 & 53 &17 & 0.5\\
JBoss & Java & \emph{Classes}       & 660 & 1379 & 10 & -- & 16 & 0.5\\
Moose\footnotemark{} & Smalltalk & \emph{Classes}  & 726  & 11785 & -- & 137& 27 & --\\
MSEModel & Smalltalk & \emph{Methods}  & 4324  & 2600 & -- & -- & 32 & 0.75\\
Outsight & Java & \emph{Classes}    & 223 & 774 & 10 & -- & 12 & 0.5\\
\hline
\end{tabular}}\\
\caption{The statistics of sample case studies, JEdit and JBoss are discussed in this work, for the other studies please refer to our previous work \cite{Kuhn05a,Kuhn06a}.}\label{fig:table1}
\end{figure}
\footnotetext{The Moose case study in \cite{Kuhn05a} did not use stemming to preprocess the text corpus, hence the large vocabulary.}

\autoref{fig:table1} summarizes the problem size of each case study. It lists for each case study: (lang) the language of the source code, (type) the granularity of  documents, (docs) the number of documents, (terms) the number of terms, (parts) the number of found topics, (links) the number of found semantic links, (rank) the dimension of the LSI-index, and (sim) the threshold  of the clustering.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On the Calibration of Parameters and Thresholds}\label{sec:parameters}

Our approach depends on several parameters, which may be difficult too choose for someone not familiar with the underlying technologies. In this section we present all parameters, discuss their calibration and share our experience gained when performing case studies using the Hapax tool.

  \textbf{Weighting the term-document-matrix.} To balance out the influence of very rare and very common terms, it is common in information retrieval to weight the occurrence values. The most common weighting scheme is \emph{tf-idf}, which we also use in the case studies, others are entropy or logarithmic weighting \cite{Duma91a}.
  %Nako01b = Weight functions impact on LSA performance.

  When experimenting with different weighting schemes, we observed that the choice of the weighting scheme has a considerable effect on the similarity values, depending on the weighting the distance within the complete text corpus becomes more compact or more loose \cite{Nako01b}. Depending on the choice of the weighting scheme, the similarity thresholds may differ significantly: as a rule of thumb, using logarithmic weighting and a similarity threshold of $\delta = 0.75$ is roughly equivalent to a threshold of $\delta = 0.5$ with \emph{tf-idf} weighting \cite{Nako01a}.

  \textbf{Dimensionality of the LSI-space.} As explained in \autoref{sec:LSI}, LSI replaces the term-document matrix with a low-rank approximation. When working with natural language text corpora that contain millions of documents and some ten thousands of terms, most authors suggest to use an approximation between rank 200 and 500. In Software Analysis the number of documents is much smaller, such that even ranks as low as 10 or 25 dimensions yield valuable results. Our tool uses rank $r = (m * n)^{0.2}$ by default for an $m \times n$-dimensional text corpus, and allows customization.

  \textbf{Choice of clustering algorithm.} There is a rich literature on different clustering algorithms \cite{Jain99a}. We performed a series of experiments using different algorithms, however as we cannot compare our results against an \emph{a priori} known partition, we cannot measure recall in hard numbers and have to rely on human judgment of the results. Therefore we decided to use a hierarchical \emph{average-linkage} clustering as it is a common standard algorithm. Further studies on the choice of clustering algorithm are open for future work.

  \textbf{Breaking the dendrogram into clusters.} Hierarchical clustering uses a threshold to break the dendrogram, which is the tree of all possible clusters, into a fix partition. Depending on the objective, we break it either into a fixed number of clusters (\eg for the Distribution Map, where the number of colors is constrained) or at a given threshold (\eg for the correlation matrix). In the user interface of the Hapax tool, there is a slider for the threshold such that we can immediately observe the effect on both correlation matrix and Distribution Map interactively.

  \textbf{Detecting semantic links.} As explained in \autoref{sec:clustering}, a semantics link is an element in cluster $A$ with a different similarity to cluster $B$ than average. Typically we use a threshold of $\delta = \pm20\%$ to decide this, however, our tool supports fixed thresholds and selecting the top-$n$ link as well. When detecting traceability-links, a problem which is closely related to semantic links, this has been proven as the best out if these three strategies \cite{Luci04a}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantic Clustering applied on JEdit}

We exemplify our approach at the case of JEdit, an open-source Text editor written in Java. The case study contains 394 classes and uses a vocabulary of 1603 distinct terms. We reduced the text corpus to an LSI-space with rank $r = 15$ and clustered it with a threshold of $\delta = 0.5$ (the choice of parameters is discussed in \autoref{sec:parameters}).

In \autoref{fig:jeditCorrelation}, we see nine clusters with a size of (from top right to bottom left) 116, 63, 26, 10, 68, 10, 12, 80, and 9 classes. The system is divided into four zones: (zone 1) the large cluster in the top left, (zone 2) two medium sized and a small clusters, (zone 3) a large cluster and two small clusters, and (zone 4) a large and a small cluster. The two zones in the middle that are both similar to the first zone but not to each other, and the fourth zone is not similar to any zone.

In fact, there is a limited area of similarity between the Zone 2 and 3. We will later on identify the two counterparts as topics \pink and \cyan, which are related to text buffers and regular expression respectively. These two topics share some of their labels (\ie start, end, length and count), however they are clustered separately since LSI does more than just keyword matching, LSI takes the context of term usage into account as well, that is the co-location of terms with other terms.

\begin{figure}[b]
  \centering
  \includegraphics{jeditcorrelationzone}\\
  \caption{The correlation matrix of JEdit.}\label{fig:jeditCorrelation}
\end{figure}

This is a common pattern that we often encountered during our experiments: zone 1 is the core of system with domain-specific implementation, zone 2 and 3 are facilities closely related to the core, and zone 4 is an unrelated component or even a third-party library. However, so far this is just an educated guess and therefore we will have a look at the labels next.

\autoref{fig:jeditLabels} lists for each cluster the top-7 most relevant labels, ordered by relevance. The labels provide a good description of the clusters and the tell same story as the correlation matrix before. We verified the labels and topics by looking at the actual classes within each cluster.

\begin{itemize}
  \item Zone 1: topic \red implements the very domain of the system: files and users, and a user can load, edit and save files.
  \item Zone 2: topic \green and \magenta implement the user interface, and topic \pink implements text buffers.
  \item Zone 3: topic \cyan is about regular expressions, topic \yellow provides XML support and topic \darkgreen is about TAR archives.
  \item Zone 4: topic \blue and \orange are the BeanShell scripting framework, a third-party library.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics{jeditCorrelationLabels}
\caption{The semantic clusters of JEdit and their labels.}\label{fig:jeditLabels}
\end{figure}

All these labels are terms taken from the vocabulary of the source code and as such they do not always describe the topics in generic terms. For example, event though JEdit is a text-editor, the term \emph{text-editor} is not used on the source code level. The same applies for topic \cyan, where the term \emph{regular expression} does not shop up in the labels.

Next are the semantic links, these are the thin lines on the correlation matrix. If a line is thicker than one pixel, this means that there are two or more similar classes which are each a semantic links. The analysis detected 53 links, below we list as a selection all links related to the scripting topic (these are the link annotated on \autoref{fig:jeditCorrelation}). For each link we list the class names and the direction:

\begin{itemize}
  \item Link 1: \emph{AppelScriptHandler} and \emph{BeanShell} link from topic \red to scripting.
  \item Link 2: \emph{Remote} and \emph{ConsoleInterface} link from topic \red to scripting
  \item Link 3: \emph{VFSDirectoryEntryTable} and \emph{JThis} link from the UI topic to scripting.
  \item Link 4: \emph{ParseException}, \emph{Token} \emph{TokenMgrError} and \emph{BSHLiteral} link from the regular expressions topic to scripting.
  \item Link 5: \emph{ClassGeneratorUtil} and \emph{ClasspathError} link from scripting to topic \red.
  \item Link 6: \emph{ReflectManager} and \emph{Modifiers} link from scripting to the UI topic.
  \item Link 7: \emph{NameSource} and \emph{SimpleNode} link from scripting to the regular expressions and the XML topic.
\end{itemize}

Evaluating semantic links is similar to evaluating traceability links \cite{Luci04a}. We inspected the source code of each link. Some of them are false positives, for example link 4 and 5 are based on obviously unrelated use of the same identifers. On the other hand, link 1 points to the main connector between core and scripting library and link 7 reveals a high-level clone, the implementation of the same datatype in three places: XML uses nodes, and both BSH and regular expressions implement their own AST node.


\autoref{fig:jeditDistribution} shows the distribution of topics over the package structure of JEdit. The large boxes are the packages (the text above is the package name), the squares are classes and the colors correspond to topics (the colors are the same as on \autoref{fig:jeditLabels}).

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{jeditDistribution}\\
  \caption{The Distribution Map of the semantic clusters over the package structure of JEdit.}\label{fig:jeditDistribution}
\end{figure}


For example, in \autoref{fig:jeditDistribution} the large box on the right represents the package named \emph{bsh}, it contains over 80 classes and most of these classes implement the topic referred to by \blue. The package boxes are ordered by their similarity, such that related packages are placed near to each other.

Topic \red, the largest cluster, shows which parts of the system belong to the core and which do not. Based on the ordering of the packages, we can conclude that the two UI topics (\eg \green and \yellow) are more closely related to the core than for example topic \cyan, which implements regular expressions.

The three most well-encapsulated topics (\eg \orange, \blue and \cyan) implement separated topics such as scripting and regular expressions. Topic \yellow and \pink cross-cut the system: \yellow implements dockable windows, a custom GUI-feature, and \pink is about handling text buffers. These two topics are good candidates for a closer inspection, since we might want to refactor them into packages of their own.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{First Impression of JBoss: Distribution Map and Labels}\label{sec:azureus}

This case study presents the outline of JBoss, an application-server written in Java. We applied semantic clustering and partitioned the system into ten topics. The system is divided into one large cluster (colored in red), which implements the core of the server, and nine smaller clusters. Most of the small clusters implement different services and protocols provided by the application server.

\begin{figure}[htbp]
\begin{center}
  \includegraphics[width=\linewidth]{JBossDistribution}
  \caption{The Distribution Map of the semantic clusters over the package structure of JBoss.}
  \label{fig:JBossDistribution}
\end{center}
\end{figure}


\begin{figure}[h]
  \centering
  \begin{scriptsize}
  \begin{tabular}{l|rl}
    \hline
    \textbf{Color} & \textbf{Size} & \textbf{Labels}\\
    \hline
    red & 223 & invocation, invoke, wire, interceptor, call, chain, proxy, share\\
    blue & 141 & jdbccmp, JDBC, cmp, field, loubyansky, table, fetch\\
    cyan & 97 & app, web, deploy, undeployed, enc, JAR, servlet\\
    green & 63 & datetime, parenthesis, arithmetic, negative, mult, div, AST\\
    yellow & 35 & security, authenticate, subject, realm, made, principle, sec\\
    dark magenta & 30 & expire, apr, timer, txtimer, duration, recreation, elapsed\\
    magenta & 20 & ASF, alive, topic, mq, dlq, consume, letter\\
    orange & 20 & qname, anonymous, jaxrpcmap, aux, xb, xmln, WSDL\\
    purple & 16 & invalid, cost, September, subscribe, emitt, asynchron, IG\\
    dark green & 15 & verify, license, warranty, foundation, USA, lesser, fit\\
    \hline
  \end{tabular}
  \end{scriptsize}
  \caption{The labels of the semantic clusters of JBoss.}\label{fig:JBossLabels}
\end{figure}


The Distribution Map is illustrated in \autoref{fig:JBossDistribution}, and the top-7 labels are listed in figure \autoref{fig:JBossLabels} in order of relevance. This is the same setup as in the first case study, except the correlation matrix and semantic links are left out due to space restrictions. We verified the clustering by looking the source code, and present the results as follows.

Topic \red is the largest cluster and implements the core functionality of the system: is labeled with terms such as \emph{invocation, interceptor, proxy \emph{and} share}. Related to that, topic \cyan implements the deployment of JAR archives.

The most well-encapsulated topics are \darkgreen, \orange, \green and \blue. The first three are placed apart from \red, whereas \blue has outliers in the red core packages. The labels and package names (which are printed above the package boxes in the Distribution Map) show that \darkgreen is a bean verifier, that \orange implements JAX-RPC and WDSL (\eg web-services), that \green implements an SQL parser and that \blue provides JDBC (\eg database access) support. These are all important topics of an application server.

The most cross-cutting topic is \yellow, it spreads across half of the system. The labels reveal that this the security aspect of JBoss, which is reasonable as security is an important feature within a server architecture.

Noteworthy is the label \emph{loubyansky} in topic \blue, it is the name of a developer. Based on the fact that his name appears as one of the labels, we assume that he is the main developers of that part of the system. Further investigation proved this to be true.

Noteworthy as well are the labels of topic \darkgreen, as they expose a failure in the preprocessing of the input data. To exclude copyright disclaimers, as for example the GPL licence, we ignore any comment above the \emph{package} statement of a Java class. In the case of topic \darkgreen this heuristic failed: the source files contained another licence within the body of the class. However, repeating the same case study with an improved preprocessing resulted in nearly the same clustering and labeled this cluster as RMI component: \emph{event, receiver, RMI, RMIiop, iiop, RMIidl, and idl}.

The topics extracted from the source code can help improving the comprehension. If a maintainer is seeking information, semantic clustering helps in identifying the related code. This is similar to the use of a search engine, for example if the web-service interface has to be changed, the maintainer can immediately look at the Orange concept, and identify the related classes. Much in the same way, if one has to maintain the database interface, he looks at the Blue concept.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In this section we evaluate and discuss success criteria, strengths and limitations of the proposed approach. We discuss how the approach stands and fails with the quality of the identifer naming. Furthermore we discuss the relation between linguistic topics and domain or application concepts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On the Quality of Identifier Names}

In the same way as structural analysis depends on correct syntax, semantic analysis is sensitive to the quality of the naming. Since we derive our topics solely based on the use of identifer names and comments, it does not come as a surprise that our approach stands and fails with the quality of the source code naming.

Our results are not generalizable to any software system, a good naming convention and well chosen identifiers yields best results, whereas bad naming (\ie too generic names, arbitrary names or cryptic abbreviations) is one of the main threats to external validation. The vocabulary of the case studies presented in this work is of good quality, however, when performing other case studies we learned of different facets that affect the outcome, these are:

\textbf{On the use of naming conventions.} Source following state-of-the-art naming conventions, as for example the Java Naming Convention, is easy to preprocess. In case of legacy code that uses other naming conventions (\eg the famous Hungarian Notation) or even none at all, other algorithms and heuristics are to be applied \cite{Capr93a,Anqu98b}.

\textbf{On generic or arbitrary named identifiers.} However, even the best preprocessing cannot guess the meaning of variables which are just named \emph{temp} or \emph{a}, \emph{b} and \emph{c}. If the developers did not name the identifiers with care, our approach fails, since the developer knowledge is missing. Due to the strength of LSI in detecting synonymy and polysemy, our approach can deal with a certain amount of such ambiguous or even completely wrong named identifiers -- but if a majority of identifiers in system is badly chosen, the approach fails.

\textbf{On abbreviated identifier names.} Abbreviated identifers are commonly found in legacy code, since early programming languages often restrict the discrimination of identifer names to the first few letters. But unlike generic names, abbreviations affect the labeling only and do not threat our approach  as whole. This might come as a surprise, but since LSI is solely based on analyzing the statistical distribution of terms across the document set, it is not relevant whether identifiers are consistently written out or consistently abbreviated.

However, if the labeling task comes up with terms such as \emph{pma}, \emph{tcm}, \emph{IPFWDIF} or \emph{sccpsn} this does not tell a human reader much about the system. These terms are examples taken from a large industry case study, which is not included in this paper, where about a third of all identifiers where abbreviations. In this case the labeling was completely useless. Please refer to \cite{Anqu98b} for approaches on how to recover abbreviations.

\textbf{On the size of the vocabulary.}  The vocabulary of source code is very small, smaller than that of a natural language text corpus. Intuitively explained: LSI is like a child learning language. In the same way as a human with a vocabulary of 2000 terms is less eloquent and knowledgeable than a human with a vocabulary of 20,000 terms, LSI performs better the larger the vocabulary. Whereas, the smaller the vocabulary the stronger the effect of missing or incorrect terms. In fact, LSI has been proven a valid model of the way children acquire language \cite{Land97a}.

\textbf{On the size of documents.} In average there are only about 5-10 distinct terms per method body, and 20-50 distinct terms per class. In a well commented software system, these numbers are higher since comments are human-readable text. This is one of the rationales why LSI does not perform as accurate on source code as on natural language text \cite{Luci04a}, however the results are of sufficient quality.

\textbf{On the combination of LSI with morphological analysis.} Even tough the benefits of stemming are not without controversy, we apply it as part of the preprocessing step \cite{Baez99b}. Our rational is: analyzing a software system at the level of methods is very sensitive to the quality of input, as the small document size threatens the success of LSI. Considering these circumstances, we  decided to rely on stemming as it is well known that the naming of identifers often includes the same term in singular and plurals: for example \emph{setProperty} and \emph{getAllProperties} or \emph{addChild} and \emph{getChildren}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On using Semantic Clustering for Topic Identification}

One of our objectives is to compare linguistic topics to domain and application concepts \cite{Bigg93a}. As discussed in \autoref{sec:wittgenstein}, we derive linguistic topics from the vocabulary usage of source code instead from external definitions. In this section we clarify some questions concerning the relation between derived topics and externally defined concepts:

\textbf{On missing vocabulary and ontologies.} Often the externally defined concepts are not captured by the labeling. The rational for this is as follows. Consider for example a text editor in whose source code the term \emph{text-editor} is never actually used, but terms like \emph{file} and \emph{user}. In this case our approach will label the text-editor concepts with these two terms, as a more generic term is missing. As our approach is not based on an ontological database, its vocabulary is limited to the terms found in source code and if terms are not used our approach will not find accurate labels. We suggest to use ontologies (\ie WordNet) to improve the results in these cases.

\textbf{On the congruence between topics and domain.} When starting this work, one of our hypotheses was that semantic clustering will reveal a system's domain semantics. But our experiments disproved this hypothesis: most linguistic topics are application concepts or architectural components, such as layers. In many experiments, our approach partitioned the system into one (or sometimes two) large domain-specific part and up to a dozen domain-independent parts, such as for example input/output or data storage facilities. Consider for example the application in \autoref{fig:outsight}, it is divided into nine parts as follows:

\begin{figure}[h]
  % Requires \usepackage{graphicx}
  \includegraphics[width=\linewidth]{outsightDistribution}\\
  \caption{The Distribution Map of Outsight, a webbased job portal application \cite{Kuhn06a}.}\label{fig:outsight}
\end{figure}

Only one topic out of nine concepts is about the system's domain: job exchange. Topic \red includes the complete domain of the system: that is users, companies and CVs. Whereas all other topics are application specific components: topic \blue is a CV search engine, topic \darkgreen implements PDF generation, topic \green is text and file handling, topic \cyan and \magenta provide access to the database, and topic DarkCyan is a testing and debugging facility. Additionally the cross-cutting topic \yellow bundles high-level clones related to time and timestamps.

\textbf{On the congruence between topics and packages.} In section \autoref{sec:distribution} we discussed the relation between topics and packages. Considering again the case study in \autoref{fig:outsight} as an example, we find occurrences of all four patterns: Topic \darkgreen for example is well-encapsulated, whereas topic \yellow cross-cuts the application. Then there is topic \blue, which is an octopus with the \emph{conditions} package as its body and tentacles reaching into six other packages, and finally we have in the \emph{logic} package an instance of a black sheep.

\textbf{On source documents that are related to more than one topic.} If we want to analyze how the topics are spread across some type of documents (\eg packages, classes or methods) we have to break the system into documents one level below the target level. For example, if we want to analyze the topic distribution over packages, we break the system into classes and analyze how the topics of classes are spread over the packages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Source code bears the semantics of an application in the names of identifiers and comments. In this paper we present our approach to retrieve the topics present in the source code vocabulary to support program comprehension. We introduce semantic clustering, a technique based on Latent Semantic Indexing and clustering to group source documents that use similar vocabulary. We call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code. As compared to our previous approach, we go a step forward and use Distribution Maps to illustrate how the semantic clusters are distributed over the system.

We applied the approach on several case studies with different characteristics, two of which are presented in this paper.  The case studies give evidence that our approach provides a useful first impression of an unfamiliar system, and that we reveal valuable developer knowledge. The Distribution Map together with the labeling provides a good first impression of the software's domain. Semantic clustering captures topics regardless of class hierarchies, packages and other structures. One can, at a glance, see whether the software covers just a few or many different topics, how these are distributed over the structure, and -- due to the labeling -- what they are about.

When starting this work, one of our hypotheses was that semantic clustering would reveal a systems domain semantics. However, our experiments showed that most linguistic topics relate to application concepts or architectural components. Usually, our approach partitions a system into one (or sometimes two) large domain-specific clusters and up to a dozen domain-independent clusters. As part of our future work, we plan to investigate more closely the relationship between linguistic topics and both domain and application concepts.

In the future we would also like to investigate in more depth recall and precision of the approach. For example, we would like to compare the results of the semantic clustering with other types of clustering. Furthermore, we would like to improve the labeling with other computer linguistic techniques.

\chapter{Summarizing Software with Lexical Clues and Temporal Clues}

As more and more open-source software components become available on the internet we need automatic ways to label and compare them. For example, a developer who searches for reusable software must be able to quickly gain an understanding of retrieved components. This understanding cannot be gained at the level of source code due to the semantic gap between source code and the domain model. In this paper we present a lexical approach that uses the log-likelihood ratios of word frequencies to automatically provide labels for software components. We present a prototype implementation of our labeling/comparison algorithm and provide examples of its application. In particular, we apply the approach to detect trends in the evolution of a software system.

In recent years, software vocabulary has been proven to be a valuable source for software analysis, often including the retrieval of labels (\eg \cite{Baldi08OOPSLA,EinarHoest,Kuhn07a}). However, labeling software is not without pitfalls. The distribution of words in software corpora follows the same power-law as word frequencies in natural-language text \cite{Linstead09SUITE}. Most of the text is made up of a small set of common terms, whereas content-bearing words are rare. Analysis of software vocabulary must deal with the reality of rare terms, thus statistical tests that assume normal distribution are not applicable. For example, textual comparison based on directly counting term frequencies is subject to overestimation when the frequencies involved are very small.  

For text analysis the use of \loglr improves the statistical results. Likelihood tests do not depend on assumptions of normal distribution, instead they use the asymptotic distribution of binomial likelihood \cite{Dunning}. Using \loglr{}s allows comparisons to be made between the significance of occurrences of both common and rare terms.

In this paper we present a lexical approach that uses the \loglr of word frequencies to automatically retrieve labels from source code. The approach can be applied i) to compare components with each other, ii) to compare a component against a normative corpus, and iii) to compare different revisions of the same component. We present a prototype implementation and give examples of its application. In particular, we apply the approach to detect trends in the evolution of the JUnit software system.

\section{Log-Likelihood in a Nutshell}\label{nutshell}

This section explains how log-likelihood ratio is applied to analyse word frequencies. The explanations are kept as concise as possible. We provide the general background and just enough details such that a programmer may implement the algorithm. Please refer to Ted Dunning's work \cite{Dunning} for more background.

The idea behind \loglr is to compare two statistical hypotheses, of which one is a subspace of the other. Given two text corpora, we compare the hypothesis that both corpora have the same distribution of term frequencies with the ``hypothesis'' given by the actual term frequencies. Because we know that terms are not equally distributed over source code, we use binomial likelihood

$$ L(p,k,n) = p^k ( 1 - p) ^ { n - k }$$
\noindent
with $p = \frac{k}{n}$, where $k$ is the term frequency (\ie number of occurrences) and $n$ the size of the corpus. Taking the logarithm of the likelihood ratio gives
\footnote{In some publications (\eg \cite{Rayson}) we found that the last two terms were omitted (since their values tend to be orders of magnitude smaller than the corresponding values of the first two terms). The results presented in this paper, however, are obtained using the complete log likelihood ratio formula.}

\begin{align*}
-2 \log \lambda =~&2 \big[ \log L(p_1,k_1,n_1) + \log L(p_2,k_2,n_2)\\
 &- \log L(p,k_1,n_1) - \log L(p,k_2,n_2) \big] 
\end{align*}
\noindent
with $p = \frac{k_1 + k_2}{n_1 + n_2}$. The higher the value of $-2log\lambda$ the more significant is the difference between the term frequencies in of both text corpora. By multiplying the $-2log\lambda$ value with the signum of $p_1 - p_2$ we can further distinguish between terms specific to the first corpus and terms specific to the second corpus. Terms that are equally frequent in both corpora have a $-2log\lambda$ value close to zero and thus fall in between.

\paragraph{Example.} Let $C_1$ be the corpus of a software project with size $n_1 = 10^6$, where the words \verb$`rare'$, \verb$`medium'$, and \verb$`common'$ appear respectively 1, 100, and $1\times10^4$ times; and let $C_2$ be the corpus of one of the project's classes with size $n_2 = 1000$, where each word appears 10 times. Then the \loglr{} values are

\begin{center}
\begin{tabular}{l | rrrr}
~ & $p_1$ & $p_2$ & $-2log\lambda$ & $\chi^2$ \\ 
\hline
\verb$rare$ & $10^{-6}$ & $10^{-2}$  & 131.58 & 9.08 \\
\verb$medium$ & $10^{-4}$ & $10^{-2}$  & 71.45 & 0.89 \\
\verb$common$ & $10^{-2}$ & $10^{-2}$  & 0.00 & 0.00 \\
\end{tabular}
\end{center}

The column $\chi^2$ lists the value of Pearson's chi-square test, which assumes normal distribution. As we can see, there is an overestimation when the frequencies involved are very small. Therefore, text analysis should use \loglr{}s to compare the occurrences of common and rare terms \cite{Dunning}.


\section{Applications}\label{applications}

In this section we present two example applications of log-likelihood ratio for software analysis.
There are two main types of corpus comparison: comparison of a sample corpus to a larger corpus, and comparison of a two equally sized corpora. In the first case, we refer to the large corpus as a \emph{normative} corpus since it provides as norm against which we compare.

Applications of these comparisons are
\begin{itemize}
\item \emph{Providing labels for components.} Comparing a component's vocabulary with a large normative corpus (as \eg Sourceforge, Github, or Sourcerer \cite{Bajrach09SUITE}), we obtain labels that describe the component. In the same way, we can compare a class's vocabulary against the containing project.
\item \emph{Comparing components to each other.} Comparing two components, we obtain labels to describe their differences as well as commonalities. This is applicable at any level of granularity, from the level of projects down to the level of methods.
\item \emph{Documenting the history of a component.} Comparing subsequent revisions of the same component, we obtain labels to describe the evolution of that component. (Using multinominal distribution we could even compare all revisions at once, although such results are harder to interpret \cite{Dunning}.)
\item \emph{Describing the results of software search.} Code-search engines have recently received much attention, both commercial engines (as \eg Krugle) and academic engines (as \eg Sourcerer \cite{Bajrach09SUITE}) are publicly available. The result of a search query are typically provided by presenting a peep-hole view on the matching source line and its context to the user. Comparing each result, or the class/project that contains the result, against the entire index we can provide labels that may help users to make better use of these results.
\item \emph{Analysing the structural use of vocabulary.} There has been much confusion regarding which parts of the software vocabulary are to be considered in software analysis. Some approaches consider the entire source code including comments, keywords and literals (\eg \cite{Kuhn08b,Kuhn07a}), other approaches consider class and methods names only (\eg \cite{Baldi08OOPSLA,EinarHoest}). A recent study compared the vocabulary of different structural level using techniques that assume normal distribution \cite{Linstead09SUITE}. Log-likelihood ratios provide a statistically more sound means to study these phenomena.
\end{itemize}

\noindent We implemented $-2log\lambda$ analysis in a Java prototype which is available on the \textsc{Hapax} website\footnote{\url{http://smallwiki.unibe.ch/adriankuhn/hapax}} under AGPL license. In the remainder of this section we present results obtained with that prototype.

\subsection{Labeling the Java API}\label{example1}

\begin{table}
{\scriptsize \begin{center}
\begin{tabular}{lr | lr | lr}
\verb$java.io$ & ~ & \verb$java.text$ & ~ &  \verb$java.util$ & ~ \\
\hline
read & 521.99 & pattern & 228.92 & iterator & 306.91\\
write & 481.93 & format & 209.40 & entry & 301.90\\
skip & 154.61 & digits & 183.24 & next & 237.82\\
close & 113.41 & FIELD & 167.58 & E & 222.33\\
mark & 111.47 & instance & 127.16 & contains & 187.69\\
println & 99.66 & fraction & 104.98 & sub & 166.49\\
UTF & 85.27 & integer & 102.77 & of & 165.57\\
flush & 80.96 & index & 93.43 & K & 154.07\\
desc & 69.19 & run & 90.99 & T & 154.30\\
TC & 68.88 & currency & 91.55 & key & 145.10\\
prim & 61.48 & decimal & 86.34 & all & 145.74\\
char & 61.28 & contract & 84.92 & V & 142.15\\
buf & 60.15 & separator & 72.11 & remove & 128.59\\
stream & 56.86 & grouping & 62.26 & last & 128.09\\
fields & 52.38 & parse & 56.93 & map & 115.68\\
bytes & 47.99 & collation & 56.60 & clear & 114.03\\
\dots & ~ & \dots & ~ & \dots & ~ \\
border & -28.35 & UI & -23.07 & create & -62.67\\
set & -33.89 & border & -22.77 & listener & -62.42\\
remove & -37.49 & property & -24.23 & action & -63.83\\
listener & -39.79 & remove & -30.11 & UI & -64.68\\
accessible & -40.97 & accessible & -32.91 & border & -63.83\\
paint & -44.90 & listener & -31.97 & accessible & -92.25\\
value & -59.10 & type & -32.18 & paint & -101.11\\
get & -64.38 & paint & -36.07 & get & -164.14\\
\end{tabular}
\end{center}}
\caption{Labels retrieved for three Java packages using the full Java 6.0 API as normative corpus.}
\label{tab:one}
\end{table}%


In this example, we compare the packages \verb$java.io$ and \verb$java.text$ and \verb$java.util$ with the normative corpus of the full Java 6.0 API. We use the Java Compiler (JSR 199) to parse the byte-code of the full Java API and then extract the vocabulary of all public and protected elements. We extract the names of packages, classes (including interfaces, annotations, and enums), fields, methods and type parameters. We split the extracted names by camel-case to accomodate to the Java naming convention.

Results are shown in \autoref{tab:one}. For each package we list the most specific words and the least specific words. All three packages are characterized by not covering UI code, in addition \verb$java.io$ and \verb$java.util$ have obviously substantially fewer \verb$get$-accessors than is usual for the Java API. The remaining findings offer no further surprises, except maybe for the uppercase letters in \verb$java.util$ which are generic type parameters; obviously the majority of the Java 6.0 API makes less use of generic types than the collection framework.

%\subsection{Compare Components to Each Other}\label{example2}

%In this example, we compare the \verb$JUnit$ project to the \verb$JExample$ project, both of which are unit testing frameworks. We parse the source code of both projects and extract the complete vocabulary, including comments and literals. We split the extracted words by camel-case to accomodate to the Java naming convention, and apply stemming. We do not, however, exclude common keywords (as \eg \verb$public$ and \verb$float$) since learning about the different use of keywords is a valuable insight as well. 

\subsection{The Evolution of JUnit}\label{example3}

In this example, we report on the vocabulary trends in the history of the \verb$JUnit$\footnote{\url{http://www.junit.org}} project. We use a collection of 14 release distributions of JUnit and parse the source code of each release.  We compare the vocabulary of each two subsequent releases and report on the most significant changes in the vocabulary. We extract all words, including comments; split by camel-case, and exclude English stopwords but not Java keywords.

Results are shown in \autoref{tab:two}. For each release we list the top removed terms and the top added terms, as well as the $-2log\lambda$ value of the top-most term. Large $-2log\lambda$ values indicate substantial changes.

The top 7 change trends (\ie $-2log\lambda \geqslant 100.0$) in the history of JUnit are as follows. In 3.2 removal of \verb$MoneyBag$ example and introduction of graphical UI; in 4.0 removal of graphical UI and introduction of annotation processing; in 4.2 removal of HTML tags from Javadoc comments; in 4.4 introduction of theory matchers and \verb$hamcrest$ framework; in 4.5 introduction of blocks and statements. We manually verified these findings with the release notes of JUnit and found that the findings are appropriate.
 
\begin{table}
{\scriptsize \begin{center}
\begin{tabular}{lrl}
\textbf{JUnit} & $2log\lambda$ & \textbf{Top-10 terms (with $-2log\lambda \geqslant10.0$)} \\
\hline
3 & -8.21 &  \\
~ & 54.11 & count, writer, wrapper \\
\hline
3.2 & -382.80 & money, CHF, assert, case, USD, equals, test, fmb,\\~&~& result, currency \\
~ & 114.21 & tree, model, constraints, combo, reload, swing,\\~&~& icon, pane, browser, text \\
\hline
3.4 & -19.48 & stack, util, button, mouse \\
~ & 15.73 & preferences, base, zip, data, awtui \\
\hline
3.5 & -38.78 & param, reload, constraints \\
~ & 69.34 & view, collector, context, left, cancel, values,\\~&~& selector, views, icons, display \\
\hline
3.6 & -1.20 &  \\
~ & 8.72 &  \\
\hline
3.7 & -8.25 &  \\
~ & 2.79 &  \\
\hline
3.8 & -13.30 & deprecated \\
~ & 23.40 & printer, boo, lines \\
\hline
4.0 & -349.34 & constraints, grid, bag, set, label, panel, path,\\~&~& icon, model, button \\
~ & 350.47 & description, code, nbsp, org, annotation,\\~&~& notifier, method, request, runner, br \\
\hline
4.1 & -1.43 &  \\
~ & 61.90 & link, param, check \\
\hline
4.2 & -288.53 & nbsp, br \\
~ & 20.03 & link, builder, pre, li \\
\hline
4.3.1 & -8.91 &  \\
~ & 53.36 & array, actuals, expecteds, multi, dimensional,\\~&~& arrays, values, javadoc \\
\hline
4.4 & -34.32 & introspector, code, todo, multi, javadoc,\\~&~& dimensional, array, runner, test, fix \\
~ & 151.98 & matcher, theories, experimental, hamcrest,\\~&~& matchers, theory, potential, item, supplier,\\~&~& parameter \\
\hline
4.5 & -30.11 & theory, theories, date, result, static, validator,\\~&~& pointer, string, assert, experimental \\
~ & 124.28 & statement, model, builder, assignment, block,\\~&~& errors, unassigned, evaluate, describable,\\~&~& statements \\
\end{tabular}
\end{center}}
\caption{Evolution of JUnit: for each release we list the removed and the added words, large $2log\lambda$ values indicate more significant changes.}
\label{tab:two}
\end{table}%

%\subsection{On the Structural Use of Vocabulary}\label{example5}

%In this example, we take again the Java API of version 1.6 as case study. We group the extracted names by structural category: package names, class names (which are further subdivided in class and interface names), method names, and field names. As in the first example, we use the Java Compiler API (JSR 199) to parse the binaries and extract the vocabulary of all public and protected elements. We proceed with split by camel-case and stemming.

%\todo{Fill in epic table with results.}

%\todo{Compare the results to Linstead's work.}

\section{Conclusion}\label{eventually}

We presented \loglr as a technique to label software components. Using \loglr{}s allows comparisons to be made between the significance of occurrences of both common and rare terms. We presented how to use the \loglr of word frequencies to retrieve labels, and how to compare a component against a normative corpus. In addition, we proposed to use \loglrs to characterize the evolution of a software component. We presented the results of two example applications, one using the Java API as case study, the other using 14 releases of JUnit as case study. By comparing subsequent revisions of JUnit to each other we were able to characterize substantial changes in the history of JUnit.

\chapter{Spatial Representation with Lexical Clues and Spatial Clues}

To be done

\chapter{Ownership Map with Social and Spatial Clues}

As systems evolve their structure change in ways not expected upfront. As time goes by, the knowledge of the developers becomes more and more critical for the process of understanding the system. That is, when we want to understand a certain issue of the system we ask the knowledgeable developers. Yet, in large systems, not every developer is knowledgeable in all the details of the system. Thus, we would want to know which developer is knowledgeable in the issue at hand. In this paper we make use of the mapping between the changes and the author identifiers (\eg user names) provided by versioning repositories. We first define a measurement for the notion of code ownership. We use this measurement to define the \omap visualization to understand when and how different developers interacted in which way and in which part of the system\footnote{The visualizations in this paper make heavy use of colors. Please obtain a color-printed or electronic version for better understanding.}. We report the results we obtained on several large systems.

Software systems need to change in ways that challenge the original design. Even if the original documentation exists, it might not reflect the code anymore. In such situations, it is crucial to get access to developer knowledge to understand the system. As systems grow larger, not all developers know about the entire system, thus, to make the best use of developer knowledge, we need to know which developer is knowledgeable in which part of the system.

From another perspective, Conway's law \cite{Conw68a} states that ``Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations." That is, the shape of the organization reflects on the shape of the system. As such, to understand the system, one also has to understand the interaction between the developers and the system \cite{Deme02a}.

In this paper we aim to understand how the developers drove the evolution of the system. In particular we provide answers to the following questions:
\begin{itemize}
\item How many authors developed the system?
\item Which author developed which part of the system?
\item What were the behaviors of the developers?
\end{itemize}

In our approach, we assume that the original developer of a line of code is the most knowledgeable in that line of code. We use this assumption to determine the owner of a piece of code (\eg a file) as being the developer that owns the largest part of that piece of code. We make use of the ownership to provide a visualization that helps to understand how developers interacted with the system. The visualization represents files as lines, and colors these lines according to the ownership over time.

Contrary to similar approaches \cite{Ryss04a}, we give a semantic order to the file axis (\ie we do not rely on the names of the files) by clustering the files based on their history of changes: files committed in the same period are related \cite{Gall98a}.

We implemented our approach in Chronia, a tool built on top of the Moose reengineering environment \cite{Duca05a}. As CVS is a de facto versioning system, our implementation relies on the CVS model. Our aim was to provide a solution that gives fast results, therefore, our approach relies only on information from the CVS log without checking out the whole repository. As a consequence, we can analyze large systems in a very short period of time, making the approach usable in the early stages of reverse engineering.

To show the usefulness of our solution we applied it on several large case studies. We report here some of the findings and discuss different facets of the approach.

The contributions of the paper are:
\begin{itemize}
\item The definition of file ownership.
\item The clustering of files based on their commit history.
\item A characterization of developer behaviors.
\item The \omap visualization.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Extraction from CVS log}\label{sec:ownership}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section introduces a measurement to characterize the code ownership. The assumption is that the original developer of a line of code is the most knowledgeable in that line of code. Based on this assumption, we determine the owner of a piece of code as being the developer that owns the most lines  of that piece of code.

The straightforward approach is to checkout all file versions ever committed to the versioning repository and to compute the code ownership from diff information between each subsequent revisions $f_{n-1}$ and $f_n$. From an implementation point of view this implies the transfer of large amounts of data over the network, and long computations.

In this paper, we aim to provide for an approach that can deal with large projects with long history, and that can provide the results fast. As CVS is the very common versioning system, we make tuned our approach to work with the information CVS can provide. In particular we compute the ownership of the code based only on the CVS log information.

Below we present a snippet from a CVS log. The log lists for each version $f_n$ of a file \-- termed revision in CVS \-- the time $t_{f_n}$ of its commit, the name of its author $\alpha_{f_n}$, some state information and finally the number of added and removed lines as deltas $a_{f_n}$ and $r_{f_n}$. We use these numbers to recover both the file size $s_{f_n}$, and the code ownership $own_{f_n}^\alpha$.

\begin{tiny}\begin{verbatim}
----------------------------
revision 1.38
date: 2005/04/20 13:11:24;  author: girba;  state: Exp;  lines: +36 -11
added implementation section
----------------------------
revision 1.37
date: 2005/04/20 11:45:22;  author: akuhn;  state: Exp;  lines: +4 -5
fixed errors in ownership formula
----------------------------
revision 1.36
date: 2005/04/20 07:49:58;  author: mseeberg;  state: Exp;  lines: +16 -16
Fixed math to get pdflatex through without errors.
----------------------------
\end{verbatim}\end{tiny}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measuring File Size}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $s_{f_n}$ be the size of revision $f_n$, measured in number of lines. The number of lines is not given in the CVS log, but can be computed from the deltas $a_{f_n}$ and $r_{f_n}$ of added and removed lines. Even though the CVS log does not give the initial size $s_{f_0}$, we can give an estimate based on the fact that one cannot remove more lines from a file than were ever contained. We define $s_{f_n}$ as in autoref{fig:filesize}: we first calculate the sizes starting with an initial size of 0, and then in a second pass adjust the values with the lowest value encountered in the first pass.

\begin{figure}[htbp]
\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=1.2\linewidth]{minimum}
 \end{minipage}
 \hfill
 \begin{minipage}[c]{.5\linewidth}
\begin{eqnarray}
& s'_{f_0} := 0 \nonumber \\
& s'_{f_n} := s'_{f_{n-1}} + a_{f_{n-1}} - r_{f_n} \nonumber  \\
& s_{f_0} := \vert min \{ s'_x \} \vert \nonumber \\
& s_{f_n} := s_{f_{n-1}} + a_{f_n} - r_{f_n} \nonumber
 \end{eqnarray}
 \end{minipage}
\caption{The computation of the initial size.}
\label{fig:filesize}
\end{figure}

This is a pessimistic estimate, since lines that never changed are not covered by the deltas in the CVS log. This is an acceptable assumption since our main focus is telling the story of the developers, not measuring lines that were never touched by a developer. Furthermore in a long-living system the content of files is entirely replaced or rewritten at least once if not several times. Thus the estimate matches the correct size of most files.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measuring Code Ownership}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A developer owns a line of code if he was the last one that committed a change to that line. In the same way, we define file ownership as the percentage of lines he owns in a file. And the overall owner of a file is the developer that owns the largest part of it.

Let $own_{f_n}^\alpha$ be the percentage of lines in revision $f_n$ owned by author $\alpha$. Given the file size $s_{f_n}$, and both the author $\alpha_{f_n}$ that committed the change and $a_{f_n}$ the number of lines he added, we defined ownership as:
\[
own_{f_0}^\alpha:=\left\{
    \begin{array}{cl}
        1, & \alpha=\alpha_{f_0} \\
        0, & else
    \end{array}\right.
\]
\[
own_{f_n}^\alpha:=own_{f_{n-1}}^\alpha \frac{s_{f_n} - a_{f_n}}{s_{f_n}} + \left\{
    \begin{array}{cl}
        \frac{a_{f_n}}{s_{f_n}}, & \alpha=\alpha_{f_n} \\
        0, & else
    \end{array}\right.
\]
In the definition we assume that the removed lines $r_{f_n}$ are evenly distributed over the ownership of the preceding owners of $f_{n-1}$.
%A better estimate than $own_{f_n}^\alpha$ can be retrieved by checking out the content of each revision and using a diff algorithm to find out to whom the removed lines actually belonged. But this would, as initially explained, require vast amounts of network traffic and time consuming calculations, and thus the advantages of only processing information from CVS log would be lost.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Ownership Map View}\label{sec:approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We introduce the \omap visualization as in autoref{fig:ownershipDetailsExample}. The visualization is similar to the Evolution Matrix \cite{Lanz02a}: each line represents a history of a file, and each circle on a line represents a change to that file.

The color of the circle denotes the author that made the change. The size of the circle reflects the proportion of the file that got changed \ie the larger the change, the larger the circle. And the color of the line denotes the author who owns most of the file.

Bertin \cite{Bert74a} assessed that one of the good practices in information visualization is to offer to the viewer visualizations that can be grasped at one glance. The colors used in our visualizations follow visual guidelines suggested by Bertin, Tufte \cite{Tuft90a}, and Ware \cite{Ware00a} \-- \eg we take into account that the human brain is not capable of processing more than a dozen distinct colors.

In a large system, we can have hundreds of developers. Because the human eye is not capable of distinguishing that many colors, we only display the authors who committed most of all changes using distinct colors; the remaining authors are represented in gray. Furthermore, we also represent with gray files that came into the CVS repository with the initial import, because these files are usually sources from another project with unknown authors and are thus not necessarily created by the author that performed the import. In short, a gray line represents either an unknown owner, or an unimportant one.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=\linewidth]{owners-map-detail}
\caption{Example of ownership visualization of two files.}
\label{fig:ownershipDetailsExample}
\end{center}
\end{figure}

In the example from autoref{fig:ownershipDetailsExample}, each line represents the lifetime of a file; each circle represents a change. \id{File A} appears gray in the first part as it originates from the initial import. Later the green author significantly changed the file, and he became the owner of the file. In the end, the blue author deleted the file. \id{File B} was created by the green author. Afterwards, the blue author changed the file, but still the green author owns the larger part, so the line remains green. At some point, the red author committed a large change and took over the ownership. The file was not deleted.

\begin{figure*}[hbt]
\begin{center}
\includegraphics{owners-map-example}
\caption{Example of the Ownership Map view. The view reveals different patterns:
Monologue, Familiarization, Edit, Takeover, Teamwork, Bug-fix.}
\label{fig:ownershipMapExample}
\end{center}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ordering the Axes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Ordering the Time Axis.}
Subsequent file revisions committed by the same author are grouped together to form a transaction of changes \ie a commit. We use a single linkage clustering with a threshold of 180 seconds to obtain these groups. This solution is similar to the sliding time window approach of Zimmerman \etal when they analyzed co-changes in the system \cite{Zimm04a}. The difference is that in our approach the revisions in a commit do not have to have the same log comment, thus any quick subsequent revisions by the same author are grouped into one commit.

\paragraph{Ordering the Files Axis.}
A system may contain thousands of files; furthermore, an author might change multiple files that are not near each other if we would represent the files in an alphabetical order. Likewise, it is important to keep an overview of the big parts of the system. Thus, we need an order that groups files with co-occurring changes near each other, while still preserving the overall structure of the system. To meet this requirement we split the system into high-level modules (\eg the top level folders), and order inside each module the files by the similarity of their history. To order the files in a meaningful way, we define a distance metric between the commit signature of files and order the files based on a hierarchical clustering.

Let $H_f$ be the commit signature of a file, a set with all timestamps $t_{f_n}$ of each of its revisions $f_n$. Based on this the distance between two commit signatures $H_a$ and $H_b$ can be defined as the modified Hausdorff distance \footnote{The Hausdorff metric is named after the german mathematician Felix Hausdorff (1868-1942) and is used to measure the distance between two sets with elements from a metric space.} $\delta(H_a,H_b)$:
\[
D(H_n,H_m) := \sum_{n \in H_n} min^2 \{ \vert m -n \vert : m \in H_m \}
\]
\[
\delta(H_a,H_b) := max \{ D(H_a,H_b), D(H_b,H_a) \}
\]

With this metric we can order the files according to the result of a hierarchical clustering algorithm \cite{Jain99a}. From this algorithm a dendrogram can be built: this is a hierarchical tree with clusters as its nodes and the files as its leaves. Traversing this tree and collecting its leaves yields an ordering that places files with similar histories near each other and files with dissimilar histories far apart of each other.

The files axes of the \omap views shown in this paper are ordered with \textit{average linkage} clustering and \textit{larger-clusters-first} tree traversal. Nevertheless, our tool Chronia allows customization of the ordering.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Behavioral Patterns}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Overview Map reveals semantical information about the work of the developer. autoref{fig:ownershipMapExample} shows a part of the \omap of the Outsight case study (for more details see  autoref{sec:outsight}). In this view we can identify several different behavioral patterns of the developers:

\begin{itemize}

\item \emph{Monologue.} Monologue denotes a period where all changes and most files belong to the same author. It shows on an \omap as a unicolored rectangle with change circles in the same color.

\item \emph{Dialogue.} As opposed to Monologue, Dialogue denotes a period with changes done by multiple authors and mixed code ownership. On an \omap it is denoted by rectangles filled with circles and lines in different colors.

\item \emph{Teamwork.} Teamwork is a special case of Dialogue, where two or more developers commit a quick succession of changes to multiple files. On an \omap it shows as circles of alternating colors looking like a bunch of bubbles. In our example, we see in the bottom right part of the figure a collaboration between Red and Green.

\item \emph{Silence.} Silence denotes an uneventful period with nearly no changes at all. It is visible on an \omap as a rectangle with constant line colors and none or just few change circles.

\item \emph{Takeover.} Takeover denotes a behavior where a developer takes over a large amount of code in a short amount of time \-- \ie the developer seizes ownership of a subsystem in a few commits. It is visible on an \omap as a vertical stripe of single color circles together with an ensuing change of the lines to that color. A Takeover is commonly followed by subsequent changes done by the same author. If a Takeover marks a transition from activity to Silence we classify it as an \emph{Epilogue}.

\item \emph{Familiarization.} As opposed to Takeover, Familiarization characterizes an accommodation over a longer period of time. The developer applies selective and small changes to foreign code, resulting in a slow but steady acquisition of the subsystem. In our example, Blue started to work on code originally owned by Green, until he finally took over ownership.

\item \emph{Expansion.} Not only changes to existing files are important, but also the expansion of the system by adding new files. In our example, after Blue familiarized himself with the code, he began to extend the system with new files.

\item \emph{Cleaning.} Cleaning is the opposite of expansion as it denotes an author that removes a part of the system. We do not see this behavior in the example.

\item \emph{Bugfix.} By bug fix we denote a small, localized change that does not affect the ownership of the file. On an \omap it shows as a sole circle in a color differing from its surrounding.

\item \emph{Edit.} Not every change necessarily fulfills a functional role. For example, cleaning the comments, changing the names of identifiers to conform to a naming convention, or reshaping the code are sanity actions that are necessary but do not add functionality. We call such an action \emph{Edit}, as it is similar to the work of a book editor. An Edit is visible on an \omap as a vertical stripe of unicolored circles, but in difference to a Takeover neither the ownership is affected nor is it ensued by further changes by the same author. If an Edit marks a transition from activity to Silence we classify it as an \emph{Epilogue}.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Validation}\label{sec:validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We applied our approach on several large case studies: Outsight, Ant, Tomcat, JEdit and JBoss. Due to the space limitations we report the details from the Outsight case study, and we give an overall impression on the other four well-known open-source projects.

\textbf{Outsight.} Outsight is a commercial web application written in Java and JSP. The CVS repository goes back three years and spans across two development iterations separated by half a year of maintenance. The system is written by four developers and has about 500 Java classes and about 500 JSP pages.

\textbf{Open-source Case Studies.} We choose Ant, Tomcat, JEdit, and JBoss to illustrate different fingerprints systems can have on an \omap. Ant has about 4500 files, Tomcat about 1250 files, JEdit about 500 files, and JBoss about 2000 files. The CVS repository of each project goes back several years.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Outsight}\label{sec:outsight}
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\columnwidth]{commit-histogram}
\caption{Number of commits per team member in periods of three months.}
\label{fig:histogram}
\end{center}
\end{figure}

\begin{figure*}[htbp]
\begin{center}
\includegraphics[height=21.3cm]{outsight}
\caption{The Ownership Map of the Outsight case study.}
\label{fig:casestudy-outsight}
\end{center}
\end{figure*}

The first step to acquire an overview of a system is to build a histogram of the team to get an impression about the fluctuations of the team members over time. autoref{fig:histogram} shows that a team of four developers is working on the system. There is also a fifth author contributing changes in the last two periods only.



autoref{fig:casestudy-outsight} shows the \omap of our case study. The upper half are Java files, the bottom half are JSP pages. The files of both modules are ordered according to the similarity of their commit signature. For the sake of readability we use \id{S1} as a shorthand for the Java files part of the system, and \id{S2} as a shorthand for the JSP files part. Time is cut into eight periods \id{P1} to \id{P8}, each covering three months. The paragraphs below discuss each period in detail, and show how to read the \omap in order to answer our initial questions.

The shorthands in parenthesis denote the labels \id{R1} to \id{R15} as given on autoref{fig:casestudy-outsight}.

\textbf{Period 1.} In this period four developers are working on the system. Their collaboration maps the separation of \id{S1} and \id{S2}: while Green is working by himself on \id{S2} (\id{R5}), the others are collaborating on \id{S1}. This is a good example of Monologue versus Dialogue. A closer look on \id{S1} reveals two hotspots of Teamwork between Red and Cyan (\id{R1,R3}), as well as large mutations of the file structure. In the top part multiple Cleanings happen (\id{R2}), often accompanied by Expansions in the lower part.

\textbf{Period 2.} Green leaves the team and Blue takes over responsibility of \id{S2}. He starts doing this during a slow Familiarization period (\id{R6}), which lasts until end of \id{P3}. In the meantime Red and Cyan continue their Teamwork on \id{S1} (\id{R4}) and Red starts adding some files, which foreshadow the future Expansion in \id{P3}.

\textbf{Period 3.} This period is dominated by a big growth of the system, the number of files doubles as large Expansions happen in both \id{S1} and \id{S2}. The histogram in autoref{fig:histogram} identifies Red as the main contributor. The Expansion of \id{S1} evolves in sudden steps (\id{R9}), and as their file base grows the Teamwork between Red and Cyan becomes less tight. In contradiction the Expansion of \id{S2} evolves in small steps (\id{R8}), as Blue continues familiarizing himself with \id{S2} and slowly but steady takes over ownership of most files in this subsystem (\id{R6}). Also an Edit of Red in \id{S2} can be identified (\id{R7}).

\textbf{Period 4.} Activity moves down from \id{S1} to \id{S2}, leaving \id{S1} in a Silence only broken by selective changes. autoref{fig:histogram} shows that Red left the team, which consists now of Cyan and Green only. Cyan acts as an allrounder providing changes to both \id{S1} and \id{S2}, and Blue is further working on \id{S2}. The work of Blue culminates in an Epilogue marking the end of this period (\id{R8}). He has now completely taken over ownership of \id{S2}, while the ownership of subsystem \id{S1} is shared between Red and Cyan.

\textbf{Period 5 and 6.} Starting with this period the system goes into maintenance. Only small changes occur, mainly by author Blue.

\textbf{Period 7.} After two periods of maintenance the team resumes work on the system. In autoref{fig:histogram} we see how the composition of the team changed: Blue leaves and Green comes back. Green restarts with an Edit in \id{S2} (\id{R11}), later followed by a quick sequence of Takeovers (\id{R13}) and thus claiming back the ownership over his former code. Simultaneous he starts expanding \id{S2} in Teamwork with Red (\id{R12}).

First we find in \id{S1} selective changes by Red and Cyan scattered over the subsystem, followed by a period of Silence, and culminating in a Takeover by Red in the end \ie an Epilogue (\id{R14}). The Takeover in \id{S1} stretches down into \id{S2}, but there being a mere Edit. Furthermore we can identify two selective Bug-fixes (\id{R10}) by author Yellow, being also a new team member.

\textbf{Period 8.} In this period, the main contributors are Red and Green: Red works in both \id{S1} and \id{S2}, while green remains true to \id{S2}. As Red finished in the previous period his work in \id{S1} with an Epilogue, his activity now moves down to \id{S2}. There we find an Edit (\id{R15}) as well as the continuation of the Teamwork between Red and Green (\id{R12}) in the Expansion started in \id{P7}. Yet again, as in the previous period, we find small Bug-fixes applied by Yellow.

To summarize these finding we give a description of each author's behavior, and in what part of the system he is knowledgeable.

\textbf{Red author.} Red is working mostly on \id{S1}, and acquires in the end some knowledge of \id{S2}. He commits some edits and may thus be a team member being responsible for ensuring code quality standards. As he owns a good part of \id{S1} during the whole history and even closed that subsystem end of \id{P7} with an Epilogue, he is the developer most knowledgeable in \id{S1}.

\textbf{Cyan author.} Cyan is the only developer that was in the team during all periods, thus he is the developer most familiar with the history of the system. He worked mostly on \id{S1} and he owned large parts of this subsystem till end of \id{P7}. His knowledge of \id{S2} depends on the kind of changes Red introduced in his Epilogue. A quick look into the CVS log messages reveals that Red's Epilogue was in fact a larger than usual Edit and not a real Takeover: Cyan is as knowledgeable in \id{S1} as Red.

\textbf{Green author.} Green only worked in \id{S2}, and he has only little impact on \id{S1}. He founded \id{S2} with a Monologue, lost his ownership to Blue during \id{P2} to \id{P6}, but in \id{P7} he claimed back again the overall ownership of this subsystem. He is definitely the developer most knowledgeable with \id{S2}, being the main expert of this subsystem.

\textbf{Blue author.} Blue left the team after \id{P4}, thus he is not familiar with any changes applied since then. Furthermore, although he became an expert of \id{S2} through Familiarization, his knowledge might be of little value since Green claimed that subsystem back with multiple Takeovers and many following changes.

\textbf{Yellow author.} Yellow is a pure Bug-fix provider.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ant, Tomcat, JEdit and JBoss}

\begin{figure*}[htb]
\begin{center}
\includegraphics[width=12.5cm]{casestudies-overview}
\caption{The Ownership Map of Ant, Tomcat, JEdit, and JBoss.}
\label{fig:casestudies-owerview}
\end{center}
\end{figure*}

autoref{fig:casestudy-outsight} shows the \omap of four open-source projects: Ant, Tomcat, JEdit, and JBoss. The views are plotted with the same parameters as the map in the previous case study, the only difference is that vertical lines slice the time axis into periods of twelve instead of three months. Ant has about 4'500 files with 60'000 revisions, Tomcat about 1'250 files and 13'000 revisions, JEdit about 500 files and 11'000 revisions, and JBoss about 2'000 files with 23'000 revisions.

Each view shows a different but common pattern. The paragraphs below discuss each pattern briefly.

\textbf{Ant.} The view is dominated by a huge Expansion. After some time of development, the very same files fall victim to a huge Cleaning. This pattern is found in many open-source projects: Developers start a new side-project and when grown up it moves to an own repository, or the side-project ceases and is removed from the repository. In this case, the spin-off is the Myrmidon project, a ceased development effort planned as successor to Ant.

\textbf{Tomcat.} The colors in this view are, apart from some large blocks of Silence, well mixed. The \omap shows much Dialogue and hotspots with Teamwork. Thus this project has developers that  collaborate well.

\textbf{JEdit.} This view is dominated by one sole developer, making him the driving force behind the project. This pattern is also often found in open-source projects: being the work of a single author that contributed about 80\% of the code.

\textbf{JBoss.} The colors in this view indicate that the team underwent to large fluctuations. We see twice a sudden change in the colors of both commits and code ownership: once mid 2001 and once mid 2003. Both changes are accompanied by Cleanings and Expansions. Thus the composition of the team changed twice significantly, and the new teams restructured the system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{On the exploratory nature of the implementation.} We implemented our approach in Chronia, a tool built on top of the Moose reengineering environment \cite{Duca05a}. autoref{fig:chronia} emphasizes the interactive nature of our tool.

On the left of autoref{fig:chronia} we see Chronia visualizing the overall history of the project, which provides a first overview. Since there is too much data we cannot give the reasoning only from this view, thus, Chronia allows for interactive zooming. For example, in the window on the lower right, we see Chronia zoomed into the bottom right part of the original view. Furthermore, when moving the mouse over the \omap, we complement the view by also showing the current position on both time and file axis are highlighted in the lists on the right. These lists show all file names and the timestamps of all commits. As Chronia is build on top of Moose, it makes use of the Moose contextual menus to open detailed views on particular files, modules or authors. For example, in the top right window we see a view with metrics and measurements of a file revision.

\begin{figure*}[htbp]
\begin{center}
\includegraphics{chronia}
\caption{Chronia is an interactive tool.}
\label{fig:chronia}
\end{center}
\end{figure*}

\textbf{On the scalability of the visualization.} Although Chronia provides zooming interaction, one may lose the focus on the interesting project periods. A solution would be to further abstract the time and group commits to versions that cover longer time periods. The same applies to the file axis grouping related files into modules.

\textbf{On the decision to rely on CVS log only.} Our approach relies only on the information from the CVS log without checking out the whole repository. There are two main reasons for that decision.

First, we aim to provide a solution that gives fast results; \eg building the \omap of JBoss takes 7,8 minutes on a regular 3 GHz Pentium 4 machine, including the time spent fetching the CVS log information from the \textit{Apache.org} server.

Second, it is much easier to get access to closed source case studies from industry, when only metainformation is required and not the source code itself. We consider this an advantage of our approach.

\textbf{On the shortcomings of CVS as a versioning system.} As CVS lacks support for true file renaming or moving, this information is not recoverable without time consuming calculations. To move a file, one must remove it and add it later under another name. Our approach identifies the author doing the renaming as the new owner of the file, where in truth she only did rename it. For that reason, renaming directories impacts the computation of code ownership in a way not desired.

\textbf{On the perspective of interpreting the \omap.} In our visualization we sought answers to questions regarding the developers and their behaviors. We analyzed the files from an author perspective point of view, and not from a file perspective of view. Thus the \omap tells the story of the developers and not of the files \eg concerning small commits: subsequent commits by different author to one file do not show up as a hotspot, while a commit by one author across multiple files does. The later being the pattern we termed \textit{Edit}.

Also from a project manager point of view the \omap can give valuable hints. Knowing whether a developer tends more to  Takeover or more to Familiarization is a good indicator to whom the responsibility of subsystem should be given. If a subsystem need rewrites and restructuring the Takeover type is a good choice, otherwise if a subsystem is a good base to be built up on the Familiarization type is a good choice.

But classifications of the authors have to be interpreted in their context. If a developer ly takes over subsystems this does not mean that he has an  character or that he will always tend to Takeovers. In our case study (autoref{fig:casestudy-outsight}) Green's Takeover of \id{S2} in \id{P7} must be seen in the context of the system history: Blue left the team and Green was the original developer of \id{S2}. He would have acted differently if Blue were still in the team.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper we aim to understand how the developers drove the evolution of the system. In particular we ask the following questions:
\begin{itemize}
\item How many authors developed the system?
\item Which author developed which part of the system?
\item What were the behaviors of the developers?
\end{itemize}

To answer them, we define the \omap visualization based on the notion of code ownership. In addition we semantically group files that have a similar \emph{commit signature} leading to a visualization that is not  based on alphabetical ordering of the files but on co-change relationships between the file histories. The \omap helps in answering which authors are knowledgeable in which part of the system and also reveal behavioral patterns. To show the usefulness we implemented the approach and applied it on several case studies. We reported some of the findings and we discussed the benefits and the limitations as we perceived them during the experiments.

In the future, we would like to investigate the application of the approach at other levels of abstractions besides files, and to take into consideration types of changes beyond just the change of a line of code.

\section{Bug-Report Triage with Lexical Clues and Social Clues}

For popular software systems, the number of daily submitted bug reports is high. Triaging these incoming reports is a time consuming task. Part of the bug triage is the assignment of a report to a developer with the appropriate expertise. In this paper, we present an approach to automatically suggest developers who have the appropriate expertise for handling a bug report. We model developer expertise using the vocabulary found in their source code contributions and compare this vocabulary to the vocabulary of bug reports. We evaluate our approach by comparing the suggested experts to the persons who eventually worked on the bug. Using eight years of Eclipse development as a case study, we achieve 33.6\% top-1 precision and 71.0\% top-10 recall.

\todo{Do not forget to remove the page numbers in the camera-ready version!}
Software repositories of large projects are typically accompanied by a bug report tracking system. In the case of popular open source software systems, the bug tracking systems receive an increasing number of reports daily. The task of triaging the incoming reports therefore consumes an increasing amount of time \cite{Anvi06b}. One part of the triage is the assignment of a report to a developer with the appropriate expertise. Since in large open source software systems the developers typically are numerous and distributed, finding a developer with a specific expertise can be a difficult task.

Expertise models of developers can be used to support the assignment of developers to bug reports. It has been proposed that tasks such as bug triage can be improved if an externalized model of each programmer's expertise of the code base is available \cite{Frit07a}. Even though approaches for expertise models based on software repository contributions are available, existing recommendation systems for bug assignment typically use expertise models based on previous bug reports only \cite{Anvi06a,Canf05a,Cubr04b,Mock02b,Lucc02a}.
Typically a classifier is trained with previously assigned bug reports, and is then used to classify and assign new, incoming bug reports.
Our approach differs, we train our recommendations system with all source code contributions up to the reporting date of the bug and then use the bug reports textual description as a search query against the expertise model's \TAM.

In this paper, we propose an expertise model based on source code contributions and apply in it a recommendation system that assigns developers to bug reports. We compare \VOC found in the \verb$diff$s of a developer's contributions with the vocabulary found in the description of a bug report. We then recommend developers whose contribution vocabulary is lexically similar to the vocabulary of the \BR.

We implemented our approach as a prototype called \TOOL\footnote{\TOOL is open source, written in Smalltalk, and available at \url{http://smallwiki.unibe.ch/develect}. The name \emph{develect} is a portmanteau word of \emph{developer} and \emph{dialect}. }, and evaluate the recommendation system using the Eclipse project as a case study. We develop and calibrate our approach on a \trainingset of bug reports. Then we report the results of evaluating it on a set of reports of the remaining case study.

The contributions of this paper are as follows:
\begin{itemize}
\item We propose a novel expertise model of developers. The approach is based on the vocabulary found in the source code contributions of developers.

\item We propose a recommendation system that applies the above expertise model to assign developers to bug reports. We evaluate the system using eight years of Eclipse development as a case study. 

\item We report on the decay of developer expertise, observed when calibrating our approach. We apply two weighting schemes to counter this effect.
\end{itemize}

%-------------------------------------------------------------------------
\section{Our Approach in a Nutshell}\label{sec:nutshell}

\begin{figure}
    \includegraphics[width=\linewidth]{FlowOfAlgo}
    \caption{Architecture of the \TOOL recommendation system: (left) bug reports and versioning repositories are processed to produce term vectors and \TAMS; (right) the cosine angle between vector and matrix columns is taken to rank developers in the suggested list of experts.}
    \label{fig:algo}
\end{figure} 

In this paper we present i) the construction of an expertise model of developers and ii) the application of a recommendation system that uses this expertise model to automatically assign developers to bug reports. Our approach requires a versioning repository to construct the expertise model as well as a bug tracking facility to assign developers to bug reports. We realized the approach as a prototype implementation, called \TOOL.

In his work on automatic bug report assignment, John Anvik proposes eight types of information sources to be used by a recommendation system \cite{Anvi06b}. Our recommendation system focuses on three of these types of information: the information of the textual description of the bug (type~1), the one of the developer who owns the associated code (type~6), and the information of the list of developers actively contributing to the project (type~8). We refine information type~6 to take into account the textual content of the code owned by a developer. Our recommendation system is based on an expertise model of the developer's source code contributions. For each developer, we count the textual word frequencies in their change sets. This includes deleted code and context lines, assuming that any kind of change (even deletions) requires developer knowledge and thus familiarity with the vocabulary. 

Our system currently does not consider the component the bug is reported for (type~2), the operation system that the bug occurs on (type~3), the hardware that the bug occurs on (type~4), the version of the software the bug was observed for (type~5), or the current workload of the developers (type~7). Information types 2--4 are indirectly covered, since textual references to the component, operation system, or hardware are taken into account when found in bug reports or source code. Information of type~7 is typically not publicly available for open source projects and thus excluded from our studies. Furthermore, we deliberately disregard information of type~5, since developer knowledge acquired in any version pre-dating the bug report might be of use. 

Given a software system with a versioning repository, the creation of the expertise model works as illustrated in \autoref{fig:algo}:

\begin{enumerate}
\item For each contributor to the versioning repository, we create an empty bag of words. 
\item For each contribution to the versioning repository, we create a \verb$diff$ of all changed files and count the word frequencies in the diff files. We assign the word frequencies to the contributor's bag of words.
\item We create a \TAM $M_{n \times m}$, where $n$ is the global number of words and $m$ the number of contributors. Each entry $m_{i,j}$ equals the frequency of the word $t_i$ in the contributions of the contributor $a_j$. 
\end{enumerate}

Given the above \TAM and a bug tracking facility, the assignment of developers works as follows:

\begin{enumerate}
\item We count the word frequencies in the bug report and create a query vector of length $n$ where $v_i$ equals the frequency of word $t_i$ in the bug report. 
\item For each developer in the \TAM, we take the cosine of the angle between the query vector and the developer's column of the matrix.
\item We rank all developers by their lexical similarity and suggest the top $k$ developers.
\end{enumerate}

To evaluate our approach we use Eclipse as a case study. We train our system with weekly summaries of all CVS commits from 2001 to 2008, and use the resulting expertise-model to assign developers to bug reports. We evaluate precision and recall of our approach by comparing the suggested developers to the persons who eventually worked on the bug and its report.

For example, for a report that was submitted in May 2005, we would train our system with all commits up to April. Then we would evaluate the suggested developers against those persons who worked on the bug and its report in May or later on to see if they match.

 

\section{The Develect Expertise Model}\label{sec:algorithm}

Given the natural language description of a bug report, we aim to find the developer with the best expertise regarding the content of the bug report. For this purpose, we model developer expertise using their source code contributions. 

Developers gain expertise by either writing new source code or working on existing source code. Therefore we use the vocabulary of source code contributions to quantify the expertise of developers. Whenever a developer writes new source code or works on existing source code, the vocabulary of mutated lines (and surrounding lines) are added to the expertise model. These lines are extracted from the \VCS using the \verb$diff$ command.

Natural language documents differ from source code in their structure and grammar, thus we treat both kind of documents as unstructured bags of words. We use Information Retrieval techniques to match the word frequencies in bug reports to the word frequencies in source code. This requires that developers use meaningful names \eg for variables and methods, which is the case given modern naming conventions~\cite{Kuhn07a}. 

Given a bug report, we rank the developers by their expertise regarding the bug report. The expertise of a developer is given by the lexical similarity of his vocabulary to the content of the bug report.

\subsection{Extracting the Vocabulary of Developers from Version Control}

To extract the vocabulary of a specific developer we must know which parts of the source code have been authored by which developer. 
%read and changed by this developer !!!

We extract the vocabulary in two steps, first building a \textsc{Chronia} model of the entire repository \cite{Girb05c} and then collecting word frequencies from the \verb$diff$ of each revisions. %Depending on the particular versioning system, it can require an additional effort to identify revisions. For example, in CVS version numbers are associated with single files instead of the entire repository.
The \verb$diff$ command provides a line-by-line summary of the changes between two versions of the same file. The diff of a revision summarizes the changes made to all files that changed in that revision of the repository. The identifier names and comments that appear on these lines give us evidence of the contributor's expertise in the system. Thus, we add the word frequencies in a revision's \verb$diff$ to the expertise model of the contributing developer.

Word frequencies are extracted as follows: the lines are split into sequences of letters, which are further split at adjacent lower- and uppercase letters to accommodate the common \emph{camel case} naming convention. Next stopwords are removed (\ie common words such as \emph{the}, \emph{and}, etc). Eventually stemming %\cite{Port80a}
is applied to remove the grammatical suffix of words.

The comment message associated with a revision is processed in the same way, and added to the expertise model of the contributing developer as well. 
%This is done on a per file basis: the comment message of a commit including many files is weighted higher than the comment message of a single file commit.

%\todo{Add conclusional paragraph which sums up this subsection}

\subsection{Modeling the Expertise of Developers as a Term-Author-Matrix}

We store the expertise model in a matrix that correlates word frequencies with developers. We refer to this matrix as a \TAM. Although, technically it is a term-document-matrix where the documents are developers. (It is common in Information Retrieval to describe documents as bags of words, thus our model is essentially the same, with developers standing in for documents.)

The \TAM has dimension $n \times m$, where $n$ is the global number of words and $m$ the number of contributors, that is developers. Each entry $m_{i,j}$ equals the frequencies of the word $t_i$ summed up over all source code contributions provided by developer $a_j$.
%Furthermore, we weight the matrix with \emph{tf-idf} (term frequencyinverse document frequency) to balance rare and common words.

We have found that results improve if the \TAM is weighted as follows:

\begin{itemize}
\item \emph{Decay of Vocabulary.} For each revision, the word frequencies are weighted by a decay factor that is proportional to the age of the revision. In the Eclipse case study, best results are obtained with a weighting of $3\%$ per week (which accumulates to $50\%$ per half year and $80\%$ per annum). Please refer to \autoref{sec:discussion} for a detailed discussion.
\end{itemize}

\subsection{Assign Developers to Bug Reports regarding their Expertise}

To assign developers to bug reports, we use the bug report's textual content as a search query to the \TAM.
%Our current tool supports the extraction of bug reports from Bugzilla, however our approach works with any textual representation of bug reports, even with raw text.
Given a Bugzilla\footnote{http://www.bugzilla.org} bug report, we count the word frequencies in its textual content. In particular we process both short and all long descriptions (threats to validity see \autoref{sec:discussion}).
\todo{ XXX, mail this to migod! }
We disregard attachments that are Base-64 encoded, such as attached images, as well as fields that refer to persons. From the extracted word frequencies, we create a \emph{term vector} that uses the same word indices as the \TAM. % The lexical similarity between term vectors is obtained using the cosine between both vectors.
We then compute the lexical similarity between two term vectors by taking the cosine of the angle between them.
The similarity values range from $1.0$ for identical vectors to $0.0$ for vectors without shared terms. (Negative similarity values are not possible, since word-frequencies cannot be negative either.)

We compare the term vector of the bug report with the term vectors of all developers (\ie the columns of the \TAM) and create a ranking of developers. For the assignment of bug reports to developers, a suggestion list of the top-$k$ developers with the highest lexical similarities is then provided.

We have found that the results improve if the \TAM is further weighted as follows:
\begin{itemize}
\item \emph{Inactive Developer Penalty.} If a developer has been inactive for more than three months, the lexical similarity is decreased by a penalty proportional to the time since his latest contribution. In the Eclipse case study, best results are obtained with a penalty of $0.2$ per annum. Please refer to \autoref{sec:discussion} for a detailed discussion.
\end{itemize}

\section{Case Study: \EC platform}\label{sec:casestudy}

To evaluate our approach we take Eclipse\footnote{http://www.eclipse.org/eclipse} as a case study. \EC is a large open source software project with numerous active developers.
\EC has been developed over several years now. Therefore, its version repository contains a great deal of source code developed by many different authors.
% a lot of source code of different authors can be obtained from its versioning system CVS.
Furthermore, \EC uses Bugzilla as its bug tracking system, storing \BRs dating back to nearly the beginning of the project.
% where \BRs dated from today until almost the beginning of the development are stored.
%These circumstances make \EC a project well suited to extract large \VOC data of many different authors from and to query that data with also from \EC extracted \BRs. Since such a query should yield the author with the best expertise regarding the bug report,
We evaluate our results by comparing the top-$k$ developers with the persons who eventually worked on the bug report.

Our case study covers the Eclipse project between April 22, 2001, and November 9, 2008. The source code contributions of Eclipse are stored in a CVS repository\footnote{:pserver:anonymous@dev.eclipse.org/cvsroot/eclipse}, the bug reports in a Bugzilla database\footnote{https://bugs.eclipse.org/bugs}. This represents almost eight years of development, including 130,769 bug reports and 162,942 global revisions (obtained from CVS's file versions using a sliding time-window of 2 minutes \cite{Zimm04a}). During this time, 210 developers contributed to the project.

\subsection{Setup of the Case Study}

The setup of the Eclipse case study consists of two different parts. The first part is about the change database, here we use all changes before the actual bug report. The second part is about the bug database, here we make 10 partitions of which two are used in this case study. We process and evaluate  both parts in weekly iterations as follows:
\begin{itemize}
\item We create a \TOOL expertise model based on contributions between April 22, 2001, and the last day of the previous week.
\item We generate a suggested list of the top-10 experts for all bug reports submitted in the current week.
\item We evaluate precision and recall by comparing the suggestion list with the developers who, between the first day of the next week and November 9, 2008, eventually worked on the bug report.
\end{itemize}

For example, for a bug report submitted on May 21, 2005, we would train our system with all commits between April 22, 2001 and May 15, 2005, and then evaluate the list of suggested experts against the set of developers who, between May 23, 2005, and November 9, 2008, eventually handled the bug report.

We use systematic sampling to create 10 partitions of 13,077 bug reports (ordered by time) that span the entire time of the project. One partition is used as \trainingset for the development of our approach, and another partition is used as \validationset to validate our approach. We applied the approach to the \validationset only after all implementation details and all calibration parameters had been finally decided on. The other partitions remain untouched for use as \validationset in future work.

In this section, we report on our results obtained on the validation partition \#2. In \autoref{sec:discussion} we report on results obtained from the training partition \#1 while calibrating the approach.

%We developed and calibrated our approach on just the first partition. Only after all implementation details and all calibration parameters had been finally decided on, did we apply the approach to the remaining partitions.

%\subsection{Settings of the Case Study}

%The results of the evaluation in this section are obtained by running \TOOL with the following settings:
%{\footnotesize \begin{verbatim}
%  project = Eclipse
%  stemming = Porter
%  scanner = CamelCaseScanner
%  stopwords = English
%  similarity = Cosine
%  TAM weighting = None
%  use LSI = false
%  added lines weighting = 1.0
%  removed lines weighting = 1.0
%  context lines weighting = 1.0
%  comments weighting = 1.0
%  decay of vocabulary (per week) = 0.97
%  inactive developer penalty (per annum) = 0.2
%  excluded fields = Persons, Base64
%  weekly diffs = true
%  co-developer weighting = Sqrt
%  start date = 2001-04-22
%  end date = 2008-11-02
%  exclude unmapped logins = true
%  partition (1->validation, else->test) = 2
%\end{verbatim}}

\subsection{Precision and Recall}

We evaluate our approach by comparing the suggested list of experts with the developers who eventually worked on the bug report. We report on precision and recall for different sizes of suggested lists, between $k = 1$ and $k = 10$. Comparing our results to the persons who eventually worked on the bug is not optimal. For example, the person could have been assigned to the bug report by some factor other than expertise. Obtaining a better list of experts requires manual interrogation of the development team.

Precision is the percentage of suggested developers who actually worked on the bug report. Recall is the percentage of developers who worked on the bug who were actually suggested. It is typical for Information Retrieval approaches that there is a trade-off between precision and recall.

Getting the list of persons who eventually worked on a bug report is tricky. The \emph{assigned-to} field does not always denote a person who eventually solved the bug report \cite{Cubr04b, Anvi06a, Anvik07}. Therefore we compare our results against three configurations (C1--C3) of bug-related persons:

\begin{enumerate}
\item Developers who committed an actual bug fix to the software repository. For \EC, this information is not stored in the Bugzilla database, therefore we must rely on information from CVS commit messages. In the \validationset, this information is provided for 14.3\% of the bug reports only. This configuration evaluates how well we perform in suggesting experts who provide actual bug fixes.
\item Persons given by the \emph{assigned-to} field or a \emph{who} field of the bug report. That is, the eventual assignee (if this is a developer) and all developers who ever discussed the bug in the comment section of the report. This configuration evaluates how well we perform in suggesting experts who are capable of understanding and discussing the bug. Note that resolving a bug is not limited to providing code fixes; often the discussion is just as important to the resolution of the bug. 
\item As in configuration \#2, but additionally including the person identified by the \emph{reporter} field, if the reporter is a developer, \ie has a CVS login. \todo{check last comment} This reflects the fact that bugs are sometimes resolved by the same people who find and track them.
\end{enumerate}

Please refer to \autoref{sec:discussion} for further discussion of the above configurations and their threats to validity.

\subsection{Results}\label{therealthing}

\begin{figure}
    \includegraphics[width=\linewidth]{runOnTestSet}
    \caption{Recall and precision of the Eclipse case study: configuration C1 scores best with $33.6\%$ top-1 precision and $71.0\%$ top-10 recall.}
    \label{fig:eclipse}
\end{figure} 

\autoref{fig:eclipse} illustrates the results of the Eclipse case study. We compare lists of suggested persons of list size 1 to 10 with set of ``bug related persons'' as given by the three configurations (C1-3) above.

The figure illustrates that recall and precision of configuration C1 are better than C2 and C3. When comparing the suggested list to the bug fixing person~(C1) we achieved the best score with 33.6\% top-1 precision only and 71.0\% top-10 recall. Comparing of the suggested list to all related persons~(C3) we score 26.0\% top-1 precision and 54.2\% top-10 recall. When excluding the reporter of the bug report~(C2) from the suggested list scores are at 22.3\% top-1 precision and 57.4\% top-10 recall.

The fact that configuration C3 scores slightly better then C2 indicates that bug reporters are sometimes indeed experts regarding the reported bug and thus should be considered when triaging bug reports. We can thus conclude that an automatic assignment system should provide to the triaging person a suggested list of people that may include the reporter.

\section{Discussion}\label{sec:discussion}

In this section, we first discuss the calibration of our approach and then cover threats to validity.

Compared to related work, an advantage of our approach is that we do not require a record of previous \BRs. We are able to recommend developers who did not work on bugs previously. For example, we do not require that developers have worked on at least 10 resolved bug reports. On the other hand, our approach requires at least a half to one year of versioning history in order to suggest developers. 

One obvious threat to validity is the quality of our evaluation benchmark. We compare our suggested list against the developers who eventually worked on the bug report and assume that these are the top experts. For example, the bug report could have been assigned to a developer by some factor other than expertise. This threat is hard to counter. A better list of experts can be obtained by manual interrogation of the development team, but even this is not a golden oracle.

Another threat to validity is that we use all long descriptions, including comments, of a bug report as information source. This may include discussions that happened after the bug has been eventually assigned or fixed, information which is not actually available when doing initial bug triage. This might impact the performance of our approach. 


\begin{figure*}
    \center
    \includegraphics[width=0.40\linewidth]{distOverTime}
    \includegraphics[width=0.28\linewidth]{fadedMatrices}
    \includegraphics[width=0.28\linewidth]{penaltyResults}
    \caption{Decay of vocabulary: (left) decreasing quality of unweighted results, compared to results with \emph{decay of vocabulary} and \emph{inactive developer penalty} settings, (middle) precision and recall for different decay of vocabulary settings, (right) precision and recall for different inactive developer penalty settings.}
    \label{fig:decay}
\end{figure*} 

\subsection{On the Calibration of \TOOL}

We used 1/10th of the Eclipse case study as a \trainingset to calibrate our approach. The calibration results are summarized in \autoref{thetable}.

The table lists top-1 precision and top-10 recall. On the first row, we list the results before calibration $(p = 19.7\%, r = 42.6\%)$, and on the last row the results of the final calibration. Please note that the final results on the \trainingset are slightly better than the results reported in \autoref{therealthing} for the \validationset.

\begin{table}
\center
{\footnotesize \begin{tabular}{l | c  c}
  Settings & Precision & Recall \\ \hline
  reference & 19.7 & 42.6 \\ 
  weighted \verb$diff$ & 18.5 & 41.1 \\
  desc. fields only & 16.7 & 37.7 \\
  with LSI & 16.5 & 35.1 \\
  decay 0.03 & 20.5 & 43.2 \\ 
  decay 0.05 & 20.4 & 42.4 \\ 
  decay 0.10 & 20.5 & 41.5 \\ 
  decay 0.20 & 18.0 & 38.0 \\ 
  penalty 0.1 & 24.5 & 50.9 \\ 
  penalty 0.2 & 24.8 & 51.6 \\ 
  penalty 0.3 & 24.8 & 51.9 \\
  final calibration & 26.2 & 54.6 \\
\end{tabular}
}\vspace{0.1in}
\caption{Summary of calibration of \trainingset, for each settings top-1 precision and top-10 recall are given.}
\label{thetable}
\end{table}

The output of the \verb$diff$ command consists of added, removed, and context lines. We experimented with different weightings for these lines (weighted \verb$diff$ in Table 1% 
%\autoref{thetable}
). However, we found that weighting all lines the same yields best results. 

As Bugzilla bug reports consist of many fields, we experimented with different selections of fields. We found that taking short and long descriptions (``desc. fields only'' in \autoref{thetable}) yields worse results than selecting all fields except those that refer to persons, or Base64 encoded attachments. 

We also experimented with Latent Semantic Indexing (LSI), an Information Retrieval technique typically used in search engines that detects polysemy and synonymy by statistical means~\cite{Deer90a}. However, we found that LSI yields poor results (``with LSI'' in \autoref{thetable}).

\subsection{On the Decay of Vocabulary}

In our experiments, we found that developer expertise decays over time. In our approach we introduced two weighting schemes to counter this effect: 

\begin{itemize}
\item \emph{Decay of Vocabulary.} For each revision, the word frequencies are weighted by a decay factor that is proportional to the age of the revision. Developers change their interests and by doing so change their expertise and \VOC. To take such a shift into account, we fade the old \VOC out bit by bit every week, so that the newest words are weighted slightly more than older ones. With time, the old words eventually fade out completely.
\item \emph{Inactive Developer Penalty.} If a developer has been inactive for more than three months, the lexical similarity is decreased by a penalty proportional to the time since his latest contribution to the software system. Inactive developers will most likely not resolve bugs anymore. In order to only recommend currently active developers (we assign \BRs during a period of eight years), developers who did not recently make a change to the software system receive a penalty.
%This penalty consists either of raising the distance to a \BR to assign (depending on the days since their last change) or of downgrading the rank in the list of the authors (ordered by distance).
\end{itemize}

\noindent \autoref{fig:decay} illustrates the effect of these settings. On the left, the unbroken curve illustrates the decreasing quality of unweighted results, whereas the dotted curve shows the results obtained with weighting. Even though results improved significantly, the quality of the weighted results still slightly decreases over time. We cannot fully explain this effect; it may be due to the increasing complexity of \EC as a project, or perhaps the lack of mappings from CVS logins to persons (see \autoref{sec:scvmapping}) in the early years of the project impacts the results. Another cause for the trend in the most recent year, \ie 2008, might be that the list of persons that worked on a bug is not yet completely known to us, which may impact the evaluation.


In the middle of \autoref{fig:decay}, precision and recall for different \emph{decay of vocabulary} settings are given. On the right, precision and recall for different \emph{inactive developer penalty} settings are given. 

Decay of vocabulary scores best results with a weighting of $3\%$ per week (which accumulates to $50\%$ per half year and $80\%$ per annum). This shows that implementation expertise acquired one year ago or earlier does not help in assigning developers to bug reports.

The inactive developer setting scores best results with a penalty of $0.2$ per annum. As a result of the penalty, the matching score of a developer who has been inactive for a year is decreased. The matching scores are the lexical similarity values (between 1.0 and 0.0). Decreasing this value by 0.1 or more is typically enough to exclude a result from the top-10 list. 

Interestingly, any penalty above 0.1 is better than none. The results obtained with different penalty values are almost the same. Please note, that even though the penalty removes inactive developers from the top of the suggested list, their vocabulary is not lost. The results reported for the calibration of the penalty do not make use of vocabulary decay. If a developer becomes active again, all his past expertise is reactivated as well. Thus, we use a moderate penalty of 0.2 in combination with a decay of 3\% as the final calibration settings.

\subsection{On Grouping Diffs by Week}

To cope with the size of our case study, we decided to run weekly iterations rather than fine-grained iterations per bug report and revision. This reduced the time complexity from over 160,000 iterations down to 394 weekly iterations.

Grouping diffs by both author \emph{and} week introduces the following threats to validity: 
If \VOC is added and removed within the same week, it does not add to the developer's expertise. 
In the same way, if a file is added and removed within the same week, it is not taken into account at all. 
If bug reports are submitted late in the week, we might miss developers who acquired novel expertise early in the week. 

If several authors worked on the same file, we cannot tell their weekly contributions apart. In this case, we weight the word frequencies by $\frac{1}{\sqrt{n}}$, where $n$ is the number of co-developers, and assign the weighted frequencies to all co-developers. For the \EC case study, this applies to 3.6\% of weekly file changes.

\subsection{On other Threats to Validity}\label{sec:scvmapping}

% the golden oracle of experts is never available.

Establishing an identity relationship between CVS logins and people mentioned in \BRs is not trivial. The developer information in the CVS log is provided as a mere login name. People mentioned in a Bugzilla \BR are listed with their email address and sometimes additionally with their first and last name. For \EC, the mapping between logins and active developers can be found on the \EC website\footnote{http://www.eclipse.org/projects/lists.php}. However, the list of names of the former \EC developers does not include their corresponding logins\footnote{http://www.eclipse.org/projects/committers-alumni.php}. We could not map 17.1\% of the CVS logins and had thus to exclude 2.7\% of the bug reports from our evaluation.   

Information about copy patterns is not available in CVS. Bulk renaming of files appears in the change history of CVS as bulk removal of files followed by bulk addition of files. Given our current implementation of \TOOL, this may lead to an incorrect acquisition of developer knowledge, since the entire vocabulary of the moved files is assigned to the developer who moved the files.
We are thus in good shape to further improve our results by using a copy pattern detection approach~\cite{Chan08a}.

\section{Conclusion}\label{sec:eventually}

We presented a novel expertise model of developers. The model is based on the source code vocabulary of developers. The vocabulary of developers is obtained from the \verb$diff$ of their source code contributions. We applied the model in a recommendation system that assigns developers to bug reports. We evaluated the recommendation system using eight years of Eclipse development as a case study, and achieved 33.6\% top-1 precision and 71.0\% top-10 recall. 

When calibrating our approach, we found that developer expertise decays over time. To counter this effect we applied two weighting schemes: i) \emph{decay of vocabulary} weights expertise by a decay factor that is proportional to the time since the developer acquired that expertise, ii) \emph{inactive developer penalty} downgrades developers that had been inactive for a certain time.

In the future, we would like to extend our expertise model with developer knowledge from other sources, \eg mailing lists. Furthermore we would like to include additional Information Retrieval techniques, as well as combine our approach with approaches that are trained on previous bug reports (\eg \cite{Anvi06a, Canf05a, Cubr04b, Lucc02a}).

For more information on \TOOL and the case study presented in this paper, please refer to the Master's thesis of Dominique Matter \cite{Matt09a}.

\cite{Credibility in Code Search with Social Clues}

The promise of search-driven development is that developers will save time and resources by reusing external code in their local projects. To efficiently integrate this code, users must be able to trust it, thus \emph{trustability} of code search results is just as important as their relevance. 
%
In this paper, we introduce a \emph{trustability metric} to help users assess the quality of code search results and therefore ease the cost-benefit analysis they undertake trying to find suitable integration candidates. The proposed trustability metric incorporates both user votes and cross-project activity of developers to calculate a \emph{``karma''} value for each developer. Through the karma value of all its developers a project is ranked on a trustability scale.
%
We present \Jbd, a proof-of-concept code search engine which implements our trustability metric and we discuss preliminary results from an evaluation of the prototype.

Code search engines help developers to find and reuse software. 
%The promise of search-driven development is that developers will save time and resources by searching for software and reusing the search results in their code. 
However, to support search-driven development it is not sufficient to implement a mere full text search over a base of source code, human factors have to be taken into account as well. At last year's SUITE workshop \cite{Kuhn09a}, \emph{suitability} and \emph{trustability} have been major issues in search-driven development, besides---of course---relevance of search results.  

In this paper we focus on the \emph{trustability} of search results. Relevance of code search results is of course paramount, but trustability in the results is just as important. Before integrating a search result the developer has to assess its trustability to take a go-or-no-go decision. A well-designed search interface allows its users to take this decision on the spot. Gallardo-Valencia \etal found that developers often look into human rather than technical factors to assess the trustability of search results \cite{Gall09a}. For example developers will prefer results from well-known open source projects over results from less popular projects.

In this paper we present a trustability metric for search results. The  trustability metric is based on human factors. We use data collected from Web 2.0 platforms to assess the trustability of both projects and developers. Our trustability metric is based on collaborative filtering of user votes and cross-project activity of developers. For example, if a little-known project is written by developers who also contributed to a popular open source project, the little-known project is considered to be as trustable as the popular project. 

As a feasibility study, we implemented the trustability metric in \Jbd, a proof-of-concept code search engine. The index of our \Jbd installation currently contains trustability assessments for over 3,700 projects, based on 193,000 user votes and the cross-project activity of over 56,000 developers. In this paper, preliminary results from an evaluation of the prototype are discussed.

Contributions of this paper are as follows.
\begin{itemize}
\item We introduce a trustability metric for software projects. The trustability metric is based on human factors, and uses collaborative filtering of both user votes and cross-project activity of developers.
\item We present \Jbd, a proof-of-concept implementation of our trustability metric and discuss preliminary results from an evaluation of the prototype.
\end{itemize}

% =====================================================================
\begin{figure}
  \centering
    \includegraphics[width=\linewidth]{src_paper_diagram2}
    \caption{
    {\small
    Architecture of the \Jbd prototype. \Jbd enhances search results from source code with a trustability estimate that is based on social data collect from the Ohloh Web 2.0 website.
    }
    }
    \label{fig:archi}
\end{figure}
% =====================================================================
\section{Trustability Metric}
\label{sec:metric}

\begin{figure*}
  \centering
  	 \includegraphics[width=\linewidth]{screenshot5.png}
    \caption{
    {\small
    Screenshot of a \Jbd search result with trustability estimate. On the right there is the actual search result, with full name and code snippet. On the left there is information about the originating project and the trust value calculated by the trustability metric.
    }
    }
    \label{fig:screenshot}
\end{figure*}

In this section, we propose a trustability metric for code search results that uses collaborative filtering of both user votes and cross-project activity of developers.

To assess the trustability of code search results we combine traditional full text search with meta-information from Web 2.0 platforms. Our trustability metric requires the following information:

\begin{itemize}
\item A matrix $M = (c_{d,p})$ with the number of contributions per contributor $d$ to a project $p$.
\item A vector $V = (v_p)$ with user votes for software projects to signal the users' trust in projects. Gallardo-Valencia \etal refer to user votes as ``fellow users'' \cite{Gall09a}.
\end{itemize}

We use collaborative filtering of both user votes and cross-project activity of developers. For example, if a little-known project is written by developers who have also contributed to a popular open source project, the little-known project is considered to be as trustable as the popular project. Since both the number of contributions per contributor and the number of votes per project follow a power-law distribution, we use \emph{log} weighting and \emph{tf-idf}\footnote{``term frequency-inverse document frequency'' \url{http://en.wikipedia.org/wiki/Tf-idf}} weighting where applicable. 

First we define the \emph{karma} of a contributor as

    $$K_{d} = \sum_{P} w_{d,p} \log v_p
    \quad \mathrm{where} \quad 
    w_{d,p} = \frac{\log c_{d,p}}{\log \mathrm{df}(d)}$$

\noindent
which is the sum of the votes of all projects, weighted by the number of contributions to these projects and divided by the inverse project frequency of the contributor (\ie the number of projects to which the contributor contributed at least one contribution).


Based on this, trustability of a project is defined as 

	$$T_{p} = \sum_{D} w_{d,p} K_d
	\quad \mathrm{where} \quad 
	w_{d,p} = \frac{\log c_{d,p}}{\sum_{d' \in D} \log c_{d',p}}$$

\noindent
which represents the sum of the karma of all the projects contributors, weighted by the number of their contributions.
Note that we divide project trustability by the total number of contributions, but not contributor karma. This is on purpose, contributors are more trustable the more they commit (based on the assumption that all accepted commits require approval of a trusted core developer, as is common in many open source projects) but projects are not per se more trustable the larger they are.

To summarize, we consider a project to be trustable if there are significant contributions by contributors who have also significantly contributed to projects (including the project in question) that have received a high number of user votes.

The proposed definition of trustability is dominated by cross-project contributors, \ie contributors who contributed many times to many projects with many votes. This is in accordance with empirical findings on open source that have shown how cross-project developers are a good indicator of project success \cite{Kats07a}. This behaviour is also known as ``the rich get richer'' in the theory of scale-free networks and is considered an inherent and thus common property of most social networks \cite{Bara03a}.

% =====================================================================
\section{The JBender Prototype}
\label{sec:approach}

We have developed a prototype, called \Jbd, which enriches code search results with trustability information. To add to the information content of search results we combine two main sources to form the \Jbd code search engine. On the one hand there is the actual code base of the search engine over which an index is created. On the other hand we have created a database of metadata for the projects in the code base. 

\autoref{fig:archi} illustrates the architecture of \Jbd. \Jbd creates a searchable index over the code base and provides a code search over it. Its novelty however lies in the underlying metadata which is linked to the projects in the searchable code base - upon finding results from the latter \Jbd can supply the meta information stored for the result's originating project.


\begin{figure*}
{\small
  \centering
\begin{tabular}{ l | l | l }%{ | l || l || l |}
%\hline
%% row 1
\textbf{Top projects (by votes)} & \textbf{Top Developer (by karma)} & \textbf{Top projects (by trustability)}
\\\hline
%% row 2
``firefox", $v_p$ = 7207 & ``darins", $K_{d}$ = 71.97 & ``grepWin", $T_{p}$ = 51.60, $v_p$ = 32
\\%\hline
%% row 3
``subversion", $v_p$ = 5687 & ``amodra", $K_{d}$ = 70.11 & ``GNU Diff Utilities", $T_{p}$ = 51.18, $v_p$ = 645
\\%\hline
%% row 4
``apache", $v_p$ = 5107 & ``darin", $K_{d}$ = 69.09 & ``Eclipse Ant Plugin", $T_{p}$ = 49.76, $v_p$ = 136
\\%\hline
%% row 5
``mysql", $v_p$ = 4834 & ``nickc", $K_{d}$ = 67.14 & ``Eclipse Java Development Tools", $T_{p}$ = 48.36, $v_p$ = 647
\\%\hline
%% row 6
``php", $v_p$ = 4081 & ``Dani Megert", $K_{d}$ = 66.51 & ``Crimson", $T_{p}$ = 42.41, $v_p$ = 2
\\%\hline
%% row 7
``openoffice", $v_p$ = 3118 & ``mlaurent", $K_{d}$ = 66.14 & ``GNU binutils", $T_{p}$ = 42.18, $v_p$ = 525
\\%\hline
%% row 8
``firebug", $v_p$ = 3109 & ``Paul Eggert", $K_{d}$ = 65.89 & ``syrep", $T_{p}$ = 42.12, $v_p$ = 2
\\%\hline
%% row 9
``gcc", $v_p$ = 2586 & ``kazu", $K_{d}$ = 65.78 & ``GNU M4", $T_{p}$ = 41.85, $v_p$ = 54
\\%\hline
%% row 10
``putty", $v_p$ = 2519 & ``rth", $K_{d}$ = 65.25 & ``gzip", $T_{p}$ = 41.61, $v_p$ = 261
\\%\hline
%% row 11
``phpmyadmin", $v_p$ = 2412 & ``hjl", $K_{d}$ = 65.04 & ``Forgotten Edge OpenZIS", $T_{p}$ = 40.86, $v_p$ = 1
\\%\hline
\end{tabular}
\caption{
{\small
Top ten results for A) project ranking by Ohloh, B) karma of developers, C) project ranking by trustabilty.
}}
\label{fig:table}
}
\end{figure*}

\subsection{JBender's Metadatabase}
Our source of meta data is the \textsc{Ohloh}\footnote{\url{http://www.ohloh.net}} project. Ohloh is a social networking platform for open source software projects where projects (or rather their developers) can specify additional information. However Ohloh does not allow users to actually search through or interact with the source code: Ohloh is not a code search engine. Ohloh provides user contributed information on both open source projects and their developers, composing valuable information for search users. Users can vote for both projects and developers whether and how much they like them by rating projects and giving kudos to certain developers. Furthermore kudos are (automatically) given to developers who have worked for successful projects, i.e. projects with large user bases. 

For the \Jbd prototype we collected the trustability meta-information from Ohloh, which is a social web platform for open source projects that provides user contributed information on both open source projects and their developers.

Metadata stored in the database includes (among others): Description of original project, project homepage, rating of the project, list of current repositories (type, url, last time of update, ...), licenses of files in the project (exact type of license, number of files), employed programming languages (percentage of total, lines of code, comment ratio, ...), the project's users and developers who worked on the project (kudos, experience, commits per project, ...).

\subsection{JBender's Codebase}
In addition to the collected metadata, \Jbd also follows the links to the version control repositories that are listed on Ohloh, creates local copies of these repositories and parses the code in Java projects to build an search index over them.
\Jbd then provides a basic structured code search over various parts of the indexed source code. Examples are method/class names and their bodies, comments, visibility, dependencies and implemented interfaces.

\subsection{Trustability enhanced results}

The following data from Ohloh was directly used for the trustability metric: As contributors we used the developers of the projects and as the number of contributions we used the number of commits. As user votes we used the number of developers who ``stacked'' a project, which is Ohloh's terminology for claiming to be an active user of a project.\footnote{That is, we interpret ``votes'' as a user expressing his trust in a project by using it.} Thus in our case, both users and contributors are open source developers. To be a user the developers must be registered on Ohloh.  This is not necessary for being a contributor, since that information is taken from version control systems.

As explained in \autoref{sec:metric} this trustability metric takes into account several of the collected meta parameters and calculates a trust metric for each result according to which the results can be sorted.

\autoref{fig:screenshot} shows a screenshot of a single search result from \Jbd. On the right there is the actual search result, with full name and code snippet. On the left there is information about the originating project and the trust value calculated by the trustability metric. Currently the raw trust measurement is displayed as a floating point number to the user. We might change that to a ranked assessment that maps the trustability to a scale from 1 to 10 to improve usability. 

The layout of our search result is deliberately kept very simple and lucid in order to be efficiently usable. It has been shown that efficient search requires compact and well-arranged interfaces, which do not burden the user with too much information or a complex information seeking process \cite{Hear09a}. 


\section{Discussion}
\label{sec:discussion}

\paragraph{Some preliminary results}
\autoref{fig:table} illustrates the top-10 results for a) project ranking through votes by Ohloh, b) karma of developers, c) project ranking by our trustability metric. Notice how the project ranking changed through consideration of cross-project developer activity: grepWin for example has only 32 users on Ohloh but is ranked by us with top trustability because its developers are very active and have a high karma value.

\paragraph{Evidence of power law distribution} 
We found that our input data (\ie the user-generated data that we crawled from Ohloh) follows a power law distribution: the number of votes per project ($r = 0.95157$), the number of commits per developer per project ($r = 0.89207$), as well as number of projects per developer ($r = 0.85029$). Therefore we applied \emph{log} and \emph{tf-idf} weighting so that the trustability metric is not dominated by high values.
At the moment project trustability ranges from zero to about 52, developer karma ranges from zero to about 72.

\paragraph{A note on Ohloh's kudo-rank}
The Ohloh website provides its own measurement of developer ``karma'', called \emph{kudo-rank}. Kudo-ranks are based on a mix of user votes for projects and of user votes for developers, called \emph{kudos}. User participation for kudos is very low and as a consequence a small clique of developers can vote themselves up to top ranks. Therefore, we decided against including kudo-ranks in our trustability function.

\paragraph{Possible weakness of karma ranking}
One must consider that developers may not use the same user names for all their commits through various repository systems. In such a case Ohloh can not auotmatically collect all the developers commits into one account; the developer would have to register and do this manually. Furthermore we blacklist commit bots. Finally the karma value could be tampered with deliberately if a user was to do a huge number of (small) commits to few highly ranked projects.

\section{Conclusion}
\label{sec:conclusion}

In this paper we have presented an approach to improve the \emph{trustability} of search results. Trustability of search results is important, so that developers can quickly assess search results from external code bases before integrating them into their local code base. 

%Our aim is that the developer gains a quick estimate of code trustability in his results. We therefore created a trust function, that calculates a trust value for each originating project depending on the projects meta data. This trust value is an indication of the projects quality, its popularity and the quality of its source code. Upon reaching a sufficient size of source code and metadata index it would also be egligible to sort search results according to their trust value. As result relevance is paramount the trustability metric would then be used to choose from a pool of \emph{relevant} search results. Under the premise that all results comply with the users technical specification this would provide the user working results of the best quality.

We have proposed $T_p$ as a trustability metric for software projects.  We have also presented \Jbd, a proof-of-concept prototype code search engine that implements the trustability metric which allows developers to quickly assess the trustability of search results from a code search engine. We have discussed the choice of our trustability metric and presented preliminary results from an ongoing evaluation. 

%We have proposed $T_p$ as a trustability metric for software projects.  We have also presented \Jbd, a proof-of-concept prototype code search engine that implements the trustability metric. Integrating the trustability metric in a code search engine allows developers to quickly assess the trustability of search results from a code search engine. We have discussed the choice of our trustability metric and presented preliminary results from an ongoing evaluation.

The current trustability metric is defined per project. We would like to combine it with code ownership data from project history, so that we can assess the trustability of single classes (or even methods) based on developers karma.

Currently we are building up our metadata and code bases for \Jbd; upon reaching a sufficient level we plan do a user study to evaluate the effect of metadata on result trustability. We would also like to compare the proposed trustability metric with other trustability measurements, \eg corporate backing of projects. It might also be promising to combine the proposed trustability metric, which is currently based on human factors only, with technical trustability assessments such as \eg test coverage. 

\chapter{API Learning, User Study}

Modern software development requires a large investment in learning application programming interfaces (APIs).
%
Recent research found that the learning materials themselves are often inadequate: developers struggle to find answers beyond simple usage scenarios.
%
Solving these problems requires a large investment in tool and search engine development.
%
To understand where further investment would be the most useful, we ran a study with 19 professional developers to understand what a solution might look like, free of technical constraints. In this paper, we report on design implications of tools for API learning, grounded in the reality of the professional developers themselves. 
The reoccurring themes in the participants' feedback were trustworthiness, confidentiality, information overload and the need for code examples as first-class documentation artifacts.

Modern software development requires a large investment in learning application programming interfaces (APIs), which allows developers to reuse existing components. API learning is a continuous process. Even when a developer makes a large initial investment in learning the API, 
for example, by reading books or going through online tutorials, 
the developer will continue to consume online material 
about the API 
throughout the development process. These materials include reference documentation from the API provider, sample code, blog posts, and forum questions and answers. 

Indeed, seeking online API information has become such a pervasive part of modern programming that emerging research tools blend the experiences of the browser and the development environment. For example, Codetrail automatically links source code and the web pages viewed while writing the code \cite{gm09}. Blueprint allows a developer to launch a web query from the development environment and incorporate code examples from the resulting web pages \cite{bdwk10}. While these new tools help reduce the cost of (re)finding relevant pages and incorporating information from them, this covers only a portion of developers' frustrations. 
%
In a recent study of API learning obstacles among professional developers, Robillard found that the learning materials themselves are often inadequate~\cite{robillard09}. Bajracharya and Lopes analysed a year's worth of search queries and found that current code search engines address only a subset of developers needs~\cite{Bajracharya2009a}.   
For example, developers struggled to find code examples beyond simple usage scenarios, to understand which parts of an API support which programming tasks, and to infer the intent behind the API's design. 
Solving these systematic problems requires a large investment, either in the API provider's official documentation, the API users' community-based documentation, or in the search engines that unite the two \cite{bdwk10,Hoffmann2007a}. 
%Where should such an investment be made? Should API providers create more extensive documentation on complex usage scenarios and design intent? Should the API users push for mass organization of content based on tagging or wikis? Should search engines create more sophisticated indices of API web pages, e.g. by parsing embedded code snippets? 
Any of these changes is difficult and expensive.

% NOTE: I originally anonymized "Microsoft" and "Silverlight" but it seemed a bit silly.

To understand where further investment would be the most useful, we ran a study with 19 professional developers from Microsoft Corporation, with the goal of understanding what an ``ideal'' solution might look like, free from technical constraints. We invited randomly chosen members of a corporate email list of  Silverlight users to participate in one-hour sessions for small gratuities. Silverlight is a large API for creating web applications, with hundreds of classes for data persistance, data presentation, and multimedia. All participants were male with an average of $12.2$ years of professional experience. 

Borrowing from participatory design, we asked the participants to act as our partners in designing a new user experience for learning Silverlight. We ran two types of sessions.  In the first, we interviewed participants to learn their common learning materials and  most challenging learning tasks and then asked them to sketch a design for a new learning portal. We compiled these ideas into five exploratory designs. In the second type of session, we ran focus groups to get feedback on our descriptions of their learning tasks and the five designs. 

This paper's main contributions are a compilation of design implications for API learning tools, grounded in the reality of the professional developers themselves. We report on the recurring themes in the participants' feedback: trustworthiness, confidentiality, information overload and the need for code examples as first-class documentation artifacts.

\begin{figure*}
    \includegraphics[width=\linewidth]{api-learning-five-design}
    \caption{The five solution designs as narrated to the participants (from left to right): \ZoomableUML, \ConceptMap, \FacettedSearch, and on the last panel \RichIntellisense~(above) and \CloudREPL~(below). Pen color has been used to distinguish the designs (green) from the participant's input (red). The stick notes are the participant's votes.}
    \label{fivedesigns}
\end{figure*} 

\moarsauce
\section{First Study: Current Practice}

In the first type of study session, we met singly with nine participants, and ran each through three activities. First, we asked the participant to describe all the  materials he used for learning Silverlight, as we recorded them on the whiteboard. Next, we asked him to consider this as a set of ``ingredients'' and to sketch a design for a learning portal that presents some or all of these ingredients to help developers learn Silverlight. Finally, we asked him to review the design by comparing the experience of learning Silverlight by using the design versus his own experience learning Silverlight.

\moarsauce
\subsection{Learning Sources}
We asked the participant to describe all the  materials he used for learning Silverlight, as we recorded them on the whiteboard. Some of the learning sources are obvious and readily reported by participants, such as books and web search. To learn about non-obvious learning sources, we asked developers ``did you ever find an answer to a technological question that is not listed here,'' which led to answers like reverse engineering or social networking. Their reported learning sources are the following:

\begin{description}
\item[``Off the top of my head''] is by far the most common way developers find answers on the job. Most participants reported that they set aside dedicated time for learning. Typical off-the-job learning sources are: lurking on mailing lists and forums, watching videos and reading books. Most knowledge however is based on experience and acquired through learning-by-doing on the job. One participant refers to this a \emph{``growing your own folklore.''}

\item[Web search] was reported by all participants as the first place to go when they have an information need. Among the search results participants are typically looking for are: blog posts, discussion forums, official reference documentation, mailing list archives, bug reports and source repositories (listed in order of typical access patterns). 
Participants prefer results with code examples over results without code examples, which is supported by existing research on API learning barriers \cite{robillard09}.

\item[Intellisense] (\ie auto-completion of identifiers) was reported as a tool for the discovery of unknown APIs by all developers. One participant called this \emph{``digging through the namespaces.''} Discovering unknown APIs is an appropriation of auto-completion, originally conceived to help recall names from familiar APIs. 

\item[Prototyping,] reverse engineering and many more forms of tinkering were reported by all participants as a last resort when all above sources failed to provide an answer. 
Some participants even resort to reverse engineering when documentation is available, as they prefer source code over natural language documentation. 
Developers typically use prototyping both as an explorative tool and to verify hypotheses about the APIs. All participants reported that having to \emph{``get your hands dirty''} is an integral part of their learning experience. 

\item[Asking another person] was reported by most participants as a last resort. Developers follow a ``due diligence'' process before asking another person for an answer. It is important to them to have put enough personal effort into finding an answer before asking on a mailing list or reaching out to a person from their social network. 
Also, they reported to prefer immediate results, such as those provided by web search, over waiting for the reply of asynchronous communication such as email and discussion forums.

\end{description}

These findings are consistent with Robillard's study of learning obstacles~\cite{robillard09}, but provide a more complete catalog of learning materials. Both studies found that developers strive to stay within the programming patterns and use cases that the API provider intends (even when that intent is undocumented) and that developers typically lack documentation when using an API for a less common task. Somewhat surprisingly, we found that developers prefer the community-based learning materials on the web, like blogs, forum posts, and tutorials, over more ``authoritative'' learning material, like books and reference documentation. Developers also prefer active but potentially time-consuming information seeking,~like iterative web search and reverse engineering, to waiting on answers from others, because they perceive the answers as more immediate.

\moarsauce
\subsection{Learning Categories}

% renamed this to learning categories, since "task categories" might be confused by reviewers with tasks in a suer study (at least Niko was clearly confused when reading this draft!) 

Based on the design sketches that participants produced, we elicited three broad categories of learning tasks:

\begin{description}
\item[Technology selection] is about learning about an API's fundamental capabilities (\emph{``Can Silverlight play video in this codec?''}) and about comparing capabilities (\emph{``Is DirectX or Silverlight better for my game?''}). Sometimes the selection decision is about growing skills rather than project requirements. 

\item[Mapping task to code] includes both discovery of unfamiliar APIs as well as remembering relevant identifier names in previously learned APIs. Getting an answer to this type of questions typically falls in two phases. Initially developers search based on natural language task descriptions (e.g. \emph{``how to implement a zoomable canvas''}) and skim through many search results to stumble on relevant identifier names. Once they have a concrete identifier, their search behavior becomes more focused and may be as simple as looking up the identifier's reference documentation.

\item[Going from code to better code] is a major concern of professional developers. All participants reported that they spend considerable effort getting answers to performance questions. Other use cases are robustness and idiomatic, \ie intended, use of an API, in particular with regard to breaking changes of newly released API versions or different target platforms.

\end{description}

The kind of learning categories impacts the preferred learning strategies of developers. For technology selection, participants sometimes use web search to learn about available technologies, but eventually prefer personal recommendations from their social network.  For mapping task to code, participants strongly prefer search results with code examples over such without code examples. For getting to better code however, such as troubleshooting a performance problem, participants prefer solving the problem themselves (including reverse engineering) but sometimes ask others to double-check the answers they find.

\moarsauce
\section{Second Study: Solution Elements}

For the second type of session, we compiled the user feedback from the first sessions into five exploratory designs. We ran 10 participants in three focus groups (with three, three, and four members) and asked them to provide feedback on the five designs. In each session, we drew each design on its own whiteboard and encouraged participants to ask questions, provide feedback, and to add their own ideas as we explained the design. 

Figure~\ref{fivedesigns} shows a photograph of the whiteboards with the five designs, taken at the end of a focus group's session. In the following the designs a described on the order they were presented to the participants in that session:

\moarsauce
\paragraph{Design: \ZoomableUML} 
This design draws from the spatial software representation of CodeCanvas~\cite{Deline2010a} and addresses answering complex reachability questions~\cite{Latoza2010a} as you code. The design extends the IDE with a zoomable UML diagram. The diagram opens zoomed on locally reachable types of the API and shows their dependencies and interaction. The user can zoom out to get a larger picture of the API, up to the level of namespaces.

\moarsauce
\paragraph{Design: \ConceptMap} 
The API is presented as a zoomable map, organized around programming domain concepts (e.g. ``controls'', ``media content''). As the user zooms in, the concepts become more refined (e.g. ``streaming video''). At the lowest zoom level, the map shows web-based content about that concept, including blogs, forum posts, tutorials, and the people who author these.
The map is searchable and keeps track of user interaction as well as the user's learning progress. Users can bookmark locations and share their bookmarks. Documentation editors can use the same feature to share tutorials as ``sight-seeing tours.''

\moarsauce
\paragraph{Design: \FacettedSearch}
This design unifies web search and asking people questions. The user types a question into a textbox. As she types, related  search results are pulled in from various sources (web sites, bug reports, code examples, mailing list archives, etc). Search results are grouped by facets, such as type of sources, type of content or semantic concepts. Besides the results, a tag cloud appears with extracted identifier names. Search results are summarized using code examples, if possible. In addition, the results include suggested people and mailing lists that are experts on the topic of the questions, to which the question can be posted.

\moarsauce
\paragraph{Design: \RichIntellisense} 
This design extends auto-completion of identifiers with search results that are automatically pulled from the world wide web. The results are ``localized'' to the current context of the IDE, such as imported libraries and reachable types \cite{Holmes2005}. Results are shown in the same pop-up windows as the auto-completion suggestions. If possible search results are displayed as code examples, ready for incorporating into the code, as in Brandt et al \cite{bdwk10}.

\moarsauce
\paragraph{Design: \CloudREPL} 
This design attaches an execution context to code examples on the web. Code examples include hidden meta-information with all context that is required to execute. Examples are editable, debuggable and can be executed live in the browser. With a single click, users can download examples into their IDEs. Similarly, users can upload the code in their IDE as runnable examples on the web, for inclusion in blogs or discussion forums.

\moarsauce
\section{Feedback}

After we explained all five designs, we then handed each participant a pen and sticky notes and gave them 10 minutes to annotate the designs, either with a blank sticky note to mean ``I like this part'' or with their own comments (typical ones were smiley faces, frowny faces, ``NO'', etc). 

The votes are summarized in Table~\ref{thetable}: the most popular design are ``\FacettedSearch`` and for learning activities the ``\ConceptMap`` design. Participants downvoted the ``\ZoomableUML'' and ``\RichIntellisense'' due to concerns about information overload, the same happened with ``\CloudREPL`` due to concerns about missing confidentiality.

\begin{table}[h]
\begin{center}
\begin{tabular}{l|ll}
\textbf{Design} & Up-Votes & Down-Votes \\
\hline
\ZoomableUML & $\star\star\star\star~$ & $\ast\ast\ast\ast\ast~$ \\
\ConceptMap & $\star\star\star\star\star\star~$ & $\ast\ast\ast~$ \\
\FacettedSearch & $\star\star\star\star\star\star\star\star\star~$ & $\ast~$ \\
\RichIntellisense & $\star\star\star~$ & $\ast\ast\ast\ast\ast\ast\ast~$ \\
\CloudREPL & $\star\star\star~$ & $\ast~$
\end{tabular}
\end{center}
\caption{At the end of the second type of sessions, participants voted with sticky notes for the designs. \FacettedSearch{} received the most up votes, \RichIntellisense{} the most down votes.}
\label{thetable}
\end{table}%

There were several recurring themes in our participants' feedback which cut across the various designs. The four top most recurring themes are discussed and summarized as design implications for tool builders in the following:

\moarsauce
\subsection{Code Examples}

We got very positive feedback on the emphasis on code examples and identifier names in the ``\FacettedSearch'' design. Participants prefer results with code examples over results without code examples, which is supported by existing research on API learning barriers \cite{robillard09}. When mapping a task to code, developers typically use web search and linearly go through all results until they find one with a code example or an identifier; often repeating this process a dozen times until they find a working answer. Participants liked about the facetted search design that it extracts code examples and identifiers from top search results. One participant even said that the summary tag cloud with identifiers, by itself, would be reason to use it.

\emph{Implication for tool builders:} Developers need the heterogeneous learning materials that web search provides, but want it to be more targeted and organized. Search engines for API learning should extract code examples and identifiers found in natural text documents, and present them to the developers in a more accessible way. This implication is supported related work on code examples \cite{bdwk10,Hoffmann2007a,Holmes2005}.

\moarsauce
\subsection{Credibility}

Credibility of web sources appeared as a major concern with all designs that included content taken from the web. For the participants, credibility is mostly a function of where the information comes from. For example, participants reported that search results from blogs are often more relevant, but typically less credible than official reference documentation. They also rely on the social reputation of its source rather than technical factors, which supports existing research \cite{Gysin2010a}. In particular with the ``\FacettedSearch'' design, which automatically summarizes search results, participants emphasized the importance of seeing the information source to judge credibility.  

\emph{Implication for tool builders:} Tools should show both credibility and relevance when presenting search results, such that the developers can make an informed decision when using API information and code examples from the web. To asses the credibility of API information tools should prefer social factors, such as the credibility of the information's author, rather than technical statistics, such as code metrics.

\moarsauce
\subsection{Confidentiality}

Confidentiality appeared as a major concern with all designs that share local information with a global audience. In particular with the ``\CloudREPL'' design, which publishes an example's execution context on the web, participants were concerned with leaking proprietary information, like the use of certain libraries. One participant was also concerned that publicly inquiring about technologies could accidentally reveal business strategies.

\emph{Implication for tool builders:} When automatically sharing local information with the web, tools must be careful about protecting proprietary information, such as not showing confidential code, nor libraries being used. Tools should give developers full control over shared information, for example by letting them review the list of automatically included terms before issuing the search query. Or alternatively, only sharing information that is on a user controlled white list.

\moarsauce
\subsection{Information Overload} 

Information overload was the major reason why participants rejected the ``\ZoomableUML'' and the ``\RichIntellisense'' designs. We got strong feedback that pulling more information into the IDE is not welcome unless it is highly task- and context specific information. Participants were also concerned that adding more features to Intellisense's popup will use too much screen real estate and slow down the IDE. 

\emph{Implication for tool builders:} Any tool that pulls additional information into the IDE must be highly selective and should only show information that is specific to the developer's current task and context. The ability to further filter down the information is crucial, as well as not slowing down the IDE and using screen real estate sparingly.

%\subsection{Visual Representation}

%\todo{Maybe some words on the visual representation of code, ie about the participants  doubt that visual languages such as UML are any better than tree views. And also about the difficulty to represent learning material on a visual map?}

%\subsection{Sharing of Answers}

%\todo{Maybe some words on social media and the participants strong division in two groups with regard to that. But also that we found that participants tend take question to a less visible channel when helping to answer them, like taking them off the mailinglist (in order to avoid other attendees information overload) but then never share the final answer to the initial audience. And that participants said that they would share content, such as their bookmarks, on the conceptual map.}

\moarsauce
\subsection{Threats to validity}

We selected all participants from the same corporation, whose common hiring practices and corporate culture may bias the results. In particular, the participants all work for the same company that produces the Silverlight API, which gives the participants unique access to the API creators. 
Therefore, the participants may not be representative of all professional developers. 
Nonetheless, participants mostly accessed public learning outside the company and many expressed hesitation about asking questions of fellow employees for fear of harming their reputation. 
The study is also based on a single API. While this choice allowed us to compare participants' experiences and gave them common ground during the focus groups, there may be issues in learning Silverlight that do not generalize to other APIs. 

\moarsauce
\section{Conclusion}

% Split by task type?

%\todo{Typically there should be no new ideas in a conclusion, so maybe should move all new ideas from here to a discussion part... or just rename this section. Folksonomies for example are not discussed before, and it would need discussion to motivate why they are an alternative to expensive tool building, so maybe this should be highlighted/hinted at through out the paper. Just the same for the duality of ``heterogeneous learning materials from the web,'' I already tried to do this but we could emphasize this more and make it a central them, like: this is very useful information but credibility and confidentality are crucial when reaching our from local to global sources / audiences. Maybe also we can add some words in the introduction that the heterogenous global content was on available like 10 years ago and that this an all new and exciting situation and thus called for that research to learn about how developers use content on the web to find answers about API questions.}

Web search is the predominant form of information seeking, but in many cases is frustrating and error-prone. Developers need the heterogeneous learning materials that web search provides, but want it to be more targeted and organized. 
Therefore, API learning tools that bring web search and development environments closer together 
	1)~should leverage examples and identifiers found in natural text documents as first-class results,
	2)~should communicate the credibility of aggregated results,
	3)~most not shared confidential information without the user's consent,
	and 4)~should filter search results by task and context to avoid information overload. 

%Doing this requires additional metadata per web page, such as the API name, version, target environment, fully qualified names of the members in code snippets, etc. The most cost-effective way to do this is by having a search engine infer this information per page. However, without this, the API user community could take a folkonomy approach.

\chapter{User Study Code Map}

\emph{Software visualization can be of great use for understanding and exploring a software system in an intuitive manner. Spatial representation of software is a promising approach of increasing interest. However, little is known about how developers interact with spatial visualizations that are embedded in the IDE. In this paper, we present a pilot study that explores the use of \SOCA for program comprehension of an unknown system. We investigated whether developers establish a spatial memory of the system, whether clustering by topic offers a sound base layout, and how developers interact with maps. We report our results in the form of observations, hypotheses, and implications. Key findings are a) that developers made good use of the map to inspect search results and call graphs, and b) that developers found the base layout surprising and often confusing. We conclude with concrete advice for the design of embedded software maps.}

Software visualization can be of great use for understanding and exploring a software system in an intuitive manner. In the past decade the software visualization community has developed a rich wealth of visualization approaches~\cite{Dieh07a} and provided evidence of their usefulness for expert tasks, such as reverse engineering, release management or dynamic analysis (\eg \cite{Tele08b,Pauw08a,Reis05b,Orso03a}). Typically, these visualization approaches had been implemented in interactive tools \cite{Sens08a}. However most of these tools are stand-alone prototypes that have never been integrated in an IDE (integrated development environment). Little is thus known about the benefits of software visualization for the ``end users'' in software engineering, that is for everyday programmers. What is lacking is how these techniques support the day to day activities of software developers \cite{Stor05a}. 

In this paper, we report on a pilot study of a spatial software visualization that is embedded in the IDE. The spatial visualization is based on the \SOCA approach that has been presented and introduced in previous work \cite{Kuhn08a,Kuhn10b,Erni10a}. Spatial representation of software is a promising research field of increasing interest \cite{Wett08a,Deli10a,Brag10b,Stei10a,Mart08a,Noac05a}, however the respective tools are either not tightly integrated in an IDE or have not yet been evaluated in a user study. Spatial representation of software is supposed to support developers in establishing a long term, spatial memory of the software system. Developers may use spatial memory to recall the location of software artifacts, and to put thematic map overlays in relation with each other \cite{Kuhn10b}.

%\ewe{Maybe comment somewhere that the level of integration may vary. Distant integration would be the case of the visualization being available within the IDE, but remain rather an supplementary tools. Tight integration implies that the map reacts to other events in the IDE, e.g. file close, etc. or at least stays sync'ed with the project}

The scenario of our user study is first contact with an unknown closed-source system. Our main question was whether and how developers make use of the embedded visualization and if our initial assumptions made when designing the visualization (as for example the choice of lexical similarity as the map's base layout \cite[Sec 3]{Kuhn10b}) are based on a valid model of developer needs. Participants had 90 minutes to solve 5 exploratory tasks and to fix one bug report. We used the think-aloud protocol and recorded the voices of the participants together with a screen capture of their IDE interactions. We took manual notes of IDE interaction sequences and annotated the sequences with the recorded think-aloud transcripts. 

Results are mixed\,---\,some support and some challenge our assumptions on how developers would use the embedded visualization. Participants found the map most useful to explore search results and call graphs, but only rarely used the map for direct navigation as we would have expected.
%It became quickly apparent that we should revise our initial assumption that lexical similarity is a valid basis for the cartographic layout. Participants interpreted visual distance as a measure of structural dependencies---even though they were aware of the underlying lexical implementation.

Contributions of this paper are as follows: 
%\nes{These aren't the contributions, they're summaries of your methodology, and promises of contributions.}

\begin{itemize}
\item We embedded the stand-alone \Codemap prototype in the Eclipse IDE, and added novel thematic overlays that support the most important development tasks with visual feedback (see \autoref{sec:tasks} and \autoref{sec:steps}).
\item We performed a think-aloud user study to evaluate the use of spatial visualization in the IDE. We discuss and comment on our results, and conclude with practical design implications (see \autoref{sec:results} and \autoref{sec:discussion}).
\item We provide suggestions on how to improve \SOCA and the \Codemap tool (see \autoref{sec:related}).
\end{itemize}

% ===================================================================================
\section{Software Cartography}
\label{sec:tasks}

\SOCA uses a spatial visualization of software systems to provide software development teams with a stable and shared mental model. The basic idea of cartographic visualization is to apply thematic cartography~\cite{Sloc05a} on software visualization. That is, to show thematic overlays on top of a stable, spatial base layout. Features on a thematic map are either point-based, arrow-based or continuous. For software this could be the dispersion of design flaws as visualized using icons; a call graph  is visualized as a flow map (as illustrated on \autoref{fig:awesome}); and test coverage is visualized as a choropleth map, \ie a heat map. 

\SOCA is most useful when it supports as many development tasks with spatial location awareness as possible. We therefore integrated our prototype into the Eclipse IDE so that a map of the software system may always be present. This helps developers to correlate as many development tasks as possible with their spatial location.

At the moment, the \Codemap plug-in for \eclipse supports the following tasks:\footnote{\url{http://scg.unibe.ch/codemap}}
% New features are added on a weekly base, please subscribe to \url{http://twitter.com/codemap} to receive latest news.}
% ON: The twitter feed is listed on the web page -- don't mention it in the paper


\begin{itemize}
\item Navigation within a software system, be it for development or analysis. \Codemap is integrated with the package explorer and editor of \eclipse. The selection in the package explorer and the selection on the map are linked. Open files are marked with an icon on the map. Double clicking on the map opens the closest file in the editor. When using heat map mode, recently visited classes are highlighted on the map.

\item Comparing software metrics to each other, \eg to compare bug density with code coverage. The map displays search results, compiler errors, and (given the Eclemma plug-in is installed) test coverage information. More information can be added through an plug-in extension point.

\item Social awareness of collaboration in the development team. \Codemap can connect two or more \eclipse instances to show open files of other developers. Colored icons are used to show the currently open files of all developers. Icons are colored by user and updated in real time.

\item Understand a software system's domain. The layout of \Codemap is based on clustering software by topic~\cite{Kuhn07a}, as it has been shown that, over time, the lexicon of source code is more stable than its structure~\cite{Anto07a}. Labels on the map are not limited to class names, but include automatically retrieved keywords and topics.


\item Exploring a system during reverse engineering. \Codemap is integrated with \eclipse's structural navigation features, such as search for callers, implementers, and references. Arrows are shown for search results. We apply the \textsc{Flow Map} algorithm \cite{Phan05a} to  avoid visual clutter by merging parallel arrow edges. \autoref{fig:awesome} shows the result of searching for calls to the {\tt \#getSettingOrDefault} method in the {\tt MenuAction} class .
\end{itemize}

\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{awesome-codemap-NIER}
\end{center}
    \caption{\emph{Thematic codemap of a software system. Here the \Codemap tool itself is shown. Arrow edges show incoming calls to the {\tt \#getSettingOrDefault} method in the {\tt MenuAction} class, which is currently active in the editor and thus labeled with a pop-up.}}
    \label{fig:awesome}
\end{figure}

% ===================================================================================
\section{The Codemap Algorithm}
\label{sec:steps}

\begin{figure*}
\begin{center}
  \includegraphics[width=\linewidth]{codemap-pipeline2}
\end{center}
    \caption{\emph{Construction steps of a software map, from left to right: 1) 2-dimensional embedding of files on the visualization pane; 2.a) circles around each file's location, based on class size in KLOC; 2.b) each file contributes a Gaussian shaped basis function to the elevation model according to its KLOC size; the contributions of all files are summed up; 3) fully rendered map with hill-shading, contour lines, and filename labels.}}
    \label{fig:steps}
\end{figure*}

\autoref{fig:steps} illustrates the construction of a software map. The sequence of the construction is basically the same as presented in previous work~\cite{Kuhn08b,Kuhn10b}. 

\paragraph{2-Dimensional Embedding}
A distance metric is used to compute the pair-wise dissimilarity of software artifacts (typically source code files). A combination of the Isomap algorithm \cite{Tene00a} and Multidimensional Scaling (MDS) \cite{Borg05a} is used to embed all software artifacts into the visualization pane. The application of Isomap is an improvement over previous work in order to assist MDS with the global layout. In contrast to our previous work, Latent Semantic Indexing (LSI) is not applied anymore, it has been found to have little impact on the final embedding. 
%The final embedding minimizes the error between the dissimilarity values and the visual distances.
%Early prototypes of \Codemap used a distance metric that was based on lexical similarity only. However, our user study revealed that developers tend to interpret visual distance as a measure of structural dependencies, even though they were aware of the underlying lexical implementation. Based on this observation, we developed an improved distance metric that takes both lexical similarity and structural distance (based on the ``Law of Demeter'' \cite{Lieb88a}) into account. 

\paragraph{Digital Elevation Model} In the next step, a digital elevation model is created. Each software artifact contributes a Gaussian shaped basis function to the elevation model according to its KLOC size. The contributions of all software artifacts are summed up and normalized. 

\paragraph{Cartographic rendering} In the final step, hill-shading is used to render the landscape of the software map. Please refer to previous work for full details~\cite{Kuhn08b,Kuhn10b}. Metrics and markers are rendered in transparent layers on top of the landscape. Users can toggle separate layers on/off and thus customize the codemap display to their needs.

% ===================================================================================
\section{Methodology}
\label{sec:method}

We evaluated our approach in a pilot study with professional developers and students. The scenario investigated by the experiment is first contact with an unknown software system. Participants have 90 minutes to solve 5 program comprehension tasks and to fix one bug report. After the experiment, participants are asked to sketch a drawing of their mental map of the system. 

Our goal for the present pilot study was to learn about the usability of \Codemap for program comprehension. We have been seeking to answer several questions. 
How can we support developers in establishing a spatial memory of software systems? How do we best support the developers spatial memory using software visualization? How to best embed spatial software visualization in the IDE? When provided with spatial representation of search results and call graphs, how do developers make use of them?  

Not covered in this study, and thus open for future user studies, are the shared team awareness and long term memory claims of the \SOCA approach \cite{Kuhn10b}.

%\AK{More to come here.}
%\todo{Need more background about the design rational of the study, about what we cover from our claims (ie that codemap helps to explore unknown software and that codemap helps to build up a spatial model) and which we do not cover (that the model is stable over months or years, that multiple team members share the same model, etc) and why we've chosen to cover those. Also tell why we do an exploratory study instead of going with the herd and doing one of those controlled experiments ... Cite interview with Andy Ko ftw! Also tell here that we would address other stake holders, like managers and architects that are supposed to share the same long term, spatial memory.}
%\on{this one? \url{http://doi.ieeecomputersociety.org/10.1109/MS.2009.122} -- does not seem right}
%
%
%\ewe{What I do lack so far is a clear explanation of your assumption about the map's role: (1) do you expect the map to become a central tool in development, or an auxiliary help (2) what's your position regarding "there is more than one way to do one thing", and how do you deal with it in the study. E.g. a dev uses the map to locate search result another not. Is it positive or negative? (3) what was the rationale to design these feature in the map, e.g. we thought that   synchronizing the map with the open files would benefit development because xzy. Then you can validate or invalidate each hypothesis}

\subsection{Design of the Study}

The study consists of six programming tasks. The training task introduced the participants to the \Codemap plug-in. The first five tasks were program comprehension tasks, starting with general questions and then going into more and more detailed questions. Eventually, the last task was to fix an actual bug in the system. Participants were asked to use the map whenever they saw fit, but otherwise they were free to use any other feature of Eclipse they wanted.

\paragraph{Task 1, Domain and Collaborators} \emph{``Find the purpose of the given application and identify the main collaborators. Explore the system, determine its domain, and fulfil the following tasks: a) describe the domain, b) list the main collaborators, c) draw a simple collaboration diagram, d) identify the main feature of the application.''}

\paragraph{Task 2, Technologies} \emph{``In this task we are interested in the technologies used in the application. List the main technologies, such as for example Ajax, XML, or unit testing.''}

\paragraph{Task 3, Architecture}  \emph{``In this task we are going to take a look at the architecture of the application. Reverse engineer the architecture by answering the following questions: a) which architectural paradigm is used (as for example pipes and filters, layers, big ball of mud, etc)? b) what are the main architectural components? c) how are those components related to one another? d) draw a UML diagram at the level of components.''}

\paragraph{Task 4, Feature Location} \emph{``In this task we are interested in classes that collaborate in a given feature. Please locate the following features: a) Interactive users are reminded after some months, and eventually deleted if they do not log in after a certain number of months, b) Depending on the kind of user, a user can see and edit more or less data. There are permission settings for each kind of user that are checked whenever data is accesses, and c) Active search: the system compares the curriculum vitae of the users with stored searches of the companies and mails new matches to the companies.''}

\paragraph{Task 5, Code Assessment} \emph{``In this task we want to assess the code quality of the application. Please answer the following questions: a) what is the degree of test coverage? b) are there any god classes? c) are the classes organized in their proper packages? Should certain classes be moved to other packages? Please list two to three examples.''} 

We provided a code coverage plug-in with the experiment, as well as a definition of what constitutes a god class \cite{Lanz06a}.

\paragraph{Task 6, Bug Fixing} In this task we provided an actual bug report and asked \emph{``Describe how you would handle the bug report, that is how and where you would change the system and which classes are involved in the bug fix. You are not asked to actually fix the bug, but just to describe how you would fix it.''}

\subsection{Participant Selection}

Participants were selected through an open call for participation on Twitter\footnote{\url{http://twitter.com/codemap}} as well as through flyers distributed at a local Eclipse event. Subjects were required to be medium level Java programmers with at least one year of experience with both Java and Eclipse programming. The six tasks had been designed so that the participants did not need to be knowledgeable with the provided application, but rather that they explore it as they go along. Seven participants took part in the experiment: 4 graduate students and 3 professional developers from industry. None of the participants was familiar with the provided application or with the Codemap plugin; even though some had attended a 15 minute presentation about the Codemap plugin at the Eclipse event mentioned above. 

\subsection{Study Setting}

The study consisted of three main parts. The first part was the training task in which the participants were given a short presentation of Codemap and a tutorial document that explained all features of the Codemap plug-in. The tutorial explained all features mentioned in \autoref{sec:tasks} using walk-through descriptions of their use. The participants were given 20 minutes to explore a small example program using the Codemap plug-in. When they felt ready, we started part two of the experiment.

The second part consisted of the actual programming tasks. A fixed amount of time was allotted to each task. Participants were asked to spend no more than 15 minutes on each task. All subjects had access to the Codemap plugin as our aim was to explore their use of the plugin rather than to compare a controlled parameter against the baseline.

Eventually, in a third part we held a debriefing session. We asked participants to draw a map (with any layout or diagram language whatsoever) of how they would explain the system under study to another developer. We asked the participants for feedback regarding their use of the Codemap plugin and how the plugin could be improved.

% ===================================================================================
\section{Data Collection}
\label{sec:analysis}

We asked the participants to think aloud, and recorded their voice together with a captured video of their computer screen using the Camtasia software\footnote{\url{http://www.techsmith.com/camtasia}}. We reminded the participants to think aloud whenever they fell  silent: we told them to imagine a junior programmer sitting beside them to whom they are to explain their actions (Master/Apprentice \cite{Hugh97a}).  The participants were asked to respond to a survey while performing the study. The survey consisted of their answers to the tasks, as well as the perceived difficulty of the tasks and whether they found the Codemap plugin useful for the task at hand. We used a combination of semantic differential statements and Likert scales with a 5 point scale.

We measured whether or not subjects were successful in completing a programming task. We used three success levels to measure the success and failure of tasks: a task could be a success, a partial success or a failure. We further subdivided tasks 4 and 5 into three subtasks and recorded success levels for each individual subtask. We asked one of the original authors of the system to assess the success levels. As this was a think-aloud study, we did not measure time, but alloted a fixed 15 minute slot to each task.

Our main interest was focused on how the participants used the IDE to solve the tasks, independent of their success level. To do this, we transcribed important quotes from the recorded participant voices and screen captures and took notes of the actions that the participants did during the tasks. For each task we tracked the use of the following IDE elements:

\begin{itemize}
\item Browsing the system using the \emph{Package Explorer} and \emph{Outline} view. This includes both drill-down as well as linear browsing of package, class and method names.
\item Browsing the system using the spatial visualization of the Codemap plugin. This includes both opening single classes, selecting a whole cluster of classes on the map, as well as reading class name labels on the map.
\item Reading source code in the editor pane, including documentation in the comments of class and method headers.
\item Navigating the structure of the system using the \emph{Type Hierarchy} and \emph{Call Hierarchy} view. We tracked whether they explored the results of these searches in Eclipse's tabular result view or using the flow-map arrows displayed on  the spatial visualization of Codemap.
\item Searching the structure of the system with either the \emph{Open Type} or \emph{Java Search} dialog. This allows users to search for specific structural elements such as classes, methods or fields. Again, we tracked whether they explored the results in Eclipse's result view or on the visualization of Codemap.
\item Searching the system with the unstructured text search, either through the \emph{Java Search} dialog or the immediate search bar of the Codemap plugin. Also here, we tracked whether they explored the results in Eclipse's result view or on the visualization of Codemap.
\end{itemize}

Replicability: the raw data of our analysis is available on the \Codemap website at \url{http://scg.unibe.ch/codemap}.

%For each task we recorded a three-level usage of the above IDE elements, that is the main means of solving the task, used among other means, or not used at all (marked with $xx$, $x$, or empty cell in \autoref{fig:table}). In addition, we took notes of particular usage sequences that we annotated with the think-aloud transcriptions. 

%========================================================
\section{Results}
\label{sec:results}

After analyzing our data, we observed different degrees of interaction with the \Codemap plug-in. We focused our analysis on interaction sequences that included interaction with the \Codemap plug-in, but also on those interaction sequences that challenged our assumptions about how developers would make use of the plug-in.

The presentation of results is structured as follows. First, we briefly cover how each task was solved. Then present an in-depth analysis of our observations, structured by triples of \emph{observation}, \emph{hypothesis}, and \emph{implication}. Implications are directed at improving the design and usability of spatial visualizations that are embedded in an IDE. %Eventually, in \autoref{sec:discussion} we discuss observations regarding the different performance of students and professional participants that are reported in the task performance subsection just below.

\subsection{Task Performance}

\paragraph{Task 1, Domain and Collaborators} Participants used an approach best described as a ``reverse Booch method''~\cite{Booc94a}. Given a two-sentence description of the system that we've provided, they searched for nouns and verbs using Eclipse's full text search. Most participants used \Codemap to assess quantity and dispersion of search results, and also to directly select and inspect large classes. Then they looked at the class names of the matches to learn about the domain and collaborators of the system. Students also read source code, whereas professional participants limited their investigation to using the package explorer and class outline.

\paragraph{Task 2, Technologies} This task showed the most uniform behavior from both student and professional participants. They inspected the build path node and opened all included JAR libraries. Professional developers typically raised the concern that possibly not all of these libraries were (still) used and started to explore whether they were used. Typically they would carry out a search to do so, but one developer showed a very interesting pattern: He would remove the libary ``on purpose'' and then look for compile errors as an indicator of its use. Students seems to implicitly assume that all libraries were actually used, at least they never raised such a concern. We interpret this as a sign that professionals are more cautious \cite{Ko04a} and thus more aware of the typical decay caused by software evolution, which may include dead libraries.

\paragraph{Task 3, Architecture}
Typically participants drilled-down with the package explorer and read all package names. All professionals started out by formulating the hypothesis of a layered three-tier architecture, and then start fitting the packages to the different layers. Most participants used \Codemap to look at the dispersion of a package's classes (when selecting a package in the package explorer, the contained classes are highlighted on the map).

To learn about the architectural constraints, professionals, for the first time in the experiment, started reading source code. They also did so quite differently from the way that students did. Whereas students typically read code line by line, trying to understand what it does, the professionals rather used the scroll-wheel to skim over the code as it flies by on the screen, thereby looking for ``landmarks'' such as constructor calls, method signatures and field definitions. Professionals made much more use of ``open call hierarchy'' and ``open type hierarchy''. Interestingly enough, only one participant opened a type hierarchy of the whole project. 

%\nes{Here's a strange thing with your paper. You sort of drift here into a HCI discussion that is interesting, but has little to do with your map. Of course, if this is where you take your research, you have to quote the relevant papers! Also, this line of finding sort of dominates the "real" contributions, but isn't even mentioned in the abstract or introduction.}\ewe{Was anybody navigating by ctrl-click on the types?}

\paragraph{Task 4, Feature Location}

\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{codemap-userstudy2010-T-fat-arrow-1937}
\end{center}
    \caption{\emph{Screen capture of ``Aha moment'' as encountered by participant~T during task 4-b (location of security features): Upon opening the call hierarchy of \texttt{Grant}'s constructor, a huge call-arrow appeared on the map: indicating dozens of individual calls that connect the security-related archipelago in the south-west with the \texttt{TreeFactory} island in the east. Given the visual evidence of this arrow, participant~T solved the task without further investigation.}}
    \label{fig:fatarrow}
\end{figure}

For this task, participants made most frequent and more interesting use of \Codemap than for any other task. Same of for task 1, participants used a reversal of the Booch method. They searched for nouns and verbs found in the feature description. Again, they used the map to assess quantity and dispersion of search results. Also two participants used the map to select and inspect search matches based on their context in the map.

Participants now began to read more source code than before. In particular, when they found a promising search result they used the ``open call hierarchy'' feature to locate related classes. All participants reported that \Codemap flow-map overlay helped them to work with the call graph. For some developers there was an actual ``Aha moment'' where one glance at the \Codemap helped them to solve the current subtask immediately without further investigation. \autoref{fig:fatarrow} illustrates one particular moment as encountered by participant~T during the location of the security feature.

\paragraph{Task 5, Code Assesment} This set of tasks made it most obvious that \Codemap's layout was not based on package structure. Participants reported that they had a hard time to interpret the thematic maps as they could not map locations on the map to packages. In particular the professional participants expressed concerns regarding the use of KLOC for hill size. They expressed concerns that this might be misleading since lines of code is not always an indicator of importance or centrality on the system's design.

\paragraph{Task 6, Bug Fixing} Participants mainly used the same approach as for the feature location tasks. They first located the implementation of the feature in which the bug occurs, and then fixed the bug. Professional participants did so successfully, whereas student participants did not manage to find the correct location in the source code.

\paragraph{Wrap-up session} In general, participants reported that \Codemap was most useful when it displayed search results, callers, implementers, and references. A participant reported: \emph{``I found it very helpful that you get a visual clue of quantity and distribution of your search results''}. In fact, we observed that that participants rarely used the map for direct navigation but often for search and reverse engineering tasks.

Another observation was that inexperienced developers (\ie students) are more likely to find the map useful than professional developers. This might be explained by the hypothesis that to power users \emph{any} new way of using the IDE is likely to slow them down, and conversely to beginners \emph{any} way of using the IDE is  novel. The only exception to this observation was \Codemap's search bar, a one-click interface to \eclipse's native search, that was appreciated and used by all participants but one that preferred to use the search dialog.

One Participant also provided us feedback comparing his experience with \Codemap to that with the Moose analysis tool \cite{Nier05c}. He uses Moose at work after having attended a tutorial by a consultant. He said he prefers the immediate feedback of Codemap, and reported that \emph{``the gap between Moose and IDE is just too large, not to mention the struggle of importing Java code. Moose helps you to \eg find god-classes but this is typically not new to developers that know a system. Codemap seems more interesting as it integrates with what you actually do in the IDE as you program.''}

\subsection{Observations, Hypotheses, Implications}

In this section, we present an in-depth analysis of our observations, structured by triples of \emph{observation}, \emph{hypothesis}, and \emph{implication}. Implications are directed at improving the design and usability of spatial visualizations that are embedded in an IDE.
% ---- ----- ----- ----- ----- ---- ---
\paragraph{Observation 6.1: When thinking aloud, developers did speak of the system's architecture in spatial terms} 

%\begin{figure}
%\begin{center}
%  \includegraphics[draft,width=\linewidth]{fig/wrapup-map}
%\end{center}
%    \caption{\emph{Spatial map of the system, drawn by one after the participants in the the wrap-up session. All utility packages are located in the upper left corner, separated by a jagged line.}}
%    \label{fig:wrapupmap}
%\end{figure}

The think-aloud protocol revealed that participants refer to the system's architecture in spatial terms. Professional participants referred to packages as being above, below, or at the some level as one another. Some of them even did so before recovering the system's 3-tier architecture in task \#3. Most professionals referred to utility packages a being spatially beside or outside the layered architecture. 

For example, participant~T located all utility packages in the upper left corner, separated by a jagged line. While doing so, he made a gesture as if pushing the utility classes away and stated, \emph{``I am putting them up here because to me they are somehow beside the system.''}

Students on the other hand made much fewer references to the system's architecture, both spatial as well as in general. They were typically reasoning about the system at the level of classes and source lines, rather than in architectural terms. The maps drawn by students in the wrap-up phase, however, showed similar spatial structure to those of the professionals. It remains thus open whether students established a genuine spatial model while working with the code (as we observed for professionals) or only because they were asked to draw the wrap-up maps.

\paragraph{Hypothesis 6.1: Professional developers do establish a spatial mental model of the system's architecture}

Based on above observations there is evidence to assume that professional developers establish a spatial mental model of the system's architecture as they work with code. Furthermore, they do so even without visual aids, since they use spatial terms and thinking even before being asked to draw a diagram of the system's architecture.

%\ewe{A few more questions: do devs cluster the things in similar ways, but positions the clusters relatively to one other in a different way? Or is the clustering different among people? Is there a natural tendency to position clusters one way or the other, e.g. UI on the left, Data on the right, Utility at the bottom? If you can say something about that it would be great}

\paragraph{Implication 6.1: Developers should be able to arrange the layout according to their mental model}

This has implications on the design of a system's spatial visualization. Developers should be able to arrange the layout according to their mental model. Developers should be able to drag and move parts of the map around as they wish, rather than having to stick with the automatically established layout. Code Canvas \cite{Deli10a} and Code Bubbles \cite{Brag10b} both already address this implication. In those tools, the user may drag individual elements around and arrange them according to his mental model. 

We observed that developers referred to architectural components, but not classes, in spatial terms. The needs of developers might thus be even better served by providing them more high-level means of arranging the map. Our next prototype will use \emph{anchored multidimensional scaling} such that developers may initialize the map to their mental model. Anchored MDS allows the developer to define anchors which influence the layout of the map \cite[Sec 4.4]{Buja08a}. Any software artifact can be used as an anchor (as long as we can compute a its distance to artifacts on the map), even for example external libraries. In this way, developers might \eg arrange the database layer in the south and the UI layer in the north using the respective libraries as anchors.

\paragraph{Observation 6.2: Participants used Codemap to assess quantity and dispersion of search results and call graphs}

The feature of Codemap that was used most often, by both professionals and students, was the illustration of search results and call graphs. Participants reported that they liked the search-result support of the map, explaining that it gives them much faster initial feedback than Eclipse's tabular presentation of search results. Many participants reported that it was \emph{``as if you could feel the search results,''} and that \emph{``you get an immediate estimate how much was found, whether it is all one place or scattered all over the place.''}

\autoref{fig:fatarrow} illustrates one particular ``Aha moment'' as encountered by participant~T during task 4-b, \ie location of security features: Upon opening the call hierarchy, a huge call-arrow appeared on the map: indicating dozens of individual calls that connect the security-related archipelago in the south-west with the \texttt{TreeFactory} island in the east. Given the visual evidence of this arrow, the participant solved the task immediately without further investigation of the system.

\paragraph{Hypothesis 6.2: Intuitive visualization to show quantity and dispersion of search results (as well as call graphs) address an important need of developers}

%\ewe{I would maybe generalize it and say: Intuitive visualization to show the quantity and dispersion.... I could imagine that the package explorer could be enriched with a smart way to show search result. E.g. packages with many hits would be red, while packages with few hits would be gray. Or a smart algorithm that would unfold the package to show the relevant class hierarchies. Or something else}

Given the above observation it seems clear that developers have urgent needs for better representation of search results than tabular lists. We found that both students and professionals used the map to get an immediate estimation of search results. This is most interesting since otherwise their use of the tabular search results differed: Professionals glanced at the results, inspected one or maybe two results, and then either accepted or rejected their hypothesis about the system, while students would resort to a linear search through all search results, not daring to reject a hypothesis on the grounds of one or two inspected results only.

Given the map's illustration of search results however, the behavior of both groups changed. Students dared to take quick decisions from a mere glance at the map, whereas professionals were more likely to inspect several results. One professional reported that he \emph{``inspected more results than usual, because the map shows them in their context and that this helps him to take a more informed choice on which results are worth inspection and which ones not.''}

\paragraph{Implication 6.2: Tools should put search results into a meaningful context, so developers can take both quicker and better-informed decisions}

The need for better presentation of search results has implications beyond the design of spatial visualizations. Work on presentation of search results goes beyond spatial maps \cite{Hear09a}, for example results can be presented as a graph. Poshyvanyk and Marcus \cite{Posh07a} have taken one such approach (representing search results as a lattice) and applied it to source code search with promising results. 
%\ewe{Can't agree more}

For our next prototype we plan to integrate search results into the package explorer view, just as is already done with compile errors (which are, from this point of view, just like the search results of a complex query that is run to find syntax errors). This planned feature addresses another implication of our study as well, as we have found that some developers establish a spatial memory of the package explorer view. It therefore makes sense to mark search results both on our map as well as in the explorer view. 
%\ewe{Can't agree more}

\paragraph{Observation 6.3: When interacting with the map, participants were attracted to isolated elements, rather than exploring clusters of closely related elements}
 
We found that participants are more likely to inspect easily discernible elements on the map. They are more likely to notice and interact with an isolated island rather than with elements that are part of a larger continent. Unfortunately, it is exactly dense and strongly correlated clusters that contain the most interesting parts of the system! When investigating this issue, participants answered that \emph{``those (isolated) elements looked more important as they visually stick out of the rest of the map.''} 
 
Also, when working with another system that had (unlike the present study) a large cluster in the middle surrounded by archipelagos on the periphery, we found that users started their exploration with isolated hills in the periphery, only then working their way towards the more dense cluster in the middle. 

\paragraph{Hypothesis 6.3/a: Developers avoided clusters of closely relates elements because they are difficult to identify and select on the map}

All participants had difficulties to open files by clicking on the map. They had difficulties to select classes on the map when they are in a crowded cluster. They would click in the middle of a label, but often the labels are not centered, which is an unavoidable artifact of any labeling algorithm, and thus the clicks would open a different (unlabeled) class.

Codemap does provide tooltips, however participants did not use them. From observing their work it was obvious why: both students and professionals were working at such a speed that waiting for a tooltip to appear would have totally taken them out of their workflow. 

\paragraph{Observation 6.3/b: Participants rarely used Codemap to return to previously visited locations, instead using package explorer and ``Open Type'' to do so}

Contrary to our assumptions, participants did not use the map to return to previously visited locations by recalling them from spatial memory. Some would use the map, but only for exposed classes that are easily recognizable and clickable. This observation is related to the previous one.

We found however some participants established a spatial memory of the package explore\,---\,and did so \emph{in addition to their spatial model of the system's architecture!} For example, participant~S would drill down with the explorer saying ``let's open that class down there'' or ``there was this class up here.'' Over the course of the experiment he got quicker at navigating back to previously visited classes in the package explorer. Other participants, as for example participant~T, relied on lexical clues and made extensive use of Eclipse's ``Open Type'' dialog to find their way back to previously visited classes.

Usability glitches will of course worsen the effect of (or might even be the main cause of) not using the map for navigation and revisiting classes. From this it follows that:

\paragraph{Implication 6.3: The map's layout should be such that all elements are easily discernable and easy to click}

Real estate on a computer screen is limited, and even more so in an IDE with all its views and panels. As tool builders we have limited space available for an embedded visualization. Given our goal of establishing a global layout we face the challenge of having to visualize all elements of a system in that very limited space. 

The current implementation of Codemap has one level of scale only, which may yield crowded clusters where elements are placed just pixels apart. A zoomable map as provided by Code Canvas \cite{Deli10a} addresses this issue. 

The fact that we are attracted by elements that visually detach from other has two impacts: one is that we tend to look at isolated elements as being of low significance, the other being that it is hard to identify elements in the cluster. These impacts are very different, but can both be addressed in a common way.
% Both are very different, but might be consolidated.
For instance, a threshold could be used to not show isolated elements at all, but only significant clusters. Alternatively, colors may be used to display isolated elements so that they do not draw our attention so readily.

%\ewe{I'm a bit puzzled with the formulation of 6.3. The fact that we are attracted by elements that visually detach from other has two impacts: (1) one is the we tend to look at isolated element of low significance, the other (2) that it's hard to identify elements in the cluster. Both are very different. The hypothesis  is more related to (1) while hypothesis and implication relates more to (2). The argumentation should be consolidated, IMHO. For instance, a threshold could be used to not show isolated elements at all, but only significant clusters. That would address (1), but not (2). Or the usage of colors, to display isolated element so that they don't attract our look }

%\paragraph{Observation 6.4: Participants rarely used Codemap to return to previously visited locations, instead using package explorer and ``Open Type'' to do so}
% 
%\paragraph{Hypothesis 6.4:  Developers failed to establish a spatial memory of the map not due to its layout but also due to missing visual clues}
%
%In general, the issue of usability is orthogonal to the map's layout. For example, offering ``search as you type'' might help to raise map interaction for those developers that mainly rely on lexical clues, no matter which base layout is used. It was our impression that any exploratory user study of embedded software visualization will be dominated just as by usability issues as by technical factors, such as your choice of layout algorithm. \ewe{I don't get this one}
%
%\paragraph{Implication 6.4: It should be possible to bookmark classes as ``top landmarks,'' both manually and automatically based on usage statistics}
% 
%What might help as well to ease the retrieval of previously visited classes is a ``top landmarks'' feature where you can manually (but also automatically based on visits) set markers on the map as starting points for further activities. We plan to work on this for our next prototype. \ewe{The apparent ability to remember position in the package explorer but not in the map may be due to other aspects. I'm wondering for instance if providing a grid in the map with row A-F, and column 0-16 would help to remember the position in the map. Maybe yes, maybe not. In the real-world analogy, maps have latitude and longitude. With never remember them, but help us to slice the map. We maybe need some rigidity to be able to anchor position in our mental model. The package explorer is too rigid, and the map is to flexible/free.} \ewe{All that to say that I don't quite get the link form observation 6.4 to implication 6.4}
%
%\ewe{Actually, I'm wondering if dropping 6.4 altogether wouldn't make the argumentation stronger. To me the main points are 6.1, 6.2, 6.3, 6.5}.

\paragraph{Observation 6.5: Participants used Codemap as if its layout were based on package structure\,---\,even though they were aware of the underlying topic-based layout}

Developers assume that packages are a valid decomposition of the system and expect that the layout of the spatial visualization corresponds to the package structure. We found that clustering classes by topic rather than packages violates the ``principle of least surprise.'' We observed that participants tended to interpret visual distance as a measure of structural dependencies\,---\,even though they were aware of the underlying lexical implementation!
 
Participants expected the layout to reflect at least some structural property. Most of them reacted surprised or confused when for example the classes of a package were not mostly in the same place. For example, Participant~S reported in the wrap-up, \emph{``this is a very useful tool but the layout does not make sense".} Another participant stated during task 3 (\ie the architecture recovery) with confusion that \emph{``the classes contained in packages are scattered on the map, it is not obvious what their spatial connection is.''}
 
\paragraph{Hypothesis 6.5: From the developers view, the predominant mental decomposition of a system is package structure}

Given our reverse engineering background  \cite{Nier05c,Kuhn07a,Duca09c} we had come to distrust package decomposition, however it seems that developers like to rely on the packaging that other developers have made when designing the system.

One problem raised by research in re-packaging legacy systems is that packages play too many roles: as distribution units, as units of namespacing, as  working sets, as topics, as unit of architectural components, etc. However, as an opposing point of view, we can relate packaging to the folksonomies of the Web 2.0, where users label elements with unstructured tags that are then exploited by other users to search for elements. In the same way, we could say that putting trust into a given package structure is a way of collaborative filtering. Developers assume that other developers had made the same choice as they would when packaging the system. 

\paragraph{Implication 6.5: The map layout should be based on code structure rather than latent topics only. However, non-structural data should be used to enrich the layout}

When running the user study, it became quickly apparent that we should revise our initial assumption that lexical similarity is a valid dissimilarity metric for the spatial layout. This was the strongest feedback, and as is often the case in exploratory user studies, already obvious from watching the first professional participant for five minutes only. From all participants we got the feedback that they expect the layout to be structural and that our clustering by topics kept surprising them even after working with the map for almost two hours.
 
Still we think that spatial layouts that go beyond package structure are worthwhile. Therefore, we propose to enrich structure-based layout with non-structural data, such as design flaws. For future work, we are about to refine our layout algorithm based on that conclusion. The new layout is based on both lexical similarity and the ideal structural proximity proposed by the ``Law of Demeter'' (LOD). This is a design guideline that states that each method should only talk to its friends, which are defined as its class's fields, its local variables and its method arguments. Based on this we can defined an {\itshape idealized} call-based distance between software artifacts. Given a LOD-based layout, software artifacts are close to one another if they are supposed to call one another and far apart if they better should not call one another. Thus we get the desired property that visualizing call-graphs conveys meaningful arrow distances. On a LOD-based map, any long-distance-call has a diagnostic interpretation that helps developers to take actions: Long flow-map arrows indicate calls that possibly violate the ``Law of Demeter''.

%\begin{table*}
%\begin{center}
%  \includegraphics[width=\linewidth]{fig/codemap2010-userstudy}
%\end{center}
%    \caption{\emph{Collected data of three selected participants. From top to bottom, categories are: success rate, perceived difficultly, perceived usefulness of the Codemap plug-in, and observed use of IDE features (a single cross indicates use of that feature, and a double cross indicates that the given feature was used the most).}}
%    \label{fig:table}
%\end{table*}

%=========================================================
\section{Threats to Validity}
\label{sec:discussion}

This section summarizes threats to validity. The study had a small sample size (3 students, 4 professionals) and might thus not be representative. We manually evaluated the data, results might thus be biased. Nevertheless, results are promising and running a pilot think-aloud study with a small user group is a state-of-the-art technique in usability engineering to learn learn about the reactions of users. Such pilot studies are typically used as feedback for further iteration of the tool and to assess the usefulness of its application \cite{Niel03a}.

In this section, we discuss observations regarding the different performance of students and professional participants that are reported in the task performance subsection just below. We found no correlation between codemap interaction and success rates, rather success was completely dominated by the participant's programming experience. Professional participants succeeded in all tasks, where as students failed more than half of all tasks. 
%
Professional developers are very focused, \chg{the} they mainly used keyboard shortcuts only and \chg{goes} go very fast, he seems to ignore not only the map but most parts of the IDE as he goes along, our impression is that he is very focused and always knows exactly where to look next. Professional developers only used a limited set of IDE means to achieve a task, whereas students would typically use all available means (of which they know of, we found that some features where unknown to the students, whereas on the other hand we learned new usages of Eclipse from the professionals. This is a serious challenge for researches, imagine for example that you'd ask your control group to solve a taks in Excel but you are unaware of for example \chg{the} of the Pivot table feature and thus etc).
%
Professional developers would formulate hypotheses first and very quickly and either accepts or rejects them with typically looking at one or two search results only, almost all their conclusions are correct but if they are not they are just as quick to abandon them and move on to the next hypothesis. Where as students, when facing an incorrect hypothesis would stick with it and turn to exhaustive linear search through the system looking for the missing evidence. Similar but not as critical they would often look for more than one prove of a correct hypothesis. \cite{Holmes...or Fritz}
%
As most researchers in computer science do have a back ground as software engineers, we are often tempted to assume that we proficient software engineers ourselves. Watching the think-aloud footage of the professional participants has though as a better, their use of the IDE showed a proficiency and efficiency that was magnitudes beyond ours and challenged many of our assumptions regarding the use of software maps in an IDE. 
%
It is a known problem to recruit professional participants for user studies, but given our experience with the think-aloud protocol we think it is safe to assume that a study with even few professionals has more predictive power than any study with dozens of students. The needs of seasoned developers are just so different from those of students that still struggling to learn their profession. 
%
One of the most striking difference between students and professionals was that a professional would never do any click or action in the IDE without having a clearly stated hypothesis of what is going on, while students soon turned into repeated ``linear search'' patterns when facing problemens they could not understand. 
%
On the other hand, the students shows a much better adoption rate of the software visualization the the professionals. That was not unexpected, since to power users any new way of using the IDE is likely to slow them down, and conversely to beginners any way of using the IDE is  novel. This raises the question of incremental versus revolutionary improvements: is it possible to prove the usefulness of a revolutionary improvement in a user study? Clearly, for well-defined incremental improvements we can measure the benefit with controlled and repeatable experiments. However, some revolutionary improvements are likely to be rejected by professionals for being not enough in line with their habits but as well as are likely to show no measurable improvement for students, since students are still struggling to learn their profession rather than being able to perform controlled tasks.
%
%\ewe{Need to be proof-read}

\chapter{The Codemap Chapter}

Software visualizations can provide a concise overview of a complex software system.
Unfortunately, since software has no physical shape, there is no ``natural'' mapping of software to a two-dimensional space. As a consequence most visualizations tend to use a layout in which position and distance have no meaning, and consequently layout typically diverges from one visualization to another.
We propose an approach to consistent layout for software visualization, called \emSOCA, in which the position of a software artifact reflects its \emph{vocabulary}, and distance corresponds to similarity of vocabulary.
We use \LSI (LSI) to map software artifacts to a vector space, and then use \MDS (MDS) to map this vector space down to two dimensions.
The resulting consistent layout allows us to develop a variety of thematic \emph{software maps} that express very different aspects of software while making it easy to compare them.
The approach is especially suitable for comparing views of evolving software, since the vocabulary of software artifacts tends to be stable over time.
We present a prototype implementation of \SOCA, and illustrate its use with practical examples from numerous open source case studies.

Software visualization offers an attractive means to abstract from the complexity of large software systems.
A single graphic can convey a great deal of information about various aspects of a complex software system, such as its structure, the degree of coupling and cohesion, growth patterns, defect rates, and so on \cite{Dieh07a,Kien07a,Reis05a,Stor05a}.
Unfortunately, the great wealth of different visualizations that have been developed to abstract away from the complexity of software has led to yet another source of complexity: it is hard to compare different visualizations of the same software system and correlate the information they present.

We can contrast this situation with that of conventional thematic maps found in an atlas.
Different phenomena, ranging from population density to industry sectors, birth rate, or even flow of trade, are all displayed and expressed using \emph{the same consistent layout}.
It easy to correlate different kinds of information concerning the same geographical entities because they are generally presented using the same kind of layout.
This is possible because (i) there is a natural mapping of position and distance information to a two-dimensional layout\footnote{Even if we consider that the Earth is not flat on a global scale, there is still a natural mapping of position and distance to a two-dimensional layout; see the many types of cartographic projections (\eg the Mercator projection) used during centuries to do that. In fact, this is true for a large class of manifolds.}, and (ii) because by convention North is normally considered to be on the top.\footnote{The orientation of modern world maps, that is North on the top, has not always been the prevailing convention. On traditional Muslim world maps, for example, South used to be in the top. Hence, if Europe would have fallen to the Ottomans at the Battle of Vienna in 1683, all our maps might be drawn upside down by now \cite{Hite99a}.}

Software artifacts, on the other hand, have no natural layout since they have no physical location.
Distance and orientation also have no obvious meaning for software.
It is presumably for this reason that there are so many different and incomparable ways of visualizing software.
A cursory survey of recent \textsc{Softvis} and \textsc{Vissoft} publications shows that the majority of the presented visualizations feature arbitrary layout, the most common being based on alphabetical order and \emph{arbitrary hash-key order}.
(Hash-key order is what we get in most programming languages when iterating over the elements of a Set or Dictionary collection.)

Consistent layout for software would make it easier to compare visualizations of different kinds of information. But what should be the basis for positioning representations of software artifacts within a ``cartographic'' software map?
What we need is a semantically meaningful notion of position and distance for software artifacts, a spatial representation of software in a multi-dimensional space, which can then be mapped to consistent layout on the 2-dimensional visualization plane.

We propose to use \emph{vocabulary} as the most natural analogue of physical position for software artifacts, and to map these positions to a two-dimensional space as a way to achieve consistent layout for software maps.
Distance between software artifacts then corresponds to distance in their vocabulary.
Drawing from previous work \cite{Kuhn07a,Duca06c} we apply \LSI (LSI) \cite{Deer90a} to the vocabulary of a system to obtain $n$-dimensional locations, and we use \MDS (MDS) \cite{Borg05a} to obtain a consistent layout.
Finally we use cartography techniques (such as digital elevation, hill-shading and contour lines) to generate a landscape representing the frequency of topics. We call our approach \emSOCA, and call a series of visualizations \emph{Software Maps}, when they all use the same consistent layout created by our approach. 


Why should we adopt vocabulary as distance metric, and not some structural property?
First of all, vocabulary can effectively \emph{abstract} away from the technical details of source code \cite{Kuhn07a} by capturing the key domain concepts of source code. Software entities with similar vocabulary are conceptually and topically close. Lexical similarity has proven useful to detect high-level clones \cite{Marc01a} and cross-cutting concerns \cite{Pali08a} in software. Furthermore, it is known that over time vocabulary tends to be more stable than the structure of software \cite{Anto07a}, and tends to grow rather than to change \cite{Vasa07b}. Although refactorings may cause functionality to be renamed or moved, the overall vocabulary tends not to change, except as a side-effect of growth. This suggests that vocabulary will be relatively \emph{stable} in the face of change, except where significant growth occurs. As a consequence, vocabulary not only offers an intuitive notion of position that can be used to provide a consistent layout for different kinds of thematic maps, but it also provides a robust and consistent layout for mapping an evolving system. System growth can be clearly positioned with respect to old and more stable parts of the same system.

This paper is an extension of previous work, in which we first proposed \emSOCA for consistent layout of software visualizations \cite{Kuhn08b}. The main contributions of the current paper are:

\begin{itemize}
\item \emph{Improved algorithm.} In our previous work we presented a technique to create software maps given either a single release, or all releases of a system at once. In this paper we propose an improved algorithm for incremental software maps that update as new changes appear.
\item \emph{Visual stability.} In our previous work we introduced \SOCA as an approach to achieve consistent layouts for software visualization. In this paper we evaluate four open source case studies to investigate the visual stability of our approach over the evolution of a system. 
\item \emph{Desiderata for spatial representation.} We present a generalization of DeLine's desiderata for spatial software navigation~\cite{Deli05b} to spatial representation in general, and complete them with the desiderata that visual distance should have a meaningful interpretation.
\end{itemize}

% ===================================================================================
\section{SOFTWARE CARTOGRAPHY}
\label{sec:techniques}

In this section we present the techniques that are used to achieve a consistent layout for software maps. We present two variations of \SOCA, an \emph{offline} algorithm that requires that all releases of a software system are available upfront, and an improved \emph{online} algorithm that updates the layout incrementally as new releases of the system appear. 

\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{codemap-pipeline}
\end{center}
    \caption{\SOCA in a nutshell: left) the raw text of source files is parsed and indexed using \LSI, center) the high-dimensional term-document-matrix is mapped down to two dimensions using \MDS, and right) cartographic visualization techniques are used to render the final map.}
    \label{fig:soca}
\end{figure}

The general approach of \SOCA, as illustrated in \autoref{fig:soca}, is as follows:
\begin{enumerate}
\item We parse the vocabulary of source files into term-frequency histograms. All text found in raw source code is taken into account, including not only identifiers but also comments and literals.
\item We transform the term-frequency histograms using \LSI (LSI) \cite{Deer90a}, an information retrieval technique that resolves synonymy and polysemy. 
\item We use \MDS (MDS) \cite{Borg05a} to map the term-frequency histograms onto the 2D visualization pane. This preserves the lexical co-relation of source files as well as possible.
\item We use cartographic visualization techniques to render an aesthetically appealing landscape.
\end{enumerate}

Possible applications of \SOCA in the software development process are, \dots

\begin{itemize}
\item \dots to navigate within a software system, be it for development or analysis.
\item \dots to relate different metrics to each other, \eg search results and bug prediction.
\item \dots to stay in touch with other developers of your team, by showing open files of other developers.
\item \dots to understand a systems domain upon first contact.
\item \dots to explore a system during reverse engineering.
\end{itemize}

\noindent We implemented a prototype of our approach, \CODEMAP, which is available as an open source project. \CODEMAP was originally programmed in Smalltalk, in the mean time development has been moved to Java. \CODEMAP is available as an Eclipse plug-in\footnote{\url{http://scg.unibe.ch/codemap}}.

\subsection{Iterative Online \SOCA}

In our previous work we presented a technique to create software maps given either a single release, or all releases of a system at once \cite{Kuhn08b}. In this paper we propose an improved algorithm for incremental software maps that update as new changes appear. 

The offline scenario processes all releases in one pass. 

\begin{itemize}

\item The offline variation processes all releases of a software system at once, up to and until \MDS. Only then the location data is grouped by release, and an separate map for each release is rendered. That is both lexical similarity as well as position on the map anticipate all future evolvements from the first map on, since indexing and scaling take information of all releases into account. This scenario requires that information about all releases is available, which is given when performing post-mortem analysis of an existing system.

\item The online variation processes the input release by release (or, when integrated into a development environment, even change by change). For each release, the current source as well as information carried over from the previous \SOCA computation are used. In a first step, the source files of the current release and of previous releases are processed by \LSI. This allows removed and added topics to be detected. In the next step, the lexical similarity of all current source files is fed into \MDS---together with the positions on the previous map as starting points, thus leading to more visual stability.

\end{itemize}

Given a series of releases both variations yield a visually stable sequence of maps. Maps generated with the iterative algorithm are less stable over time compared to the post-mortem approach. The instability of the iterative approach decreases over time, since the amount of accumulated historical data increases with each release. In general, the iterative algorithm is less sensitive to the addition or removal of entire topics between releases, such changes are better observed by performing post-mortem analysis. Please refer to the evaluation in Section~\ref{sec:hausdorff} for more details.

%For example, the JUnit case-study in section  (Hausdorff distance $0.25 \pm 0.11$ versus $0.10 \pm 0.14$).

% -----------------------------------------------------------------------------------
\subsection{Lexical Similarity between Source Files}
\label{sec:lsi}

As motivated in the introduction, the distance between software entities on the map is based on the lexical similarity of source files. Lexical similarity is an Information Retrieval (IR) technique based on the vocabulary of text files. Formally, lexical similarity is defined as the cosine between the term frequency vectors of two text documents. That is, the more terms (\ie identifiers names and operators, but also words in comments) two source files share, the closer they are on the map. 

First, the raw source files are split into terms. Then a matrix is created, which lists for each document the occurrences of terms. Typically, the vocabulary of source code consists of 500--20'000 terms. In fact, studies have shown that the relation between term count and software size follows a power law \cite{Zhan08a}. For this work, we consider all text found in raw source files as terms. This includes class names, methods names, parameter names, local variables names, names of invoked methods, but also words found in comments and literal values. Identifiers are further preprocessed by splitting up the camel-case name convention which is predominantly used in Java source code. Note that since our approach is based on raw text, any programming language that uses textual source files might  be processed.

In a next step, \LSI \cite{Deer90a} is applied to reduce the rank of the term-document matrix to about 50 dimensions. LSI is able to resolve issues of synonymy and polysemy without the use of predefined dictionaries. This is advantageous for the vocabulary of source code which often deviates from common English usage. For more details on \LSI and lexical similarity, please refer to our previous work on software clustering \cite{Kuhn07a}.

% -----------------------------------------------------------------------------------
\subsection{Multi-dimensional Scaling}

In order to visualize the lexical similarity between software entities, we must find a mapping that places source files (or classes, or packages, depending in our definition of a document) on the visualization pane. The placement should reflect the lexical similarity between source files.

We use \MDS (MDS) in order to map from the previously established multi-dimensional term-document matrix down to two dimensions.
 \MDS tries to minimize a stress function while iteratively placing elements into a low-level space. \MDS yields the best approximation of a vector space's orientation, \ie it preserves the distance relation between elements as best as possible. This is good for data exploration problems.

Note that \MDS is not a force-based graph layout algorithm. MDS does not operate on a graph of vertices and edges. MDS maps elements from a high-dimensional metric space to a low-dimensional metric space. In this work, the high-dimensional space is a term-document matrix using Pearson-8 as metric\footnote{When computing the lexical similarity between text documents, it is important to use a cosine or Pearson distance metric. The standard Euclidian distance has no meaningful interpretation when applied to term-document vector space!} and the low-dimensional space a visualization pane with Euclidian metric.

\newlength{\figwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{codemap-pipeline2}
\end{center}
    \caption{Construction steps: left) MDS placement of files on the visualization pane, middle) circles around each file's location, based on class size in KLOC, right) digital elevation model with hill-shading and contour lines. Sidebox on digital elevation model) each file contributes a Gaussian shaped basis function to the elevation model according to its KLOC size. The contributions of all files are summed up to a digital elevation model.}
    \label{fig:steps}
\end{figure}

This work applies High-throughput MDS (Hit-MDS), which is an optimized implementation of MDS particularly well-suited for dealing with large data sets \cite{Stri05a}. The algorithm was originally designed for clustering multi-parallel gene expression probes. These data sets contain thousands of gene probes and the corresponding similarity matrix dimension reflects this huge data amount. The price paid for fast computation is less accurate approximation and a simplified distance metric.

% -----------------------------------------------------------------------------------
\subsection{Cartographic Visualization Techniques}

Eventually, we use hill-shading \cite{Burr98a} to render an aesthetically appealing landscape. \autoref{fig:steps} illustrates the final rendering steps of \SOCA. On the final map, each source file is rendered as a hill whose height corresponds to the entity's KLOC size.

Hill-shading uses a digital elevation model (DEM) to render the illumination of a landscape. The digital elevation model is a two-dimensional scalar field. Each each entity contributes a Gaussian shaped basis function to the elevation model. To avoid that closely located entities occlude each other, the contributions of all files are summed up as shown in \figref{steps}. 

\begin{figure*}
  \includegraphics[width=\linewidth]{ludo-history.png}
  \caption{From left to right: each map shows an consecutieve iteration of the same software system. As all four views use the same layout, a user can build up a mental model of the system's spatial structure. For example, {\tt Board/LudoFactory} is on all four views located in the south-western quadrant. See also Figure~5 and 6 for more views of this system.}
  \label{fig:ludo}
\end{figure*}


A map without labels is of little use. On a software map, all entities are labeled with their name (class or file name). Labeling is a non-trivial problem, as an algorithm is needed to ensure that labels do not overlap. Also labels should not overlap important landmarks. Most labeling approaching are semi-automatic and need manual adjustment, an optimal labeling algorithm does not exist \cite{Sloc05a}. 

For locations that are near to each other it is difficult to place the labels so that they do not overlap and hide each other. For software maps it is even harder due to often long class names and clusters of closely related classes. This work uses a greedy brute-force algorithm for labeling. Labels are placed in order of hill size, \ie the name of the largest file is placed first, and so on. If a to-be placed label would overlap with an already placed label, the to-be placed label is omitted. Thus, the labels of smaller files are typically omitted in favor of the labels of larger files.

%\begin{figure}
%\begin{center}
%  \includegraphics[width=\linewidth]{figures/workflow2}
%\end{center}
%    \caption{\SOCA variations: (top) the offline approach processes all releases of a software system at once, and eventually renders a separate map for each release; (bottom) the iterative approach processes all source files up to the current release with LSI, and when applying MDS reuses the previous map as starting configuration.}
%    \label{fig:soca2}
%\end{figure}

% =============================================================================
\section{ON THE CHOICE OF VOCABULARY}
\label{sec:onvocabulary}

The decision to use a distance based on lexical similarity does, indeed, create a distribution of distances that should not change a lot in time. This is because programmers will not use a completely new set of lexical tokens in each new version of the software. In fact, it has been shown that over time vocabulary tends to be more stable than the structure of software \cite{Anto07a}. However, this also will create software maps that naturally only can show how items are similar from a lexical point of view.

The map layout as presented in this work can, of course, be used to see how items are related from the point of view of some other distance, such as considering structural similarity, similarity with regard to a complexity or testability metric. In that case, the distance may vary a lot over time during the evolution of a product, and this will create unstable layouts. The focus of this work, however, is the creation of maps that help programmers to establish a stable mental model of their software system under work. In any case, if maps  based on other metrics are ever to be used in conjunction with vocabulary-based \SOCA maps, we strongly recommend to visually distinguish them by using another rendering scheme. This helps to reduce the likeliness that programmers confuse the spatial layout of these other maps, with the mental model acquired through the use of \SOCA maps. 

As mentioned in the introduction, \SOCA is vocabulary-based because vocabulary can effectively \emph{abstract} away from the technical details of source code \cite{Kuhn07a} by capturing the key domain concepts of source code. The assumption being that software entities with similar vocabulary are conceptually and topically close. Consider, for example, programming languages and software where name overloading is applied. Even though overloaded methods differ in their implementation strategy, they will typically implement the same concept using the same vocabulary. In fact, lexical similarity has proven useful to detect high-level clones \cite{Marc01a} and cross-cutting concerns \cite{Pali08a} in software.  

Due to name scoping, semantically different scopes can have identical names with different meanings. Consider, for example, two large functions having mostly identifiers such as i, j, prev, next, end, stop, flag, \dots;  the one does some matrix computations, while the other is a hash-table implementation. Without the application of \LSI (\secref{lsi}) the two would be classified as being very similar, while this is clearly not true from a developer's perspective. \LSI, however, can identify words that have different meaning depending on their context. LSI has the ability to resolve certain synonymy and polysemy \cite{Deer90a}.


Although refactorings may cause functionality to be renamed or moved, the overall vocabulary tends not to change, except as a side-effect of growth \cite{Zhan08a,Vasa07b}. Consider the example of a rename refactoring. Two effects may occur. 
In the first case, all occurrences of a symbol are replaced with new symbol. This will not affect the map, since both lexical similarity and LSI are based on statistical analysis only. Replacing all occurrences of one term with a new term is, from the point of these IR technologies, a null operation. In the second case, some occurrences of a symbol are replaced with another symbol which is already used. This will indeed affect the layout of the map. Given that the new name was well chosen by the programmer, the new layout constitutes a better representation of the system. On the other hand, if the new name is a bad choice, the new layout is flawed. However, what constitutes bad naming is not merely a matter of taste. Approaches that combine vocabulary with structural information can indeed assess the quality of naming. Please refer to H\o{}st's recent work on debugging method names for further reading \cite{Hoes09a}.

Not considered in the present work is the relative weight of different lexical tokens. For example, it seems reasonable to weight local identifiers differently than identifiers in top-level namespaces. Also, one may treat names coming from library functions different from the ones coming from the actual user code. Given the absence of evaluation benchmarks, we decided to use equal weighting for all lexical token. Also, preliminary experiments with different weighting schemes indicate that relative weights below boost level, \ie below a factor of 10, do often not significantly affect the overall layout.

% =============================================================================
\section{EXAMPLES}
\label{sec:casestudy}

In this section we present examples of \SOCA. The first example visualizes the evolution of a small software system. The second example shows an overview of six open-source systems. As the third example, we provide two thematic overlays of the same software map.

% -----------------------------------------------------------------------------------
\subsection{The Evolution of Ludo}

Figure~\ref{fig:ludo} shows the complete history of the Ludo system, consisting of four iterations. Ludo is used in a first year programming course to teach iterative development (please mail the first author to get the sources). The 4th iteration is the largest with 30 classes and a total size of 3-4 KLOC. We selected Ludo because in each iteration, a crucial part of the final system is added. 

\begin{itemize}

\item The first map (\autoref{fig:ludo}, leftmost) shows the initial prototype. This iteration implements the board as a linked list of squares. Most classes are located in the south-western quadrant. The remaining space is occupied by ocean, nothing else has been implemented so far.

\item In the second iteration (\autoref{fig:ludo}, second to the left) the board class is extended with a factory class. In order to support players and stones, a few new classes and tests for future game rules are added. On the new map the test classes are positioned in the north-eastern quadrant, opposite to the other classes. This indicates that the newly added test classes implement a novel feature (\ie testing of the game's ``business rules'') and are thus not related to the factory's domain of board initialization. 

\item During the third iteration (\autoref{fig:ludo}, second to the right) the actual game rules are implemented. Most rules are implemented in the {\tt Square} and {\tt Ludo} class, thus their mountain rises. In the south-west, we can notice that, although the {\tt BoardFactory} has been renamed to {\tt LudoFactory}, its position on the map has not changed considerably. 

\item The fourth map (\autoref{fig:ludo}, rightmost) shows the last iteration. A user interface and a printer class have been added. Since both of them depend on most previous parts of the application they are located in the middle of the map. Since the new UI classes use vocabulary from all parts of the system, the islands are joined into a continent.

\end{itemize}

The layout of elements remains stable over all four iterations.  For example, {\tt Board/LudoFactory} is on all four views located in the south-western quadrant. This is due to LSI's robustness in the face of synonymy and polysemy; as a consequence most renamings do not significantly change the vocabulary of a software artifact \cite{Kuhn07a}.

%\setlength{\figwidth}{0.32\textwidth}
%\begin{figure*}
%\begin{center}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{apache-tomcat-bw}\\
%  Apache Tomcat
%\end{center}
%\end{minipage}~
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{columba-bw}\\
%  Columba
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{google-taglib-bw}\\
%  Google Taglib
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{j-fdp-bw}\\
%  JFtp
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{JoSQL-bw}\\
%  JoSQL
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{figures-bw/jcgrid-bw}\\
%  JCGrid
%\end{center}
%\end{minipage}
%\end{center}
%\vspace{1ex}
%    \caption{Overview of the software maps of six open source systems. Each map reveals a distinct spatial structure. When consequently applied to every visualization, the consistent layout may soon turn into the system's iconic fingerprint. An engineer might \eg point to the top left map and say: ``Look, this huge {\tt Digester} peninsula in the north, that must be Tomcat. I know it from last year's code review.''.
%    }
%    \label{fig:fullpage}
%\end{figure*} 

% -----------------------------------------------------------------------------------
\subsection{Open-source examples}

We applied the \SOCA approach to all systems listed in the field study by Cabral and Marques \cite{Cabr07a}. They list 32 systems, including 4 of each type of application (Standalone, Server, Server Applications, Libraries) and selected programming language (Java, .NET).  

Figure \ref{fig:fullpage} shows the software map for six of these systems: Apache Tomcat, Columba, Google Taglib, JFtp, JCGrid and JoSQL. Each system reveals a distinct spatial structure. Some fall apart into many islands, like JFtp, whereas others cluster into one (or possibly two) large contents, like Columba and Apache Tomcat. The 36 case-studies raised interesting questions for future work regarding the correlation between a system's layout and code quality. For example, do large continents indicate bad modularizations? Or, do archipelagoes indicate low coupling?

%Each system's size in TLC and KLOC is listed in Table~\ref{tab:six}.

%\begin{table}[htdp]
%\begin{center}
%\begin{tabular}{l|rr}
%\textbf{System} & \textbf{\# Top-level} & \textbf{KLOC} \\
% & \textbf{classes} & \\
%\hline
%Apache Tomcat & 162 & 14'700 \\
%Columba & 1'549 & 53'500 \\
%Google Taglib & 20 & 940 \\ 
%JFtp & 78 & 3'470 \\
%JCGrid & 94 & 3'630 \\
%JoSQL & 83 & 6'480 \\
%\end{tabular}
%\end{center}
%\vspace{1ex}
%\caption{Statistics of the six systems in Figure~\ref{fig:fullpage}. \AK{Are you sure this is KLOC? Does Google Taglib really have 0.9 million(!) lines of code in 20 classes only?}
%}
%\label{tab:six}
%\end{table}

% -----------------------------------------------------------------------------------
\subsection{Thematic cartography examples}

\begin{figure}
\begin{center}
\begin{minipage}{1.4\figwidth}
\begin{center}
  \includegraphics[width=1.4\figwidth]{ludo-v4-testtubes}
\end{center}
\end{minipage}
\begin{minipage}{1.4\figwidth}
\begin{center}
  \includegraphics[width=1.4\figwidth]{ludo-v4-testexecution}
\end{center}
\end{minipage}
\end{center}
    \caption{Software maps with thematic overlay: (left) glyphs are drawn on top of the map, to display additional information. Each test tube glyph indicates the location of unit test case, (right) invocation edges are drawn on top of the map, showing the trace of executing the {\tt RuleThreeTest} test case.}
    \label{fig:mock}
\end{figure}

Software maps can be used as canvas for more specialized visualizations of the same system. In the following, we provide two thematic visualization of the Ludo system that might benefit from consistent layout. (The maps in this subsection are mockups, not yet fully supported by \CODEMAP.)

\begin{itemize}

\item Boccuzzo and Gall present a set of metaphors for the visual shape of entities \cite{Bocc07a}. They use simple and well-known graphical elements from daily life, such as houses and tables. However they use conventional albeit arbitrary layouts, where the distribution of glyphs often does not bear a meaningful interpretation. The first map in Figure~\ref{fig:mock} (on the left) employs their technique on top of a software map, using test tubes to indicate the distribution of test cases.

\item Greevy \etal present a three-dimensional variation of System Complexity View to visualize a System's dynamic runtime state \cite{Gree05d}. They connect classes with edges representing method invocation, and stack boxes on top of each other to represent a class's instances.
Since System Complexity Views do not capture any notion of position, the lengths of their invocation edges do not express any real sense of distance.

Figure~\ref{fig:mock} (on the right) employs their approach on top of a software map, drawing invocation edges in a two-dimensional plane.
Here the distances have an interpretation in terms of lexical distance, so the lengths of invocation edges are meaningful.
A short edge indicates that closely related artifacts are invoking each other, whereas long edges indicate a ``long-distance call'' to a lexically unrelated class.

\end{itemize}

\section{Conclusion}

This paper presents \SOCA, a spatial representation of software. Our approach visualizes software entities using a consistent layout. Software maps present the entire program and are continuous. Software maps contain visual landmarks that allow developers to find parts of the system perceptually rather then relying on conceptual clues, \eg names. Since all software maps of a system use the same layout, maps with thematic overlays can be compared to each other.

The layout of software maps is based on the lexical similarity of software entities. Our algorithm uses \LSI to position software entities in an multi-dimensional space, and \MDS to map these positions on a two-dimensional display. Software maps can be generated to depict evolution of a software system over time. We evaluated the visual stability of iteratively generated maps considering four open source case studies.

In spite of the aesthetic appeal of hill shading and contour lines, the main contribution of this paper is not the cartographic look of software maps. The main contribution of \SOCA is (i) that cartographic position reflects topical distance of software entities, and (ii) that consistent layout allows different software maps to be easily compared.
In this way, software maps reflect world maps in an atlas that exploit the same consistent layout to depict various kinds of thematic information about geographical sites.

We have presented several examples to illustrate the usefulness of software maps to depict the evolution of software systems, and to serve as a background for thematic visualizations.
The examples have been produced using \TOOL, a proof-of-concept tool that implements our technique.

As future work, we can identify the following promising directions:
\begin{itemize}
  \item Software maps at present are largely static.
  We envision a more interactive environment in which the user can ``zoom and pan'' through the landscape to see features in closer detail, or navigate to other views of the software.
  \item Selectively displaying features would make the environment more attractive for navigation. Instead of generating all the labels and thematic widgets up-front, users can annotate the map, adding comments and waymarks as they perform their tasks.
  \item Orientation and layout are presently consistent for a single project only.
  We would like to investigate the usefulness of conventions for establishing consistent layout and orientation (\ie ``testing'' is North-East) that will work across multiple projects, possibly within a reasonably well-defined domain.
  \item We plan to perform an empirical user study to evaluate the application of \SOCA for software comprehension and reverse engineering, but also for source code navigation in development environments.
\end{itemize}


\end{document}
