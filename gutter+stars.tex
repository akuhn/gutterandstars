\documentclass[11pt]{book}

%%%
\usepackage{xspace}
\newcommand{\ie}{\textit{i.e.,}\xspace}
\newcommand{\eg}{\textit{e.g.,}\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
%%%
\usepackage{amsmath, amsthm, amssymb} 
\usepackage[pdftex,colorlinks=true,pdfstartview=FitV,
 linkcolor=black,citecolor=black,urlcolor=black]{hyperref}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\figureautorefname}{Figure}
%%%
\usepackage[draft]{graphicx}
%%%
% --> for chapter on Hapax work
\newcommand\concept[1]{#1\xspace}
\newcommand\red{\concept{Red}}
\newcommand\blue{\concept{Blue}}
\newcommand\green{\concept{Green}}
\newcommand\orange{\concept{Orange}}
\newcommand\cyan{\concept{Cyan}}
\newcommand\yellow{\concept{Yellow}}
\newcommand\pink{\concept{Pink}}
\newcommand\magenta{\concept{Magenta}}
\newcommand\black{\concept{Black}}
\newcommand\darkgreen{\concept{DarkGreen}}
\newcommand\navy{\concept{Navy}}
%%%
% --> for chapter on LogLR work
\newcommand{\loglr}{log-likelihood ratio\xspace}
\newcommand{\loglrs}{log-likelihood ratios\xspace}
%%%
% --> for chapter on Chronia
\newcommand{\omap}{\emph{Ownership Map}\xspace}
\newcommand{\id}[1]{{\sf\small{#1}}}
%%%

\begin{document}

\title{Halfway between the Gutter and the Stars}
\author{Adrian Kuhn}
\date{November 2010}
\maketitle

\tableofcontents

\chapter{Introduction}

Other than common believe, software engineers do not spend most time writing code. An approximate 50--80\% of their time is spend on code orientation, ie navigation and understanding \cite{many}. User studies have found that developers use a wide set of cognitive clues for code orientation \cite{many}. Tool support for orientation, however, is typically limited to text file processing or hyperlinks at best. In our research, we explore how to support code orientation by naming, spatial and social clues in development tools.

\begin{itemize}
\item Lexical clues
\item Structural clues
\item Spatial clues
\item Temporal clues
\item Episodic clues
\item Social clues
\end{itemize}

In this work, we present the following tools that explore the use of orientation clues for tool building. Each of these contributions has been published as one or more peer-review paper at an international conference or in an international journal. 

\begin{itemize}
\item Software Clustering \cite{Kuhn07a,Kuhn05a,Lung05a}
\item  Feature Classification \cite{Kuhn06c,Kuhn05b}
\item  Software Summarization \cite{Kuhn09a}
\item  Spatial Representation \cite{Kuhn10c,Kuhn10b,Duca06c,Kuhn08a}
\item  Ownership Map \cite{Girb05a}
\item  Bug-Report Triage \cite{Matt09a}
\item  Credibility in Code Search \cite{Gysi10b}
\end{itemize}

%%%

Acquiring knowledge about a software system is one of the main activities in software reengineering, it is estimated that up to 60 percent of software maintenance is spent on comprehension \cite{Abra04a}. This is because a lot of knowledge about the software system and its associated business domain is not captured in an explicit form. Most approaches that have been developed focus on program structure \cite{Duca05b} or on external documentation \cite{Maar91a,Anto02b}. However, there is another fundamental source of information: the developer knowledge contained in identifier names and source code comments.

{\small\begin{quotation}\emph{The informal linguistic information that the software engineer deals with is not simply supplemental information that can
be ignored because automated tools do not use it. Rather, this information is fundamental. [\ldots] If we are to use this informal information in design recovery tools, we must propose a form for it, suggest how that form relates to the formal information captured in program source code or in formal specifications, and propose a set of operations on these structures that implements the design recovery process} \cite{Bigg89c}.
\end{quotation}}

Languages are a means of communication, and programming languages are no different. Source code contains two levels of communication: human-machine communication through program instructions, and human to human communications through names of identifiers and comments. Let us consider a small code example:

When we strip away all identifiers and comments, from the machine point of view the functionality remains the same, but for a human reader the meaning is obfuscated and almost impossible to figure out. In our example, retaining formal information only yields:

When we keep only the informal information, the purpose of the code is still recognizable. In our example, retaining only the naming yields:

is morning hours minutes seconds is date hours minutes
seconds invalid time value hours 12 minutes 60 seconds 60

%%%%%%%%%%%%

\chapter{Approches for Code Orientation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work, Semantic Clustering}\label{sec:soa}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The use of information retrieval techniques for reverse engineering dates back to the late eighties. Frakes and Nejmeh proposed to apply them on source code as if it would be a natural language text corpus \cite{Frak87a}. They applied an IR system based on keyword matching, which allowed to perform simple searches using wildcards and set expressions.

Antoniol \etal have published a series of papers on recovering code to documentation traceability \cite{Anto00c,Anto02b}. They use information retrieval as well, however with another approach. They rely on external documentation as text corpus, then they query the documentation with identifiers taken from source code to get matching external documents.

Maletic and Marcus were the first to propose using LSI to analyze software systems. In a first work they categorized the source files of the Mosaic web browser and presented in several follow-ups other applications of LSI in software analysis \cite{Male00a}. Their work is a precursor of our work, as they proved that LSI is usable technique to compare software source documents. They apply a minimal-spanning-tree clustering and report on class names and average similarity of selected clusters. We broaden the approach by providing a visual notation that gives an overview of all the clusters and their relationships, and we provide the automatic labeling that takes the entire vocabulary into account and not only the class names.

LSI was also used in other related areas: Marcus and Maletic used LSI to detect high-level conceptual clones, that is they go beyond just string based clone detection using the LSI capability to spot similar terms \cite{Marc01a}. They
select a known implementation of an abstract datatype, and manually investigate all similar source documents to find high-level concept clones. The same authors also used LSI to recover links between external documentation and source code by querying the source code with queries from documentation \cite{Marc03b}.

Kawaguchi \etal used LSI to categorize software systems in open-source software repositories \cite{Kawa04a}. Their approach uses the same techniques as ours, but with a different set up and other objectives. They present a tool that categorizes software projects in a source repository farm, that is they use entire software systems as the documents of their LSI space. They use clustering to provide an overlapping categorizations of software, whereas we use clustering to partition the software into distinct topics. They use a visualization of they results with the objective to navigate among categorizations and projects, similar to the Softwarenaut tool \cite{Lung06a}, whereas we use visualizations to present an overview, including all documents and the complete partition, at one glance.

Marcus \etal employed LSI to detect concepts in the code \cite{Marc04a}. They used the LSI as a search engine and searched in the code the concepts formulated as queries. Their work is about concept location of externally defined concepts, whereas we derive our concepts from the vocabulary usage on the source-code level. Their  article also gives a good overview of the related work. Marcus \etal also use LSI to compute the cohesion of a class based on the semantic similarity of its methods \cite{Marc05a}. In our work, we extend this approach and illustrate on the correlation matrix both, the semantic similarity within a cluster and the semantic similarity between clusters.

De Lucia \etal introduce strategies to improve LSI-based traceability detection \cite{Luci04a}. They use three techniques of link classification: taking the top-\emph{n} search results, using a fix threshold or a variable threshold. Furthermore they create separate LSI spaces for different document categories and observe better results that way, with best results on pure natural language spaces. Lormans and Deursen present two additional links classification strategies \cite{Lorm06a}, and discuss open research questions in traceability link recovery.

Di Lucca \etal also focus on external documentation, doing automatic assignment of maintenance requests to teams \cite{Lucc02b}. They compare approaches based on pattern matching and clustering to information retrieval techniques, of which clustering performs better.

Huffman-Hayes \etal compare the results of several information retrieval techniques in recovering links between document and source code to the results of a senior engineer \cite{Huff06a}. The results suggest that automatic recovery performs better than human analysis, both in terms of precision and recall and with comparable signal-to-noise ratio. In accordance with these findings, we automate the ``Read all the Code in One Hour'' pattern using information retrieval techniques.

\v{C}ubrani\'{c} \etal build a searchable database with artifacts related to a software system, both source code and external documentation \cite{Cubr03a}. They use a structured meta model, which relates bug reports, news messages, external documentation and source files to each other. Their goal is the support software engineers, especially those new to a project, with a searchable database of what they call ``group memory''. To search the database they use information retrieval, however they do not apply LSI and use a plain vector space model only. They implemented their approach in an eclipse plug-in called Hipikat.

\section{Related work for Software Summarization}\label{relwork}

The present work is related to Jonathan Feinberg's comparison of inaugural addresses \cite{Feinberg09blog}. Feinberg analysed the inaugural address of Mr. President Barack Obama and his predecessors in office. For each inaugural addresses he provides a pair of \textsc{Wordle}\footnote{\url{http://www.wordle.net}} word clouds. One cloud consists of words that are specific to the address, and the other cloud consists of words that are missing in the address. Font size is used to represent frequency of a word and saturation to represent the log-likelihood ratio. The color blue is used in the left cloud to represent likely terms, and red is used in the right cloud to represent unlikely terms.

Anslow \etal \cite{Anslow08OOPSLA} visualized the evolution of words in class names in Java version 1.1 and Java version 6.0. They illustrated the history in a combined word cloud that contains terms from both versions. Each word is printed twice, font size represents word frequency and color the corpus. As such they compared word counts, which assumes normal distribution and is thus not as sound as using log-likelihood ratios.
 
Linstead \etal \cite{Linstead09SUITE} analysed the vocabulary of 12,151 open source projects from Sourceforge and Apache. They provide strong evidence of power-law behavior for word distribution across program entities. In addition, they analyse the vocabulary of structural entities (class, interface, method, field) and report the top-10 most frequent terms, as well as the top-10 unique terms for each structural category. As such, they compare word counts which assumes normal distribution and is thus not as sound as using log-likelihood ratios. In \autoref{example5} we report on the likelihood of words for the same structural categories, although our analysis is limited to a smaller corpus, \ie the Java API only.

Kawaguchi \etal \cite{Kawa04a} presented \textsc{MUDABlue}, a tool that provides labels for projects. They used Sourceforge as normative corpus and applied Latent Semantic Indexing (LSI) at the level of projects.
For each project they grouped together all source files into one retrieval document, and applied LSI to retrieve the semantic similarity among both terms and document.
They categorized projects and described the categories with the most similar terms. However, the probabilistic model of LSI does not match observed data: LSI assumes that words and documents form a joint Gaussian model, while a Poisson distribution has been observed \cite{Hofmann99PLSA}.

Lexical information of source code has been further proven useful for various tasks in software engineering (\eg \cite{Anto02a,Marc05a,Posh09a}). Many of these approaches apply Latent Semantic Indexing and inverse-document frequency weighting, which are well-accpeted techniques in Information Retrieval but are according to Dunning ``only justified on very sketchy grounds \cite{Dunning}.''

Baldi \etal \cite{Bald08a} present a theory of aspects (the programming language feature) as latent topics. They apply Latent Dirichlet Analysis (LDA) to detect topic distributions that are possible candidates for aspect-oriented programming. They present the retrieved topics as a list of the 5 most likely words. The model of LDA assumes that each topic is associated with a multinomial distribution over words, and each document is associated with a multinomial distribution over topics; their approach is thus sound.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work for Chronia}\label{sec:soa}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Analyzing the way developers interact with the system has only attracted few research. A visualization similar to the \omap is used to visualize how authors change a wiki page \cite{Vieg04a}.

Xiaomin Wu \etal visualize \cite{Wu04b} the change log information to provide an overview of the active places in the system as well as of the authors activity. They display measurements like the number of times an author changed a file, or the date of the last commitment.

Measurements and visualization have long been used to analyze how software systems evolve.

Ball and Eick \cite{Ball96a} developed multiple visualizations for showing changes that appear in the source code. For example, they show what is the percentage of bug fixes and feature addition in files, or which lines were changed recently.

Eick \etal proposed multiple visualizations to show changes using colors and third dimension \cite{Eick02a}.

Chuah and Eick proposed a three visualizations for comparing and correlating different evolution information like the number of lines added, the errors recorded between versions, number of people working etc. \cite{Chua98a}.

Rysselberghe and Demeyer use a scatter plot visualization of the changes  to provide an overview of the evolution of systems and to detect patterns of change\cite{Ryss04a}.

Jingwei Wu \etal use the spectrograph metaphor to visualize how changes occur in software systems \cite{Wu04a}. They used colors to denote the age of changes on different parts of the systems.

Jazayeri analyzes the stability of the architecture \cite{Jaza02a} by using colors to depict the changes. From the visualization he concluded that old parts tend to stabilize over time.

Lanza and Ducasse visualize the evolution of classes in the Evolution Matrix \cite{Lanz02a}. Each class version is represented using a rectangle. The size of the rectangle is given by different measurements applied on the class version. From the visualization different evolution patterns can be detected such as continuous growth, growing and shrinking phases etc.

Another relevant reverse engineering domain is the analysis of the co-change history.

Gall \etal aimed to detect logical coupling between parts of the system \cite{Gall98a} by identifying the parts of the system which change together. They used this information to define a coupling measurement based on the fact that the more times two modules were changed at the same time, the more they were coupled.

Zimmerman \etal aimed to provide mechanism to warn developers about the correlation of changes between functions. The authors placed their analysis at the level of entities in the meta-model (\eg methods) \cite{Zimm04a}. The same authors defined a measurement of coupling based on co-changes \cite{Zimm03a}.

Hassan \etal analyzed the  types of data that are good predictors of change propagation, and came to the conclusion that historical co-change is a better mechanism than structural dependencies like call-graph \cite{Hass04a}.

% Related work above
%%%%%%%%%%%%%

\chapter{Clustering Software by Lexical Clues}

Many of the existing approaches in Software Comprehension focus on program program structure or external documentation. However, by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked. To understand software as a whole, we need to enrich software analysis with the developer knowledge hidden in the code naming. This paper proposes the use of information retrieval to exploit linguistic information found in source code, such as identifier names and comments. We introduce \emph{Semantic Clustering}, a technique based on Latent Semantic Indexing and clustering to group source artifacts that use similar vocabulary. We call these groups \emph{semantic clusters} and we interpret them as \emph{linguistic topics} that reveal the intention of the code. We compare the topics to each other, identify links between them, provide automatically retrieved labels, and use a visualization to illustrate how they are distributed over the system. Our approach is language independent as it works at the level of identifier names. To validate our approach we applied it on several case studies, two of which we present in this paper.

In this paper, we use information retrieval techniques to \emph{derive topics from the vocabulary usage at the source code level}. Apart from external documentation, the location and use of source-code identifiers is the most frequently consulted source of information in software maintenance \cite{Kosk04a}. The objective of our work is to analyze software without taking into account any external documentation. In particular we aim at:

\begin{itemize}
  \item \textbf{Providing a first impression of an unfamiliar software system}. A common pattern when encountering an unknown or not well known software for the first time is ``Read all the Code in One Hour'' \cite{Deme02a}. Our objective is to support this task, and to provide a map with a survey of the system's most important topics and their location.
  \item \textbf{Revealing the developer knowledge hidden in identifiers.} In practice, it is not external documentation, but identifer names and comments where developers put their knowledge about a system. Thus, our objective is not to locate externally defined domain concepts, but rather to derive topics from the actual use of vocabulary in source code.
  \item \textbf{Enriching Software Analysis with informal information.} When analyzing formal information (\eg structure and behavior) we get only half of the picture: a crucial source of information is missing, namely, the semantics contained in the vocabulary of source code. Our objective is to reveal components or aspects when, for example, planning a large-scale refactoring. Therefore, we analyze how the code naming compares to the code structure: What is the distribution of linguistic topics over a system's modularization? Are the topics well-encapsulated by the modules or do they cross-cut the structure?
\end{itemize}


Our approach is based on Latent Semantic Indexing (LSI), an information retrieval technique that locates linguistic topics in a set of documents \cite{Deer90a,Marc04a}. We apply LSI to compute the linguistic similarity between source artifacts (\eg packages, classes or methods) and cluster them according to their similarity. This clustering partitions the system into linguistic topics that represent groups of documents using similar vocabulary. To identify how the clusters are related to each other, we use a correlation matrix \cite{Ling73a}. We employ LSI again to automatically label the clusters with their most relevant terms. And finally, to complete the picture, we use a map visualization to analyze the distribution of the concepts over the system's structure.

We implemented this approach in a tool called Hapax\footnote{The name is derived from the term \emph{hapax legomenon}, that refers to a word occurring only once a given body of text.}, which is built on top of the Moose reengineering environment \cite{Duca05a,Nier05c}, and we apply the tool on several case studies, two of which are presented in this work: JEdit\footnote{http://www.jedit.org/} and JBoss\footnote{http://www.JBoss.org/}.

This paper is based on our previous work, in which we first proposed semantic clustering \cite{Kuhn05a}. The main contributions of the current paper are:
\begin{itemize}

\item \emph{Topic distribution analysis.} In our previous work we introduced semantic clustering to detect linguistic topics given by parts of the system that use similar vocabulary. We complement the approach with the analysis of how topics are distributed over the system using a Distribution Map \cite{Duca06c}.

\item \emph{Improvement of the labeling algorithm.} One important feature of semantic clustering is the automatic labeling -- \ie given a cluster we retrieve the most relevant labels for it. We propose an improved algorithm that takes also the similarity to the whole system into account.

\item \emph{Case studies.} In our previous work, we showed the results of the clustering and labeling on different levels of abstraction on three case studies. In this paper we report on other two case studies.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Latent Semantic Indexing}\label{sec:LSI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As with most information retrieval techniques, Latent Semantic Indexing (LSI) is based on the vector space model approach. This approach models documents as bag-of-words and arranges them in a term-document matrix $A$, such that $a_{i,j}$ equals the number of times term $t_i$ occurs in document $d_j$.

LSI has been developed to overcome problems with synonymy and polysemy that occurred in prior vectorial approaches, and thus improves the basic vector space model by replacing the original term-document matrix with an approximation. This is done using singular value decomposition (SVD), a principal components analysis (PCA) technique originally used in signal processing to reduce noise while preserving the original signal. Assuming that the original term-document matrix is noisy (the aforementioned synonymy and polysemy), the approximation is interpreted as a noise reduced -- and thus better -- model of the text corpus.

As an example, a typical search engine covers a text corpus with millions of web pages, containing some ten thousands of terms, which is reduced to a vector space with 200-500 dimensions only. In Software Analysis, the number of documents is much smaller and we typically reduce the text corpus to 20-50 dimensions.

Even though search engines are the most common uses of LSI \cite{Berr94a}, there is a wide range of applications, such as automatic essay grading \cite{Folt99a}, automatic assignment of reviewers to submitted conference papers \cite{Duma92a},  cross-language search engines, thesauri, spell checkers and many more.
In the field of software engineering LSI has been successfully applied to categorized source files \cite{Male00a} and open-source projects \cite{Kawa04a}, detect high-level conceptual clones \cite{Marc01a}, recover links between external documentation and source code \cite{Luci04a,Marc05a} and to compute the class cohesion \cite{Marc05a}. Furthermore LSI has proved useful in psychology to simulate language understanding of the human brain, including processes such as the language acquisition of children and other high-level comprehension phenomena \cite{Land97a}.

\autoref{fig:lsi} schematically represents the LSI process. The document collection is modeled as a vector space. Each document is represented by the vector of its term occurrences, where terms are words appearing in the document. The term-document-matrix $A$ is a sparse matrix and represents the document vectors on the rows. This matrix is of size $n \times m$, where $m$ is the number of documents and $n$ the total number of terms over all documents. Each entry $a_{i,j}$ is the frequency of term $t_i$ in document $d_j$. A geometric interpretation of the term-document-matrix is as a set of document vectors occupying a vector space spanned by the terms. The similarity between documents is typically defined as the cosine or inner product between the corresponding vectors. Two documents are considered similar if their corresponding vectors point in the same direction.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=.8\columnwidth]{lsi}
\caption{LSI takes as input a set of documents and the terms occurrences, and returns as output a vector space containing all the terms and all the documents. The similarity between two items (\ie terms or documents) is given by the angle between their corresponding vectors.}
\label{fig:lsi}
\end{center}
\end{figure}

LSI starts with a raw term-document-matrix, weighted by a weighting function to balance out very rare and very common terms. SVD is used to break down the vector space model into less dimensions. This algorithm preserves as much information as possible about the relative distances between the document vectors, while collapsing them into a much smaller set of dimensions.

SVD decomposes matrix $A$ into its singular values and its singular vectors, and yields -- when truncated at the $k$ largest singular values -- an approximation $A'$ of $A$ with rank $k$. Furthermore, not only the low-rank term-document matrix $A'$ can be computed but also a term-term matrix and a document-document matrix. Thus, LSI allows us to compute term-document, term-term and document-document similarities.

As the rank is the number of linear-independent rows and columns of a matrix, the vector space spanned by $A'$ is of dimension $k$ only and much less complex than the initial space. When used for information retrieval, $k$ is typically about 200-500, while $n$ and $m$ may go into millions. When used to analyze software on the other hand, $k$ is typically about $20-50$ with vocabulary and documents in the range of thousands only. And since $A'$ is the best approximation of $A$ under the least-square-error criterion, the similarity between documents is preserved, while in the same time mapping semantically related terms on one axis of the reduced vector space and thus taking into account synonymy and polysemy. In other words, the initial term-document-matrix $A$ is a table with term occurrences and by breaking it down to much less dimension the latent meaning \emph{must} appear in $A'$ since there is now much less space to encode the same information. Meaningless occurrence data is transformed into meaningful concept information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semantic Clustering: Grouping Source Documents}\label{sec:sekla}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The result of applying LSI is a vector space, based on which we can compute the similarity between both documents or terms. We use this similarity measurement to identify topics in the source code.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=.8\columnwidth]{clustering}
\caption{Semantic clustering of software source code (\eg classes, methods).}
\label{fig:clustering}
\end{center}
\end{figure}

\autoref{fig:clustering} illustrates the first three steps of the approach: preprocessing, applying LSI, and clustering. Furthermore we retrieve the most relevant terms for each cluster and visualize the clustering on a 2D-map, thus in short the approach is:

\begin{enumerate}
  \item \emph{Preprocessing the software system.} In \autoref{sec:parsing}, we show how we break the system into documents and how we build a term-document-matrix that contains the vocabulary usage of the system.
  \item \emph{Applying Latent Semantic Indexing.} In \autoref{sec:lsi} we use LSI to compute the similarities between source code documents and illustrate the result in a correlation matrix \cite{Ling73a}.
  \item \emph{Identifying topics.} In \autoref{sec:clustering} we cluster the documents based on their similarity, and we rearrange the correlation matrix. In \autoref{sec:wittgenstein} we discuss that each cluster is a \emph{linguistic topic}.
  \item \emph{Describing the topics with labels.} In \autoref{sec:labeling} we use LSI again to retrieve for each cluster the top-$n$ most relevant terms.
  \item \emph{Comparing the topics to the structure.} In \autoref{sec:distribution} we illustrate the distribution of topics over the system on a Distribution Map \cite{Duca06c}.
\end{enumerate}

We want to emphasize that the primary contribution of our work is semantic clustering and the labeling. The visualization we describe are just used as a means to convey the results and are not original contributions of this paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On the Relationship between Concepts and Semantic Clustering}\label{sec:wittgenstein}

In this paper we are interested in locating concepts in the source code. However, our concern is not to locate the implementation of externally defined domain concepts, but rather to derive the implemented topics from the vocabulary of source code. We tackle the problem of concept location on the very level of source code, where we apply information retrieval to analyze the use of words in source code documents. The clusters retrieved by our approach are not necessarily domain or application concepts, but rather code-oriented topics.

Other than with domain or application concepts in Software Analysis \cite{Bigg93a}, it is not uncommon in information retrieval to derive \emph{linguistic topics} from the distribution of words over a set of documents. This is in accordance with Wittgenstein who states that \emph{``Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache---the meaning of a word is given by its use in language''} \cite{Witt53a}. Unlike in classical philosophy, as for example Plato, there is no external definition of a word's meaning, but rather the meaning of a word is given by the relations it bears with other terms and sentences being used in the same context. Certainly, there is a congruence between the external definition of a word and its real meaning, since our external definitions are human-made as well, and thus, also part of the language game, but this congruence is never completely accurate.

In accordance with \cite{Bigg89c} we call our clusters \emph{linguistic topics} since they are derived from language use. Certainly, some linguistic topics do map to the domain and others do map to application concepts, however, this mapping is never complete. There is no guarantee that semantic clustering locates all or even any externally defined domain concept. But nonetheless, our case studies show that semantic clustering is able to capture important domain and application concepts of a software system. Which does not come as a surprise, since it is well known that identifer names and comments are one of the most prominent places where developers put their knowledge about a system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preprocessing the Software System}\label{sec:parsing}

When we apply LSI to a software system we partition its source code into documents and we use the vocabulary found therein as terms. The system can be split into documents at any level of granularity, such as packages or classes and methods. Other slicing solutions are possible as well, for example execution traces \cite{Kuhn05b}, or we can even use entire projects as documents and analyze a complete source repository \cite{Kawa04a}.

To build the term-document-matrix, we extract the vocabulary from the source code: we use both identifier names and the content of comments. Natural language text in comments is broken into words, whereas compound identifier names are split into parts. As most modern naming conventions use camel case, splitting identifiers is straightforward: for example \emph{FooBar} becomes \emph{foo} and \emph{bar}.

We exclude common stopwords from the vocabulary, as they do not help to discriminate documents. In addition, if the first comment of a class contains a copyright disclaimer, we exclude it as well. To reduce words to their morphological root we apply a stemming algorithm: for example \emph{entity} and \emph{entities} both become \emph{entiti} \cite{Port80a}. And finally, the term-document matrix is weighted with \emph{tf-idf} to balance out the influence of very rare and very common terms \cite{Duma91a}.

When preprocessing object-oriented software systems we take the inheritance relationship into account as well. For example, when applying our approach on the level of classes, each class inherits some of the vocabulary of its superclass. If a method is defined only in the superclass we add its vocabulary to the current class. Per level of inheritance a weighting factor of $w = 0.5$ applies to the term occurrences, to balance out between the abstractness of high level definitions and concrete implementations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Using Latent Semantic Indexing to Build the Similarity Index}
\label{sec:lsi}

We use LSI to extract linguistic information from the source code, which results in an LSI-index with similarities between source documents (\ie packages, classes or methods). Based on the index we can determine the similarity between source code documents. Documents are more similar if they cover the same topic, terms are more similar if they denote related topics.

In the vector space model there is a vector for each document. For example, if we use methods as documents, there is a vector for each method and the cosine between these vectors denotes the semantic similarity between the methods. In general cosine values are in the $[-1,1]$ range, however when using an LSI-index the cosine between its element never strays much below zero. This is since the LSI-index is derived from a term-document matrix that contains positive occurrence data only.

\emph{First matrix in \autoref{fig:comaFourStep}.} To visualize similarities between documents we map them to gray values: the darker, the more similar. The similarities between elements are arranged in a square matrix called \emph{correlation matrix} or \emph{dot plot}. Correlation matrix is a common visualization tool to analyze patterns in a set of entities \cite{Ling73a}. Each dot $a_{i,j}$ denotes the similarity between element $d_i$ and element $d_j$. Put in other words, the elements are arranged on the diagonal and the dots in the off-diagonal show the relationship between them.

\begin{figure}[h]
  \includegraphics{clusteringEx}
  \caption{From left to right: unordered correlation matrix, then sorted by similarity, then grouped by clusters, and finally including semantic links.}\label{fig:comaFourStep}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clustering: Ordering the Correlation Matrix}\label{sec:clustering}

%do not change the first sentence of this section, it is a hidden reference to the fist sentence from William  Gibsons novel Necromancer: ``The sky above the port was the color of television, tuned to a dead channel''.

Without proper ordering the correlation matrix looks like television tuned to a dead channel. An unordered matrix does not reveal any patterns. An arbitrary ordering, such as for example the names of the elements, is generally as useful as random ordering \cite{Bert73a}. Therefore, we cluster the matrix to put similar elements near each other and dissimilar elements far apart of each other.

A clustering algorithm groups similar elements together and aggregates them into clusters \cite{Jain99a}. Hierarchical clustering creates a tree of nested clusters, called \emph{dendrogram}, which has two features: breaking the tree at a given threshold groups the elements into clusters, and traversing the tree imposes a sort order upon its leaves. We use these two features to rearrange the matrix and to group the dots into rectangular areas.

\emph{Second and third matrix in \autoref{fig:comaFourStep}.} Each rectangle on the diagonal represents a semantic cluster: the size is given by the number of classes that belong to a topic, the color refers to the \emph{semantic cohesion} \cite{Marc05a} (\ie the average similarity among its classes\footnote{Based on the similarity ${\rm sim}(a,b)$ between elements, we define the similarity between cluster $A$ and cluster $B$ as $\frac{1}{|B| \times |A|}\sum \sum {\rm sim}(a_m,b_n)$ with $a \in A$ and $b \in B$ and in the same way the similarity between an element $a_0$ and a cluster $B$ as $\frac{1}{|B|}\sum {\rm sim}(a_0,b_n)$ with $B \in B$.
}). The color in the off-diagonal is the darker the more similar to clusters are, if it is white they are not similar at all. The position on the diagonal is ordered to make sure that similar topics are placed together.

The clustering takes the focus of the visualization from similarity between elements to similarity between clusters. The tradeoff is, as with any abstraction, that some valuable detail information is lost. Our experiments showed that one-to-many relationships between an element and an entire cluster are valuable patterns.

\emph{Fourth matrix in \autoref{fig:comaFourStep}.} If the similarity between an element $d_n$ from a cluster and another cluster differs more than a fix threshold from the average similarity between the two clusters, we plot $d_n$ on top of the clustered matrix either as a bright line if $d_n$ is less similar than average, or as a dark line if $d_n$ is more similar than average. We call such a one-to-many relationship a \emph{semantic link}, as it reveals an element that links from its own topic to another topic.

\autoref{fig:hotspot} illustrates an example of a semantic link. The relationship between cluster A and cluster B is represented by the rectangle found at the intersection of the two clusters. The semantic link can be identified by a horizontal (or vertical) line that is darker than the rest of the rectangle. In our example, we find such a semantic link between two clusters. Please note that the presence of a link does not mean that the element in A should belong to B, it only means that the element in A is more similar to B then the other elements in A.

\begin{figure}[h]
    \centering
  \includegraphics{hotspot}
  \caption{Illustration of a semantic link. A semantic link is a one-to-many relation: A document in cluster A that is more similar than its siblings in A to cluster B.}\label{fig:hotspot}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Labeling the Clusters}\label{sec:labeling}

With the clustering we partitioned the source documents by their vocabulary, but we do not know about the actual vocabulary that connects these documents together. In other words, what are the most important terms for each cluster? In this section we use LSI to retrieve for each cluster the top-$n$ list with its most relevant terms. We use these lists to label the topics.

The labeling works as follows. As we already have an LSI-index at hand, we use it as a search engine \cite{Berr94a}. We reverse the usual search process where a search query of terms is used to find documents, and instead, we use the documents in a cluster as search query to find the most similar terms. To label a cluster, we take the top-$n$ most similar terms, using a top-7 list provides a useful labeling for most case studies.

To compute the relevance of a term, we compare the similarity to the current cluster with the similarity to all other clusters. This raises better results than just retrieving the top most similar terms \cite{Kuhn05a}. Common terms, as for example the names of custom data structures and utility classes, are often highly similar to many clusters. Thus, these terms would pollute the top-$n$ lists with non-discriminating labels if using plain similarity only, whereas subtracting the average lowers the ranking of such common terms. The following formula ranks high those terms that are very similar to the current cluster but not common to all other clusters.

$$\mathrm{rel}(t_0,A_0) = \mathrm{sim}(t_0,A_0) - \frac{1}{|\mathcal{A}|} \sum_{A_n \in \mathcal{A}}{\mathrm{sim}(t_0,A_n)}$$

Term $t_0$ is relevant to the current cluster $A_0$, if it has a high similarity to the current cluster $A_0$ but not to the remaining clusters $A_n \in \mathcal{A}$. Given the similarity between a term $t$ and a cluster $A$ as $\mathrm{sim}(t,A)$, we define the relevance of term $t_0$ according to cluster $A_0$ as given above.

Another solution to avoid such terms, is to include the common terms into a custom stopword list. However this cannot be done without prior knowledge about the system, which is more work and contradicts our objectives. Furthermore, this ranking formula is much smoother than the strict opt-out logic of a stopword list.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analyzing the Distribution of Semantic Clusters}\label{sec:distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The semantic clusters help us grasp the topics implemented in the source code. However, the clustering does not take the structure of the system into account. As such, an important question is: How are these topics distributed over the system?

To answer this question, we use a Distribution Map \cite{Tuft01a,Duca06c}. A Distribution Map visualizes the distribution of properties over system parts \ie a set of entities. In this paper, we visualize packages and their classes, and color these classes according to the semantic cluster to which they belong.

For example, in \autoref{fig:distMap} we show an example of a Distribution Map representing 5 packages, 37 classes and 4 semantic clusters. Each package is represented by a rectangle, which includes classes represented as small squares. Each class is colored by the semantic cluster to which it belongs.

\begin{figure}[h]
    \centering
  \includegraphics{MetricExamples}\\
  \caption{Example of a Distribution Map.}\label{fig:distMap}
\end{figure}

Using the Distribution Map visualization we correlate linguistic information with structural information. The semantic partition of a system, as obtained by semantic clustering, does generally not correspond one-on-one to its structural modularization. In most systems we find both, topics that correspond to the structure as well as topics that cross-cut it. Applying this visualization on several case studies, we identified the following patterns:

\begin{itemize}
  \item \emph{Well-encapsulated topic} -- if a topic corresponds to system parts, we call this a \emph{well-encapsulated topic}. Such a topic is spread over one or multiple parts and includes almost all source code within those parts. If a well-encapsulated topic covers only one part we speak of a \emph{solitary topic}.

  \item \emph{Cross-Cutting topic} -- if a topic is orthogonal to system parts, we call this a \emph{cross-cutting topic}. Such a topic spreads across multiple parts, but includes only one or very few elements within each parts. As linguistic information and structure are independent of each other, cross-cutting identifiers or names do not constitute a design flaw. Whether a cross-cutting topic has to be considered a design smell or not depends on the particular circumstances. Consider for example the popular three-tier architecture: It separates \emph{accessing, processing \emph{and} presenting data} into three layers; where application specific topics -- such as \eg \emph{accounts, transactions \emph{or} customers} -- are deliberately designated to cross-cut the layers. That is, it emphasizes on the separation of those three topics and deliberately designates the others as cross-cutting concerns.

 \item \emph{Octopus topic} -- if a topic dominates one part, as a solitary does, but also spreads across other parts, as a cross-cutter does, we call this an \emph{octopus topic}. Consider for example a framework or a library: there is a core part with the implementation and scattered across other parts there is source code that plug into the core, and hence use the same vocabulary as the core.

  \item \emph{Black Sheep topic} -- if there is a topic that consists only of one or a few separate source documents, we call this a \emph{black sheep}. Each black sheep deserves closer inspection, as these documents are sometimes a severe design smell. Yet as often, a black sheep is just an unrelated helper class and thus not similar enough to any other topic of the system.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Case studies}\label{sec:validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To show evidence of the usefulness of our approach for software comprehension, in this section we apply it on two case studies. Due to space limitations, only the first case study is presented in full length.

First, we exemplify each step of the approach and discuss its findings in the case of JEdit, a text editor written in Java. This case study is presented in full length. Secondly, we present JBoss, an application-server written in Java, which includes interesting anomalies in its vocabulary.

\begin{figure}[h]
\centering
{\scriptsize
\begin{tabular}{l|llrrrrrr}
\hline
\textbf{Case Study}&\textbf{language}&\textbf{type}&\textbf{docs}&\textbf{terms}
&\textbf{parts}&\textbf{links}&\textbf{rank}&\textbf{sim}\\
\hline
Ant & Java & \emph{Classes} & 665 & 1787 & 9 & -- & 17 & 0.4\\
Azureus & Java & \emph{Classes}       & 2184 & 1980 & 14 & -- & 22 & 0.4\\
JEdit & Java & \emph{Classes}       & 394  & 1603 & 9 & 53 &17 & 0.5\\
JBoss & Java & \emph{Classes}       & 660 & 1379 & 10 & -- & 16 & 0.5\\
Moose\footnotemark{} & Smalltalk & \emph{Classes}  & 726  & 11785 & -- & 137& 27 & --\\
MSEModel & Smalltalk & \emph{Methods}  & 4324  & 2600 & -- & -- & 32 & 0.75\\
Outsight & Java & \emph{Classes}    & 223 & 774 & 10 & -- & 12 & 0.5\\
\hline
\end{tabular}}\\
\caption{The statistics of sample case studies, JEdit and JBoss are discussed in this work, for the other studies please refer to our previous work \cite{Kuhn05a,Kuhn06a}.}\label{fig:table1}
\end{figure}
\footnotetext{The Moose case study in \cite{Kuhn05a} did not use stemming to preprocess the text corpus, hence the large vocabulary.}

\autoref{fig:table1} summarizes the problem size of each case study. It lists for each case study: (lang) the language of the source code, (type) the granularity of  documents, (docs) the number of documents, (terms) the number of terms, (parts) the number of found topics, (links) the number of found semantic links, (rank) the dimension of the LSI-index, and (sim) the threshold  of the clustering.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On the Calibration of Parameters and Thresholds}\label{sec:parameters}

Our approach depends on several parameters, which may be difficult too choose for someone not familiar with the underlying technologies. In this section we present all parameters, discuss their calibration and share our experience gained when performing case studies using the Hapax tool.

  \textbf{Weighting the term-document-matrix.} To balance out the influence of very rare and very common terms, it is common in information retrieval to weight the occurrence values. The most common weighting scheme is \emph{tf-idf}, which we also use in the case studies, others are entropy or logarithmic weighting \cite{Duma91a}.
  %Nako01b = Weight functions impact on LSA performance.

  When experimenting with different weighting schemes, we observed that the choice of the weighting scheme has a considerable effect on the similarity values, depending on the weighting the distance within the complete text corpus becomes more compact or more loose \cite{Nako01b}. Depending on the choice of the weighting scheme, the similarity thresholds may differ significantly: as a rule of thumb, using logarithmic weighting and a similarity threshold of $\delta = 0.75$ is roughly equivalent to a threshold of $\delta = 0.5$ with \emph{tf-idf} weighting \cite{Nako01a}.

  \textbf{Dimensionality of the LSI-space.} As explained in \autoref{sec:LSI}, LSI replaces the term-document matrix with a low-rank approximation. When working with natural language text corpora that contain millions of documents and some ten thousands of terms, most authors suggest to use an approximation between rank 200 and 500. In Software Analysis the number of documents is much smaller, such that even ranks as low as 10 or 25 dimensions yield valuable results. Our tool uses rank $r = (m * n)^{0.2}$ by default for an $m \times n$-dimensional text corpus, and allows customization.

  \textbf{Choice of clustering algorithm.} There is a rich literature on different clustering algorithms \cite{Jain99a}. We performed a series of experiments using different algorithms, however as we cannot compare our results against an \emph{a priori} known partition, we cannot measure recall in hard numbers and have to rely on human judgment of the results. Therefore we decided to use a hierarchical \emph{average-linkage} clustering as it is a common standard algorithm. Further studies on the choice of clustering algorithm are open for future work.

  \textbf{Breaking the dendrogram into clusters.} Hierarchical clustering uses a threshold to break the dendrogram, which is the tree of all possible clusters, into a fix partition. Depending on the objective, we break it either into a fixed number of clusters (\eg for the Distribution Map, where the number of colors is constrained) or at a given threshold (\eg for the correlation matrix). In the user interface of the Hapax tool, there is a slider for the threshold such that we can immediately observe the effect on both correlation matrix and Distribution Map interactively.

  \textbf{Detecting semantic links.} As explained in \autoref{sec:clustering}, a semantics link is an element in cluster $A$ with a different similarity to cluster $B$ than average. Typically we use a threshold of $\delta = \pm20\%$ to decide this, however, our tool supports fixed thresholds and selecting the top-$n$ link as well. When detecting traceability-links, a problem which is closely related to semantic links, this has been proven as the best out if these three strategies \cite{Luci04a}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantic Clustering applied on JEdit}

We exemplify our approach at the case of JEdit, an open-source Text editor written in Java. The case study contains 394 classes and uses a vocabulary of 1603 distinct terms. We reduced the text corpus to an LSI-space with rank $r = 15$ and clustered it with a threshold of $\delta = 0.5$ (the choice of parameters is discussed in \autoref{sec:parameters}).

In \autoref{fig:jeditCorrelation}, we see nine clusters with a size of (from top right to bottom left) 116, 63, 26, 10, 68, 10, 12, 80, and 9 classes. The system is divided into four zones: (zone 1) the large cluster in the top left, (zone 2) two medium sized and a small clusters, (zone 3) a large cluster and two small clusters, and (zone 4) a large and a small cluster. The two zones in the middle that are both similar to the first zone but not to each other, and the fourth zone is not similar to any zone.

In fact, there is a limited area of similarity between the Zone 2 and 3. We will later on identify the two counterparts as topics \pink and \cyan, which are related to text buffers and regular expression respectively. These two topics share some of their labels (\ie start, end, length and count), however they are clustered separately since LSI does more than just keyword matching, LSI takes the context of term usage into account as well, that is the co-location of terms with other terms.

\begin{figure}[b]
  \centering
  \includegraphics{jeditcorrelationzone}\\
  \caption{The correlation matrix of JEdit.}\label{fig:jeditCorrelation}
\end{figure}

This is a common pattern that we often encountered during our experiments: zone 1 is the core of system with domain-specific implementation, zone 2 and 3 are facilities closely related to the core, and zone 4 is an unrelated component or even a third-party library. However, so far this is just an educated guess and therefore we will have a look at the labels next.

\autoref{fig:jeditLabels} lists for each cluster the top-7 most relevant labels, ordered by relevance. The labels provide a good description of the clusters and the tell same story as the correlation matrix before. We verified the labels and topics by looking at the actual classes within each cluster.

\begin{itemize}
  \item Zone 1: topic \red implements the very domain of the system: files and users, and a user can load, edit and save files.
  \item Zone 2: topic \green and \magenta implement the user interface, and topic \pink implements text buffers.
  \item Zone 3: topic \cyan is about regular expressions, topic \yellow provides XML support and topic \darkgreen is about TAR archives.
  \item Zone 4: topic \blue and \orange are the BeanShell scripting framework, a third-party library.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics{jeditCorrelationLabels}
\caption{The semantic clusters of JEdit and their labels.}\label{fig:jeditLabels}
\end{figure}

All these labels are terms taken from the vocabulary of the source code and as such they do not always describe the topics in generic terms. For example, event though JEdit is a text-editor, the term \emph{text-editor} is not used on the source code level. The same applies for topic \cyan, where the term \emph{regular expression} does not shop up in the labels.

Next are the semantic links, these are the thin lines on the correlation matrix. If a line is thicker than one pixel, this means that there are two or more similar classes which are each a semantic links. The analysis detected 53 links, below we list as a selection all links related to the scripting topic (these are the link annotated on \autoref{fig:jeditCorrelation}). For each link we list the class names and the direction:

\begin{itemize}
  \item Link 1: \emph{AppelScriptHandler} and \emph{BeanShell} link from topic \red to scripting.
  \item Link 2: \emph{Remote} and \emph{ConsoleInterface} link from topic \red to scripting
  \item Link 3: \emph{VFSDirectoryEntryTable} and \emph{JThis} link from the UI topic to scripting.
  \item Link 4: \emph{ParseException}, \emph{Token} \emph{TokenMgrError} and \emph{BSHLiteral} link from the regular expressions topic to scripting.
  \item Link 5: \emph{ClassGeneratorUtil} and \emph{ClasspathError} link from scripting to topic \red.
  \item Link 6: \emph{ReflectManager} and \emph{Modifiers} link from scripting to the UI topic.
  \item Link 7: \emph{NameSource} and \emph{SimpleNode} link from scripting to the regular expressions and the XML topic.
\end{itemize}

Evaluating semantic links is similar to evaluating traceability links \cite{Luci04a}. We inspected the source code of each link. Some of them are false positives, for example link 4 and 5 are based on obviously unrelated use of the same identifers. On the other hand, link 1 points to the main connector between core and scripting library and link 7 reveals a high-level clone, the implementation of the same datatype in three places: XML uses nodes, and both BSH and regular expressions implement their own AST node.


\autoref{fig:jeditDistribution} shows the distribution of topics over the package structure of JEdit. The large boxes are the packages (the text above is the package name), the squares are classes and the colors correspond to topics (the colors are the same as on \autoref{fig:jeditLabels}).

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{jeditDistribution}\\
  \caption{The Distribution Map of the semantic clusters over the package structure of JEdit.}\label{fig:jeditDistribution}
\end{figure}


For example, in \autoref{fig:jeditDistribution} the large box on the right represents the package named \emph{bsh}, it contains over 80 classes and most of these classes implement the topic referred to by \blue. The package boxes are ordered by their similarity, such that related packages are placed near to each other.

Topic \red, the largest cluster, shows which parts of the system belong to the core and which do not. Based on the ordering of the packages, we can conclude that the two UI topics (\eg \green and \yellow) are more closely related to the core than for example topic \cyan, which implements regular expressions.

The three most well-encapsulated topics (\eg \orange, \blue and \cyan) implement separated topics such as scripting and regular expressions. Topic \yellow and \pink cross-cut the system: \yellow implements dockable windows, a custom GUI-feature, and \pink is about handling text buffers. These two topics are good candidates for a closer inspection, since we might want to refactor them into packages of their own.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{First Impression of JBoss: Distribution Map and Labels}\label{sec:azureus}

This case study presents the outline of JBoss, an application-server written in Java. We applied semantic clustering and partitioned the system into ten topics. The system is divided into one large cluster (colored in red), which implements the core of the server, and nine smaller clusters. Most of the small clusters implement different services and protocols provided by the application server.

\begin{figure}[htbp]
\begin{center}
  \includegraphics[width=\linewidth]{JBossDistribution}
  \caption{The Distribution Map of the semantic clusters over the package structure of JBoss.}
  \label{fig:JBossDistribution}
\end{center}
\end{figure}


\begin{figure}[h]
  \centering
  \begin{scriptsize}
  \begin{tabular}{l|rl}
    \hline
    \textbf{Color} & \textbf{Size} & \textbf{Labels}\\
    \hline
    red & 223 & invocation, invoke, wire, interceptor, call, chain, proxy, share\\
    blue & 141 & jdbccmp, JDBC, cmp, field, loubyansky, table, fetch\\
    cyan & 97 & app, web, deploy, undeployed, enc, JAR, servlet\\
    green & 63 & datetime, parenthesis, arithmetic, negative, mult, div, AST\\
    yellow & 35 & security, authenticate, subject, realm, made, principle, sec\\
    dark magenta & 30 & expire, apr, timer, txtimer, duration, recreation, elapsed\\
    magenta & 20 & ASF, alive, topic, mq, dlq, consume, letter\\
    orange & 20 & qname, anonymous, jaxrpcmap, aux, xb, xmln, WSDL\\
    purple & 16 & invalid, cost, September, subscribe, emitt, asynchron, IG\\
    dark green & 15 & verify, license, warranty, foundation, USA, lesser, fit\\
    \hline
  \end{tabular}
  \end{scriptsize}
  \caption{The labels of the semantic clusters of JBoss.}\label{fig:JBossLabels}
\end{figure}


The Distribution Map is illustrated in \autoref{fig:JBossDistribution}, and the top-7 labels are listed in figure \autoref{fig:JBossLabels} in order of relevance. This is the same setup as in the first case study, except the correlation matrix and semantic links are left out due to space restrictions. We verified the clustering by looking the source code, and present the results as follows.

Topic \red is the largest cluster and implements the core functionality of the system: is labeled with terms such as \emph{invocation, interceptor, proxy \emph{and} share}. Related to that, topic \cyan implements the deployment of JAR archives.

The most well-encapsulated topics are \darkgreen, \orange, \green and \blue. The first three are placed apart from \red, whereas \blue has outliers in the red core packages. The labels and package names (which are printed above the package boxes in the Distribution Map) show that \darkgreen is a bean verifier, that \orange implements JAX-RPC and WDSL (\eg web-services), that \green implements an SQL parser and that \blue provides JDBC (\eg database access) support. These are all important topics of an application server.

The most cross-cutting topic is \yellow, it spreads across half of the system. The labels reveal that this the security aspect of JBoss, which is reasonable as security is an important feature within a server architecture.

Noteworthy is the label \emph{loubyansky} in topic \blue, it is the name of a developer. Based on the fact that his name appears as one of the labels, we assume that he is the main developers of that part of the system. Further investigation proved this to be true.

Noteworthy as well are the labels of topic \darkgreen, as they expose a failure in the preprocessing of the input data. To exclude copyright disclaimers, as for example the GPL licence, we ignore any comment above the \emph{package} statement of a Java class. In the case of topic \darkgreen this heuristic failed: the source files contained another licence within the body of the class. However, repeating the same case study with an improved preprocessing resulted in nearly the same clustering and labeled this cluster as RMI component: \emph{event, receiver, RMI, RMIiop, iiop, RMIidl, and idl}.

The topics extracted from the source code can help improving the comprehension. If a maintainer is seeking information, semantic clustering helps in identifying the related code. This is similar to the use of a search engine, for example if the web-service interface has to be changed, the maintainer can immediately look at the Orange concept, and identify the related classes. Much in the same way, if one has to maintain the database interface, he looks at the Blue concept.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In this section we evaluate and discuss success criteria, strengths and limitations of the proposed approach. We discuss how the approach stands and fails with the quality of the identifer naming. Furthermore we discuss the relation between linguistic topics and domain or application concepts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On the Quality of Identifier Names}

In the same way as structural analysis depends on correct syntax, semantic analysis is sensitive to the quality of the naming. Since we derive our topics solely based on the use of identifer names and comments, it does not come as a surprise that our approach stands and fails with the quality of the source code naming.

Our results are not generalizable to any software system, a good naming convention and well chosen identifiers yields best results, whereas bad naming (\ie too generic names, arbitrary names or cryptic abbreviations) is one of the main threats to external validation. The vocabulary of the case studies presented in this work is of good quality, however, when performing other case studies we learned of different facets that affect the outcome, these are:

\textbf{On the use of naming conventions.} Source following state-of-the-art naming conventions, as for example the Java Naming Convention, is easy to preprocess. In case of legacy code that uses other naming conventions (\eg the famous Hungarian Notation) or even none at all, other algorithms and heuristics are to be applied \cite{Capr93a,Anqu98b}.

\textbf{On generic or arbitrary named identifiers.} However, even the best preprocessing cannot guess the meaning of variables which are just named \emph{temp} or \emph{a}, \emph{b} and \emph{c}. If the developers did not name the identifiers with care, our approach fails, since the developer knowledge is missing. Due to the strength of LSI in detecting synonymy and polysemy, our approach can deal with a certain amount of such ambiguous or even completely wrong named identifiers -- but if a majority of identifiers in system is badly chosen, the approach fails.

\textbf{On abbreviated identifier names.} Abbreviated identifers are commonly found in legacy code, since early programming languages often restrict the discrimination of identifer names to the first few letters. But unlike generic names, abbreviations affect the labeling only and do not threat our approach  as whole. This might come as a surprise, but since LSI is solely based on analyzing the statistical distribution of terms across the document set, it is not relevant whether identifiers are consistently written out or consistently abbreviated.

However, if the labeling task comes up with terms such as \emph{pma}, \emph{tcm}, \emph{IPFWDIF} or \emph{sccpsn} this does not tell a human reader much about the system. These terms are examples taken from a large industry case study, which is not included in this paper, where about a third of all identifiers where abbreviations. In this case the labeling was completely useless. Please refer to \cite{Anqu98b} for approaches on how to recover abbreviations.

\textbf{On the size of the vocabulary.}  The vocabulary of source code is very small, smaller than that of a natural language text corpus. Intuitively explained: LSI is like a child learning language. In the same way as a human with a vocabulary of 2000 terms is less eloquent and knowledgeable than a human with a vocabulary of 20,000 terms, LSI performs better the larger the vocabulary. Whereas, the smaller the vocabulary the stronger the effect of missing or incorrect terms. In fact, LSI has been proven a valid model of the way children acquire language \cite{Land97a}.

\textbf{On the size of documents.} In average there are only about 5-10 distinct terms per method body, and 20-50 distinct terms per class. In a well commented software system, these numbers are higher since comments are human-readable text. This is one of the rationales why LSI does not perform as accurate on source code as on natural language text \cite{Luci04a}, however the results are of sufficient quality.

\textbf{On the combination of LSI with morphological analysis.} Even tough the benefits of stemming are not without controversy, we apply it as part of the preprocessing step \cite{Baez99b}. Our rational is: analyzing a software system at the level of methods is very sensitive to the quality of input, as the small document size threatens the success of LSI. Considering these circumstances, we  decided to rely on stemming as it is well known that the naming of identifers often includes the same term in singular and plurals: for example \emph{setProperty} and \emph{getAllProperties} or \emph{addChild} and \emph{getChildren}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On using Semantic Clustering for Topic Identification}

One of our objectives is to compare linguistic topics to domain and application concepts \cite{Bigg93a}. As discussed in \autoref{sec:wittgenstein}, we derive linguistic topics from the vocabulary usage of source code instead from external definitions. In this section we clarify some questions concerning the relation between derived topics and externally defined concepts:

\textbf{On missing vocabulary and ontologies.} Often the externally defined concepts are not captured by the labeling. The rational for this is as follows. Consider for example a text editor in whose source code the term \emph{text-editor} is never actually used, but terms like \emph{file} and \emph{user}. In this case our approach will label the text-editor concepts with these two terms, as a more generic term is missing. As our approach is not based on an ontological database, its vocabulary is limited to the terms found in source code and if terms are not used our approach will not find accurate labels. We suggest to use ontologies (\ie WordNet) to improve the results in these cases.

\textbf{On the congruence between topics and domain.} When starting this work, one of our hypotheses was that semantic clustering will reveal a system's domain semantics. But our experiments disproved this hypothesis: most linguistic topics are application concepts or architectural components, such as layers. In many experiments, our approach partitioned the system into one (or sometimes two) large domain-specific part and up to a dozen domain-independent parts, such as for example input/output or data storage facilities. Consider for example the application in \autoref{fig:outsight}, it is divided into nine parts as follows:

\begin{figure}[h]
  % Requires \usepackage{graphicx}
  \includegraphics[width=\linewidth]{outsightDistribution}\\
  \caption{The Distribution Map of Outsight, a webbased job portal application \cite{Kuhn06a}.}\label{fig:outsight}
\end{figure}

Only one topic out of nine concepts is about the system's domain: job exchange. Topic \red includes the complete domain of the system: that is users, companies and CVs. Whereas all other topics are application specific components: topic \blue is a CV search engine, topic \darkgreen implements PDF generation, topic \green is text and file handling, topic \cyan and \magenta provide access to the database, and topic DarkCyan is a testing and debugging facility. Additionally the cross-cutting topic \yellow bundles high-level clones related to time and timestamps.

\textbf{On the congruence between topics and packages.} In section \autoref{sec:distribution} we discussed the relation between topics and packages. Considering again the case study in \autoref{fig:outsight} as an example, we find occurrences of all four patterns: Topic \darkgreen for example is well-encapsulated, whereas topic \yellow cross-cuts the application. Then there is topic \blue, which is an octopus with the \emph{conditions} package as its body and tentacles reaching into six other packages, and finally we have in the \emph{logic} package an instance of a black sheep.

\textbf{On source documents that are related to more than one topic.} If we want to analyze how the topics are spread across some type of documents (\eg packages, classes or methods) we have to break the system into documents one level below the target level. For example, if we want to analyze the topic distribution over packages, we break the system into classes and analyze how the topics of classes are spread over the packages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Source code bears the semantics of an application in the names of identifiers and comments. In this paper we present our approach to retrieve the topics present in the source code vocabulary to support program comprehension. We introduce semantic clustering, a technique based on Latent Semantic Indexing and clustering to group source documents that use similar vocabulary. We call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code. As compared to our previous approach, we go a step forward and use Distribution Maps to illustrate how the semantic clusters are distributed over the system.

We applied the approach on several case studies with different characteristics, two of which are presented in this paper.  The case studies give evidence that our approach provides a useful first impression of an unfamiliar system, and that we reveal valuable developer knowledge. The Distribution Map together with the labeling provides a good first impression of the software's domain. Semantic clustering captures topics regardless of class hierarchies, packages and other structures. One can, at a glance, see whether the software covers just a few or many different topics, how these are distributed over the structure, and -- due to the labeling -- what they are about.

When starting this work, one of our hypotheses was that semantic clustering would reveal a systems domain semantics. However, our experiments showed that most linguistic topics relate to application concepts or architectural components. Usually, our approach partitions a system into one (or sometimes two) large domain-specific clusters and up to a dozen domain-independent clusters. As part of our future work, we plan to investigate more closely the relationship between linguistic topics and both domain and application concepts.

In the future we would also like to investigate in more depth recall and precision of the approach. For example, we would like to compare the results of the semantic clustering with other types of clustering. Furthermore, we would like to improve the labeling with other computer linguistic techniques.

\chapter{Summarizing Software with Lexical Clues and Temporal Clues}

As more and more open-source software components become available on the internet we need automatic ways to label and compare them. For example, a developer who searches for reusable software must be able to quickly gain an understanding of retrieved components. This understanding cannot be gained at the level of source code due to the semantic gap between source code and the domain model. In this paper we present a lexical approach that uses the log-likelihood ratios of word frequencies to automatically provide labels for software components. We present a prototype implementation of our labeling/comparison algorithm and provide examples of its application. In particular, we apply the approach to detect trends in the evolution of a software system.

In recent years, software vocabulary has been proven to be a valuable source for software analysis, often including the retrieval of labels (\eg \cite{Baldi08OOPSLA,EinarHoest,Kuhn07a}). However, labeling software is not without pitfalls. The distribution of words in software corpora follows the same power-law as word frequencies in natural-language text \cite{Linstead09SUITE}. Most of the text is made up of a small set of common terms, whereas content-bearing words are rare. Analysis of software vocabulary must deal with the reality of rare terms, thus statistical tests that assume normal distribution are not applicable. For example, textual comparison based on directly counting term frequencies is subject to overestimation when the frequencies involved are very small.  

For text analysis the use of \loglr improves the statistical results. Likelihood tests do not depend on assumptions of normal distribution, instead they use the asymptotic distribution of binomial likelihood \cite{Dunning}. Using \loglr{}s allows comparisons to be made between the significance of occurrences of both common and rare terms.

In this paper we present a lexical approach that uses the \loglr of word frequencies to automatically retrieve labels from source code. The approach can be applied i) to compare components with each other, ii) to compare a component against a normative corpus, and iii) to compare different revisions of the same component. We present a prototype implementation and give examples of its application. In particular, we apply the approach to detect trends in the evolution of the JUnit software system.

\section{Log-Likelihood in a Nutshell}\label{nutshell}

This section explains how log-likelihood ratio is applied to analyse word frequencies. The explanations are kept as concise as possible. We provide the general background and just enough details such that a programmer may implement the algorithm. Please refer to Ted Dunning's work \cite{Dunning} for more background.

The idea behind \loglr is to compare two statistical hypotheses, of which one is a subspace of the other. Given two text corpora, we compare the hypothesis that both corpora have the same distribution of term frequencies with the ``hypothesis'' given by the actual term frequencies. Because we know that terms are not equally distributed over source code, we use binomial likelihood

$$ L(p,k,n) = p^k ( 1 - p) ^ { n - k }$$
\noindent
with $p = \frac{k}{n}$, where $k$ is the term frequency (\ie number of occurrences) and $n$ the size of the corpus. Taking the logarithm of the likelihood ratio gives
\footnote{In some publications (\eg \cite{Rayson}) we found that the last two terms were omitted (since their values tend to be orders of magnitude smaller than the corresponding values of the first two terms). The results presented in this paper, however, are obtained using the complete log likelihood ratio formula.}

\begin{align*}
-2 \log \lambda =~&2 \big[ \log L(p_1,k_1,n_1) + \log L(p_2,k_2,n_2)\\
 &- \log L(p,k_1,n_1) - \log L(p,k_2,n_2) \big] 
\end{align*}
\noindent
with $p = \frac{k_1 + k_2}{n_1 + n_2}$. The higher the value of $-2log\lambda$ the more significant is the difference between the term frequencies in of both text corpora. By multiplying the $-2log\lambda$ value with the signum of $p_1 - p_2$ we can further distinguish between terms specific to the first corpus and terms specific to the second corpus. Terms that are equally frequent in both corpora have a $-2log\lambda$ value close to zero and thus fall in between.

\paragraph{Example.} Let $C_1$ be the corpus of a software project with size $n_1 = 10^6$, where the words \verb$`rare'$, \verb$`medium'$, and \verb$`common'$ appear respectively 1, 100, and $1\times10^4$ times; and let $C_2$ be the corpus of one of the project's classes with size $n_2 = 1000$, where each word appears 10 times. Then the \loglr{} values are

\begin{center}
\begin{tabular}{l | rrrr}
~ & $p_1$ & $p_2$ & $-2log\lambda$ & $\chi^2$ \\ 
\hline
\verb$rare$ & $10^{-6}$ & $10^{-2}$  & 131.58 & 9.08 \\
\verb$medium$ & $10^{-4}$ & $10^{-2}$  & 71.45 & 0.89 \\
\verb$common$ & $10^{-2}$ & $10^{-2}$  & 0.00 & 0.00 \\
\end{tabular}
\end{center}

The column $\chi^2$ lists the value of Pearson's chi-square test, which assumes normal distribution. As we can see, there is an overestimation when the frequencies involved are very small. Therefore, text analysis should use \loglr{}s to compare the occurrences of common and rare terms \cite{Dunning}.


\section{Applications}\label{applications}

In this section we present two example applications of log-likelihood ratio for software analysis.
There are two main types of corpus comparison: comparison of a sample corpus to a larger corpus, and comparison of a two equally sized corpora. In the first case, we refer to the large corpus as a \emph{normative} corpus since it provides as norm against which we compare.

Applications of these comparisons are
\begin{itemize}
\item \emph{Providing labels for components.} Comparing a component's vocabulary with a large normative corpus (as \eg Sourceforge, Github, or Sourcerer \cite{Bajrach09SUITE}), we obtain labels that describe the component. In the same way, we can compare a class's vocabulary against the containing project.
\item \emph{Comparing components to each other.} Comparing two components, we obtain labels to describe their differences as well as commonalities. This is applicable at any level of granularity, from the level of projects down to the level of methods.
\item \emph{Documenting the history of a component.} Comparing subsequent revisions of the same component, we obtain labels to describe the evolution of that component. (Using multinominal distribution we could even compare all revisions at once, although such results are harder to interpret \cite{Dunning}.)
\item \emph{Describing the results of software search.} Code-search engines have recently received much attention, both commercial engines (as \eg Krugle) and academic engines (as \eg Sourcerer \cite{Bajrach09SUITE}) are publicly available. The result of a search query are typically provided by presenting a peep-hole view on the matching source line and its context to the user. Comparing each result, or the class/project that contains the result, against the entire index we can provide labels that may help users to make better use of these results.
\item \emph{Analysing the structural use of vocabulary.} There has been much confusion regarding which parts of the software vocabulary are to be considered in software analysis. Some approaches consider the entire source code including comments, keywords and literals (\eg \cite{Kuhn08b,Kuhn07a}), other approaches consider class and methods names only (\eg \cite{Baldi08OOPSLA,EinarHoest}). A recent study compared the vocabulary of different structural level using techniques that assume normal distribution \cite{Linstead09SUITE}. Log-likelihood ratios provide a statistically more sound means to study these phenomena.
\end{itemize}

\noindent We implemented $-2log\lambda$ analysis in a Java prototype which is available on the \textsc{Hapax} website\footnote{\url{http://smallwiki.unibe.ch/adriankuhn/hapax}} under AGPL license. In the remainder of this section we present results obtained with that prototype.

\subsection{Labeling the Java API}\label{example1}

\begin{table}
{\scriptsize \begin{center}
\begin{tabular}{lr | lr | lr}
\verb$java.io$ & ~ & \verb$java.text$ & ~ &  \verb$java.util$ & ~ \\
\hline
read & 521.99 & pattern & 228.92 & iterator & 306.91\\
write & 481.93 & format & 209.40 & entry & 301.90\\
skip & 154.61 & digits & 183.24 & next & 237.82\\
close & 113.41 & FIELD & 167.58 & E & 222.33\\
mark & 111.47 & instance & 127.16 & contains & 187.69\\
println & 99.66 & fraction & 104.98 & sub & 166.49\\
UTF & 85.27 & integer & 102.77 & of & 165.57\\
flush & 80.96 & index & 93.43 & K & 154.07\\
desc & 69.19 & run & 90.99 & T & 154.30\\
TC & 68.88 & currency & 91.55 & key & 145.10\\
prim & 61.48 & decimal & 86.34 & all & 145.74\\
char & 61.28 & contract & 84.92 & V & 142.15\\
buf & 60.15 & separator & 72.11 & remove & 128.59\\
stream & 56.86 & grouping & 62.26 & last & 128.09\\
fields & 52.38 & parse & 56.93 & map & 115.68\\
bytes & 47.99 & collation & 56.60 & clear & 114.03\\
\dots & ~ & \dots & ~ & \dots & ~ \\
border & -28.35 & UI & -23.07 & create & -62.67\\
set & -33.89 & border & -22.77 & listener & -62.42\\
remove & -37.49 & property & -24.23 & action & -63.83\\
listener & -39.79 & remove & -30.11 & UI & -64.68\\
accessible & -40.97 & accessible & -32.91 & border & -63.83\\
paint & -44.90 & listener & -31.97 & accessible & -92.25\\
value & -59.10 & type & -32.18 & paint & -101.11\\
get & -64.38 & paint & -36.07 & get & -164.14\\
\end{tabular}
\end{center}}
\caption{Labels retrieved for three Java packages using the full Java 6.0 API as normative corpus.}
\label{tab:one}
\end{table}%


In this example, we compare the packages \verb$java.io$ and \verb$java.text$ and \verb$java.util$ with the normative corpus of the full Java 6.0 API. We use the Java Compiler (JSR 199) to parse the byte-code of the full Java API and then extract the vocabulary of all public and protected elements. We extract the names of packages, classes (including interfaces, annotations, and enums), fields, methods and type parameters. We split the extracted names by camel-case to accomodate to the Java naming convention.

Results are shown in \autoref{tab:one}. For each package we list the most specific words and the least specific words. All three packages are characterized by not covering UI code, in addition \verb$java.io$ and \verb$java.util$ have obviously substantially fewer \verb$get$-accessors than is usual for the Java API. The remaining findings offer no further surprises, except maybe for the uppercase letters in \verb$java.util$ which are generic type parameters; obviously the majority of the Java 6.0 API makes less use of generic types than the collection framework.

%\subsection{Compare Components to Each Other}\label{example2}

%In this example, we compare the \verb$JUnit$ project to the \verb$JExample$ project, both of which are unit testing frameworks. We parse the source code of both projects and extract the complete vocabulary, including comments and literals. We split the extracted words by camel-case to accomodate to the Java naming convention, and apply stemming. We do not, however, exclude common keywords (as \eg \verb$public$ and \verb$float$) since learning about the different use of keywords is a valuable insight as well. 

\subsection{The Evolution of JUnit}\label{example3}

In this example, we report on the vocabulary trends in the history of the \verb$JUnit$\footnote{\url{http://www.junit.org}} project. We use a collection of 14 release distributions of JUnit and parse the source code of each release.  We compare the vocabulary of each two subsequent releases and report on the most significant changes in the vocabulary. We extract all words, including comments; split by camel-case, and exclude English stopwords but not Java keywords.

Results are shown in \autoref{tab:two}. For each release we list the top removed terms and the top added terms, as well as the $-2log\lambda$ value of the top-most term. Large $-2log\lambda$ values indicate substantial changes.

The top 7 change trends (\ie $-2log\lambda \geqslant 100.0$) in the history of JUnit are as follows. In 3.2 removal of \verb$MoneyBag$ example and introduction of graphical UI; in 4.0 removal of graphical UI and introduction of annotation processing; in 4.2 removal of HTML tags from Javadoc comments; in 4.4 introduction of theory matchers and \verb$hamcrest$ framework; in 4.5 introduction of blocks and statements. We manually verified these findings with the release notes of JUnit and found that the findings are appropriate.
 
\begin{table}
{\scriptsize \begin{center}
\begin{tabular}{lrl}
\textbf{JUnit} & $2log\lambda$ & \textbf{Top-10 terms (with $-2log\lambda \geqslant10.0$)} \\
\hline
3 & -8.21 &  \\
~ & 54.11 & count, writer, wrapper \\
\hline
3.2 & -382.80 & money, CHF, assert, case, USD, equals, test, fmb,\\~&~& result, currency \\
~ & 114.21 & tree, model, constraints, combo, reload, swing,\\~&~& icon, pane, browser, text \\
\hline
3.4 & -19.48 & stack, util, button, mouse \\
~ & 15.73 & preferences, base, zip, data, awtui \\
\hline
3.5 & -38.78 & param, reload, constraints \\
~ & 69.34 & view, collector, context, left, cancel, values,\\~&~& selector, views, icons, display \\
\hline
3.6 & -1.20 &  \\
~ & 8.72 &  \\
\hline
3.7 & -8.25 &  \\
~ & 2.79 &  \\
\hline
3.8 & -13.30 & deprecated \\
~ & 23.40 & printer, boo, lines \\
\hline
4.0 & -349.34 & constraints, grid, bag, set, label, panel, path,\\~&~& icon, model, button \\
~ & 350.47 & description, code, nbsp, org, annotation,\\~&~& notifier, method, request, runner, br \\
\hline
4.1 & -1.43 &  \\
~ & 61.90 & link, param, check \\
\hline
4.2 & -288.53 & nbsp, br \\
~ & 20.03 & link, builder, pre, li \\
\hline
4.3.1 & -8.91 &  \\
~ & 53.36 & array, actuals, expecteds, multi, dimensional,\\~&~& arrays, values, javadoc \\
\hline
4.4 & -34.32 & introspector, code, todo, multi, javadoc,\\~&~& dimensional, array, runner, test, fix \\
~ & 151.98 & matcher, theories, experimental, hamcrest,\\~&~& matchers, theory, potential, item, supplier,\\~&~& parameter \\
\hline
4.5 & -30.11 & theory, theories, date, result, static, validator,\\~&~& pointer, string, assert, experimental \\
~ & 124.28 & statement, model, builder, assignment, block,\\~&~& errors, unassigned, evaluate, describable,\\~&~& statements \\
\end{tabular}
\end{center}}
\caption{Evolution of JUnit: for each release we list the removed and the added words, large $2log\lambda$ values indicate more significant changes.}
\label{tab:two}
\end{table}%

%\subsection{On the Structural Use of Vocabulary}\label{example5}

%In this example, we take again the Java API of version 1.6 as case study. We group the extracted names by structural category: package names, class names (which are further subdivided in class and interface names), method names, and field names. As in the first example, we use the Java Compiler API (JSR 199) to parse the binaries and extract the vocabulary of all public and protected elements. We proceed with split by camel-case and stemming.

%\todo{Fill in epic table with results.}

%\todo{Compare the results to Linstead's work.}

\section{Conclusion}\label{eventually}

We presented \loglr as a technique to label software components. Using \loglr{}s allows comparisons to be made between the significance of occurrences of both common and rare terms. We presented how to use the \loglr of word frequencies to retrieve labels, and how to compare a component against a normative corpus. In addition, we proposed to use \loglrs to characterize the evolution of a software component. We presented the results of two example applications, one using the Java API as case study, the other using 14 releases of JUnit as case study. By comparing subsequent revisions of JUnit to each other we were able to characterize substantial changes in the history of JUnit.

\chapter{Spatial Representation with Lexical Clues and Spatial Clues}

To be done

\chapter{Ownership Map with Social and Spatial Clues}

As systems evolve their structure change in ways not expected upfront. As time goes by, the knowledge of the developers becomes more and more critical for the process of understanding the system. That is, when we want to understand a certain issue of the system we ask the knowledgeable developers. Yet, in large systems, not every developer is knowledgeable in all the details of the system. Thus, we would want to know which developer is knowledgeable in the issue at hand. In this paper we make use of the mapping between the changes and the author identifiers (\eg user names) provided by versioning repositories. We first define a measurement for the notion of code ownership. We use this measurement to define the \omap visualization to understand when and how different developers interacted in which way and in which part of the system\footnote{The visualizations in this paper make heavy use of colors. Please obtain a color-printed or electronic version for better understanding.}. We report the results we obtained on several large systems.

Software systems need to change in ways that challenge the original design. Even if the original documentation exists, it might not reflect the code anymore. In such situations, it is crucial to get access to developer knowledge to understand the system. As systems grow larger, not all developers know about the entire system, thus, to make the best use of developer knowledge, we need to know which developer is knowledgeable in which part of the system.

From another perspective, Conway's law \cite{Conw68a} states that ``Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations." That is, the shape of the organization reflects on the shape of the system. As such, to understand the system, one also has to understand the interaction between the developers and the system \cite{Deme02a}.

In this paper we aim to understand how the developers drove the evolution of the system. In particular we provide answers to the following questions:
\begin{itemize}
\item How many authors developed the system?
\item Which author developed which part of the system?
\item What were the behaviors of the developers?
\end{itemize}

In our approach, we assume that the original developer of a line of code is the most knowledgeable in that line of code. We use this assumption to determine the owner of a piece of code (\eg a file) as being the developer that owns the largest part of that piece of code. We make use of the ownership to provide a visualization that helps to understand how developers interacted with the system. The visualization represents files as lines, and colors these lines according to the ownership over time.

Contrary to similar approaches \cite{Ryss04a}, we give a semantic order to the file axis (\ie we do not rely on the names of the files) by clustering the files based on their history of changes: files committed in the same period are related \cite{Gall98a}.

We implemented our approach in Chronia, a tool built on top of the Moose reengineering environment \cite{Duca05a}. As CVS is a de facto versioning system, our implementation relies on the CVS model. Our aim was to provide a solution that gives fast results, therefore, our approach relies only on information from the CVS log without checking out the whole repository. As a consequence, we can analyze large systems in a very short period of time, making the approach usable in the early stages of reverse engineering.

To show the usefulness of our solution we applied it on several large case studies. We report here some of the findings and discuss different facets of the approach.

The contributions of the paper are:
\begin{itemize}
\item The definition of file ownership.
\item The clustering of files based on their commit history.
\item A characterization of developer behaviors.
\item The \omap visualization.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Extraction from CVS log}\label{sec:ownership}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section introduces a measurement to characterize the code ownership. The assumption is that the original developer of a line of code is the most knowledgeable in that line of code. Based on this assumption, we determine the owner of a piece of code as being the developer that owns the most lines  of that piece of code.

The straightforward approach is to checkout all file versions ever committed to the versioning repository and to compute the code ownership from diff information between each subsequent revisions $f_{n-1}$ and $f_n$. From an implementation point of view this implies the transfer of large amounts of data over the network, and long computations.

In this paper, we aim to provide for an approach that can deal with large projects with long history, and that can provide the results fast. As CVS is the very common versioning system, we make tuned our approach to work with the information CVS can provide. In particular we compute the ownership of the code based only on the CVS log information.

Below we present a snippet from a CVS log. The log lists for each version $f_n$ of a file \-- termed revision in CVS \-- the time $t_{f_n}$ of its commit, the name of its author $\alpha_{f_n}$, some state information and finally the number of added and removed lines as deltas $a_{f_n}$ and $r_{f_n}$. We use these numbers to recover both the file size $s_{f_n}$, and the code ownership $own_{f_n}^\alpha$.

\begin{tiny}\begin{verbatim}
----------------------------
revision 1.38
date: 2005/04/20 13:11:24;  author: girba;  state: Exp;  lines: +36 -11
added implementation section
----------------------------
revision 1.37
date: 2005/04/20 11:45:22;  author: akuhn;  state: Exp;  lines: +4 -5
fixed errors in ownership formula
----------------------------
revision 1.36
date: 2005/04/20 07:49:58;  author: mseeberg;  state: Exp;  lines: +16 -16
Fixed math to get pdflatex through without errors.
----------------------------
\end{verbatim}\end{tiny}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measuring File Size}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $s_{f_n}$ be the size of revision $f_n$, measured in number of lines. The number of lines is not given in the CVS log, but can be computed from the deltas $a_{f_n}$ and $r_{f_n}$ of added and removed lines. Even though the CVS log does not give the initial size $s_{f_0}$, we can give an estimate based on the fact that one cannot remove more lines from a file than were ever contained. We define $s_{f_n}$ as in autoref{fig:filesize}: we first calculate the sizes starting with an initial size of 0, and then in a second pass adjust the values with the lowest value encountered in the first pass.

\begin{figure}[htbp]
\begin{minipage}[c]{.45\linewidth}
\includegraphics[width=1.2\linewidth]{minimum}
 \end{minipage}
 \hfill
 \begin{minipage}[c]{.5\linewidth}
\begin{eqnarray}
& s'_{f_0} := 0 \nonumber \\
& s'_{f_n} := s'_{f_{n-1}} + a_{f_{n-1}} - r_{f_n} \nonumber  \\
& s_{f_0} := \vert min \{ s'_x \} \vert \nonumber \\
& s_{f_n} := s_{f_{n-1}} + a_{f_n} - r_{f_n} \nonumber
 \end{eqnarray}
 \end{minipage}
\caption{The computation of the initial size.}
\label{fig:filesize}
\end{figure}

This is a pessimistic estimate, since lines that never changed are not covered by the deltas in the CVS log. This is an acceptable assumption since our main focus is telling the story of the developers, not measuring lines that were never touched by a developer. Furthermore in a long-living system the content of files is entirely replaced or rewritten at least once if not several times. Thus the estimate matches the correct size of most files.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Measuring Code Ownership}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A developer owns a line of code if he was the last one that committed a change to that line. In the same way, we define file ownership as the percentage of lines he owns in a file. And the overall owner of a file is the developer that owns the largest part of it.

Let $own_{f_n}^\alpha$ be the percentage of lines in revision $f_n$ owned by author $\alpha$. Given the file size $s_{f_n}$, and both the author $\alpha_{f_n}$ that committed the change and $a_{f_n}$ the number of lines he added, we defined ownership as:
\[
own_{f_0}^\alpha:=\left\{
    \begin{array}{cl}
        1, & \alpha=\alpha_{f_0} \\
        0, & else
    \end{array}\right.
\]
\[
own_{f_n}^\alpha:=own_{f_{n-1}}^\alpha \frac{s_{f_n} - a_{f_n}}{s_{f_n}} + \left\{
    \begin{array}{cl}
        \frac{a_{f_n}}{s_{f_n}}, & \alpha=\alpha_{f_n} \\
        0, & else
    \end{array}\right.
\]
In the definition we assume that the removed lines $r_{f_n}$ are evenly distributed over the ownership of the preceding owners of $f_{n-1}$.
%A better estimate than $own_{f_n}^\alpha$ can be retrieved by checking out the content of each revision and using a diff algorithm to find out to whom the removed lines actually belonged. But this would, as initially explained, require vast amounts of network traffic and time consuming calculations, and thus the advantages of only processing information from CVS log would be lost.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Ownership Map View}\label{sec:approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We introduce the \omap visualization as in autoref{fig:ownershipDetailsExample}. The visualization is similar to the Evolution Matrix \cite{Lanz02a}: each line represents a history of a file, and each circle on a line represents a change to that file.

The color of the circle denotes the author that made the change. The size of the circle reflects the proportion of the file that got changed \ie the larger the change, the larger the circle. And the color of the line denotes the author who owns most of the file.

Bertin \cite{Bert74a} assessed that one of the good practices in information visualization is to offer to the viewer visualizations that can be grasped at one glance. The colors used in our visualizations follow visual guidelines suggested by Bertin, Tufte \cite{Tuft90a}, and Ware \cite{Ware00a} \-- \eg we take into account that the human brain is not capable of processing more than a dozen distinct colors.

In a large system, we can have hundreds of developers. Because the human eye is not capable of distinguishing that many colors, we only display the authors who committed most of all changes using distinct colors; the remaining authors are represented in gray. Furthermore, we also represent with gray files that came into the CVS repository with the initial import, because these files are usually sources from another project with unknown authors and are thus not necessarily created by the author that performed the import. In short, a gray line represents either an unknown owner, or an unimportant one.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=\linewidth]{owners-map-detail}
\caption{Example of ownership visualization of two files.}
\label{fig:ownershipDetailsExample}
\end{center}
\end{figure}

In the example from autoref{fig:ownershipDetailsExample}, each line represents the lifetime of a file; each circle represents a change. \id{File A} appears gray in the first part as it originates from the initial import. Later the green author significantly changed the file, and he became the owner of the file. In the end, the blue author deleted the file. \id{File B} was created by the green author. Afterwards, the blue author changed the file, but still the green author owns the larger part, so the line remains green. At some point, the red author committed a large change and took over the ownership. The file was not deleted.

\begin{figure*}[hbt]
\begin{center}
\includegraphics{owners-map-example}
\caption{Example of the Ownership Map view. The view reveals different patterns:
Monologue, Familiarization, Edit, Takeover, Teamwork, Bug-fix.}
\label{fig:ownershipMapExample}
\end{center}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ordering the Axes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Ordering the Time Axis.}
Subsequent file revisions committed by the same author are grouped together to form a transaction of changes \ie a commit. We use a single linkage clustering with a threshold of 180 seconds to obtain these groups. This solution is similar to the sliding time window approach of Zimmerman \etal when they analyzed co-changes in the system \cite{Zimm04a}. The difference is that in our approach the revisions in a commit do not have to have the same log comment, thus any quick subsequent revisions by the same author are grouped into one commit.

\paragraph{Ordering the Files Axis.}
A system may contain thousands of files; furthermore, an author might change multiple files that are not near each other if we would represent the files in an alphabetical order. Likewise, it is important to keep an overview of the big parts of the system. Thus, we need an order that groups files with co-occurring changes near each other, while still preserving the overall structure of the system. To meet this requirement we split the system into high-level modules (\eg the top level folders), and order inside each module the files by the similarity of their history. To order the files in a meaningful way, we define a distance metric between the commit signature of files and order the files based on a hierarchical clustering.

Let $H_f$ be the commit signature of a file, a set with all timestamps $t_{f_n}$ of each of its revisions $f_n$. Based on this the distance between two commit signatures $H_a$ and $H_b$ can be defined as the modified Hausdorff distance \footnote{The Hausdorff metric is named after the german mathematician Felix Hausdorff (1868-1942) and is used to measure the distance between two sets with elements from a metric space.} $\delta(H_a,H_b)$:
\[
D(H_n,H_m) := \sum_{n \in H_n} min^2 \{ \vert m -n \vert : m \in H_m \}
\]
\[
\delta(H_a,H_b) := max \{ D(H_a,H_b), D(H_b,H_a) \}
\]

With this metric we can order the files according to the result of a hierarchical clustering algorithm \cite{Jain99a}. From this algorithm a dendrogram can be built: this is a hierarchical tree with clusters as its nodes and the files as its leaves. Traversing this tree and collecting its leaves yields an ordering that places files with similar histories near each other and files with dissimilar histories far apart of each other.

The files axes of the \omap views shown in this paper are ordered with \textit{average linkage} clustering and \textit{larger-clusters-first} tree traversal. Nevertheless, our tool Chronia allows customization of the ordering.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Behavioral Patterns}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Overview Map reveals semantical information about the work of the developer. autoref{fig:ownershipMapExample} shows a part of the \omap of the Outsight case study (for more details see  autoref{sec:outsight}). In this view we can identify several different behavioral patterns of the developers:

\begin{itemize}

\item \emph{Monologue.} Monologue denotes a period where all changes and most files belong to the same author. It shows on an \omap as a unicolored rectangle with change circles in the same color.

\item \emph{Dialogue.} As opposed to Monologue, Dialogue denotes a period with changes done by multiple authors and mixed code ownership. On an \omap it is denoted by rectangles filled with circles and lines in different colors.

\item \emph{Teamwork.} Teamwork is a special case of Dialogue, where two or more developers commit a quick succession of changes to multiple files. On an \omap it shows as circles of alternating colors looking like a bunch of bubbles. In our example, we see in the bottom right part of the figure a collaboration between Red and Green.

\item \emph{Silence.} Silence denotes an uneventful period with nearly no changes at all. It is visible on an \omap as a rectangle with constant line colors and none or just few change circles.

\item \emph{Takeover.} Takeover denotes a behavior where a developer takes over a large amount of code in a short amount of time \-- \ie the developer seizes ownership of a subsystem in a few commits. It is visible on an \omap as a vertical stripe of single color circles together with an ensuing change of the lines to that color. A Takeover is commonly followed by subsequent changes done by the same author. If a Takeover marks a transition from activity to Silence we classify it as an \emph{Epilogue}.

\item \emph{Familiarization.} As opposed to Takeover, Familiarization characterizes an accommodation over a longer period of time. The developer applies selective and small changes to foreign code, resulting in a slow but steady acquisition of the subsystem. In our example, Blue started to work on code originally owned by Green, until he finally took over ownership.

\item \emph{Expansion.} Not only changes to existing files are important, but also the expansion of the system by adding new files. In our example, after Blue familiarized himself with the code, he began to extend the system with new files.

\item \emph{Cleaning.} Cleaning is the opposite of expansion as it denotes an author that removes a part of the system. We do not see this behavior in the example.

\item \emph{Bugfix.} By bug fix we denote a small, localized change that does not affect the ownership of the file. On an \omap it shows as a sole circle in a color differing from its surrounding.

\item \emph{Edit.} Not every change necessarily fulfills a functional role. For example, cleaning the comments, changing the names of identifiers to conform to a naming convention, or reshaping the code are sanity actions that are necessary but do not add functionality. We call such an action \emph{Edit}, as it is similar to the work of a book editor. An Edit is visible on an \omap as a vertical stripe of unicolored circles, but in difference to a Takeover neither the ownership is affected nor is it ensued by further changes by the same author. If an Edit marks a transition from activity to Silence we classify it as an \emph{Epilogue}.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Validation}\label{sec:validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We applied our approach on several large case studies: Outsight, Ant, Tomcat, JEdit and JBoss. Due to the space limitations we report the details from the Outsight case study, and we give an overall impression on the other four well-known open-source projects.

\textbf{Outsight.} Outsight is a commercial web application written in Java and JSP. The CVS repository goes back three years and spans across two development iterations separated by half a year of maintenance. The system is written by four developers and has about 500 Java classes and about 500 JSP pages.

\textbf{Open-source Case Studies.} We choose Ant, Tomcat, JEdit, and JBoss to illustrate different fingerprints systems can have on an \omap. Ant has about 4500 files, Tomcat about 1250 files, JEdit about 500 files, and JBoss about 2000 files. The CVS repository of each project goes back several years.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Outsight}\label{sec:outsight}
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\columnwidth]{commit-histogram}
\caption{Number of commits per team member in periods of three months.}
\label{fig:histogram}
\end{center}
\end{figure}

\begin{figure*}[htbp]
\begin{center}
\includegraphics[height=21.3cm]{outsight}
\caption{The Ownership Map of the Outsight case study.}
\label{fig:casestudy-outsight}
\end{center}
\end{figure*}

The first step to acquire an overview of a system is to build a histogram of the team to get an impression about the fluctuations of the team members over time. autoref{fig:histogram} shows that a team of four developers is working on the system. There is also a fifth author contributing changes in the last two periods only.



autoref{fig:casestudy-outsight} shows the \omap of our case study. The upper half are Java files, the bottom half are JSP pages. The files of both modules are ordered according to the similarity of their commit signature. For the sake of readability we use \id{S1} as a shorthand for the Java files part of the system, and \id{S2} as a shorthand for the JSP files part. Time is cut into eight periods \id{P1} to \id{P8}, each covering three months. The paragraphs below discuss each period in detail, and show how to read the \omap in order to answer our initial questions.

The shorthands in parenthesis denote the labels \id{R1} to \id{R15} as given on autoref{fig:casestudy-outsight}.

\textbf{Period 1.} In this period four developers are working on the system. Their collaboration maps the separation of \id{S1} and \id{S2}: while Green is working by himself on \id{S2} (\id{R5}), the others are collaborating on \id{S1}. This is a good example of Monologue versus Dialogue. A closer look on \id{S1} reveals two hotspots of Teamwork between Red and Cyan (\id{R1,R3}), as well as large mutations of the file structure. In the top part multiple Cleanings happen (\id{R2}), often accompanied by Expansions in the lower part.

\textbf{Period 2.} Green leaves the team and Blue takes over responsibility of \id{S2}. He starts doing this during a slow Familiarization period (\id{R6}), which lasts until end of \id{P3}. In the meantime Red and Cyan continue their Teamwork on \id{S1} (\id{R4}) and Red starts adding some files, which foreshadow the future Expansion in \id{P3}.

\textbf{Period 3.} This period is dominated by a big growth of the system, the number of files doubles as large Expansions happen in both \id{S1} and \id{S2}. The histogram in autoref{fig:histogram} identifies Red as the main contributor. The Expansion of \id{S1} evolves in sudden steps (\id{R9}), and as their file base grows the Teamwork between Red and Cyan becomes less tight. In contradiction the Expansion of \id{S2} evolves in small steps (\id{R8}), as Blue continues familiarizing himself with \id{S2} and slowly but steady takes over ownership of most files in this subsystem (\id{R6}). Also an Edit of Red in \id{S2} can be identified (\id{R7}).

\textbf{Period 4.} Activity moves down from \id{S1} to \id{S2}, leaving \id{S1} in a Silence only broken by selective changes. autoref{fig:histogram} shows that Red left the team, which consists now of Cyan and Green only. Cyan acts as an allrounder providing changes to both \id{S1} and \id{S2}, and Blue is further working on \id{S2}. The work of Blue culminates in an Epilogue marking the end of this period (\id{R8}). He has now completely taken over ownership of \id{S2}, while the ownership of subsystem \id{S1} is shared between Red and Cyan.

\textbf{Period 5 and 6.} Starting with this period the system goes into maintenance. Only small changes occur, mainly by author Blue.

\textbf{Period 7.} After two periods of maintenance the team resumes work on the system. In autoref{fig:histogram} we see how the composition of the team changed: Blue leaves and Green comes back. Green restarts with an Edit in \id{S2} (\id{R11}), later followed by a quick sequence of Takeovers (\id{R13}) and thus claiming back the ownership over his former code. Simultaneous he starts expanding \id{S2} in Teamwork with Red (\id{R12}).

First we find in \id{S1} selective changes by Red and Cyan scattered over the subsystem, followed by a period of Silence, and culminating in a Takeover by Red in the end \ie an Epilogue (\id{R14}). The Takeover in \id{S1} stretches down into \id{S2}, but there being a mere Edit. Furthermore we can identify two selective Bug-fixes (\id{R10}) by author Yellow, being also a new team member.

\textbf{Period 8.} In this period, the main contributors are Red and Green: Red works in both \id{S1} and \id{S2}, while green remains true to \id{S2}. As Red finished in the previous period his work in \id{S1} with an Epilogue, his activity now moves down to \id{S2}. There we find an Edit (\id{R15}) as well as the continuation of the Teamwork between Red and Green (\id{R12}) in the Expansion started in \id{P7}. Yet again, as in the previous period, we find small Bug-fixes applied by Yellow.

To summarize these finding we give a description of each author's behavior, and in what part of the system he is knowledgeable.

\textbf{Red author.} Red is working mostly on \id{S1}, and acquires in the end some knowledge of \id{S2}. He commits some edits and may thus be a team member being responsible for ensuring code quality standards. As he owns a good part of \id{S1} during the whole history and even closed that subsystem end of \id{P7} with an Epilogue, he is the developer most knowledgeable in \id{S1}.

\textbf{Cyan author.} Cyan is the only developer that was in the team during all periods, thus he is the developer most familiar with the history of the system. He worked mostly on \id{S1} and he owned large parts of this subsystem till end of \id{P7}. His knowledge of \id{S2} depends on the kind of changes Red introduced in his Epilogue. A quick look into the CVS log messages reveals that Red's Epilogue was in fact a larger than usual Edit and not a real Takeover: Cyan is as knowledgeable in \id{S1} as Red.

\textbf{Green author.} Green only worked in \id{S2}, and he has only little impact on \id{S1}. He founded \id{S2} with a Monologue, lost his ownership to Blue during \id{P2} to \id{P6}, but in \id{P7} he claimed back again the overall ownership of this subsystem. He is definitely the developer most knowledgeable with \id{S2}, being the main expert of this subsystem.

\textbf{Blue author.} Blue left the team after \id{P4}, thus he is not familiar with any changes applied since then. Furthermore, although he became an expert of \id{S2} through Familiarization, his knowledge might be of little value since Green claimed that subsystem back with multiple Takeovers and many following changes.

\textbf{Yellow author.} Yellow is a pure Bug-fix provider.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ant, Tomcat, JEdit and JBoss}

\begin{figure*}[htb]
\begin{center}
\includegraphics[width=12.5cm]{casestudies-overview}
\caption{The Ownership Map of Ant, Tomcat, JEdit, and JBoss.}
\label{fig:casestudies-owerview}
\end{center}
\end{figure*}

autoref{fig:casestudy-outsight} shows the \omap of four open-source projects: Ant, Tomcat, JEdit, and JBoss. The views are plotted with the same parameters as the map in the previous case study, the only difference is that vertical lines slice the time axis into periods of twelve instead of three months. Ant has about 4'500 files with 60'000 revisions, Tomcat about 1'250 files and 13'000 revisions, JEdit about 500 files and 11'000 revisions, and JBoss about 2'000 files with 23'000 revisions.

Each view shows a different but common pattern. The paragraphs below discuss each pattern briefly.

\textbf{Ant.} The view is dominated by a huge Expansion. After some time of development, the very same files fall victim to a huge Cleaning. This pattern is found in many open-source projects: Developers start a new side-project and when grown up it moves to an own repository, or the side-project ceases and is removed from the repository. In this case, the spin-off is the Myrmidon project, a ceased development effort planned as successor to Ant.

\textbf{Tomcat.} The colors in this view are, apart from some large blocks of Silence, well mixed. The \omap shows much Dialogue and hotspots with Teamwork. Thus this project has developers that  collaborate well.

\textbf{JEdit.} This view is dominated by one sole developer, making him the driving force behind the project. This pattern is also often found in open-source projects: being the work of a single author that contributed about 80\% of the code.

\textbf{JBoss.} The colors in this view indicate that the team underwent to large fluctuations. We see twice a sudden change in the colors of both commits and code ownership: once mid 2001 and once mid 2003. Both changes are accompanied by Cleanings and Expansions. Thus the composition of the team changed twice significantly, and the new teams restructured the system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{On the exploratory nature of the implementation.} We implemented our approach in Chronia, a tool built on top of the Moose reengineering environment \cite{Duca05a}. autoref{fig:chronia} emphasizes the interactive nature of our tool.

On the left of autoref{fig:chronia} we see Chronia visualizing the overall history of the project, which provides a first overview. Since there is too much data we cannot give the reasoning only from this view, thus, Chronia allows for interactive zooming. For example, in the window on the lower right, we see Chronia zoomed into the bottom right part of the original view. Furthermore, when moving the mouse over the \omap, we complement the view by also showing the current position on both time and file axis are highlighted in the lists on the right. These lists show all file names and the timestamps of all commits. As Chronia is build on top of Moose, it makes use of the Moose contextual menus to open detailed views on particular files, modules or authors. For example, in the top right window we see a view with metrics and measurements of a file revision.

\begin{figure*}[htbp]
\begin{center}
\includegraphics{chronia}
\caption{Chronia is an interactive tool.}
\label{fig:chronia}
\end{center}
\end{figure*}

\textbf{On the scalability of the visualization.} Although Chronia provides zooming interaction, one may lose the focus on the interesting project periods. A solution would be to further abstract the time and group commits to versions that cover longer time periods. The same applies to the file axis grouping related files into modules.

\textbf{On the decision to rely on CVS log only.} Our approach relies only on the information from the CVS log without checking out the whole repository. There are two main reasons for that decision.

First, we aim to provide a solution that gives fast results; \eg building the \omap of JBoss takes 7,8 minutes on a regular 3 GHz Pentium 4 machine, including the time spent fetching the CVS log information from the \textit{Apache.org} server.

Second, it is much easier to get access to closed source case studies from industry, when only metainformation is required and not the source code itself. We consider this an advantage of our approach.

\textbf{On the shortcomings of CVS as a versioning system.} As CVS lacks support for true file renaming or moving, this information is not recoverable without time consuming calculations. To move a file, one must remove it and add it later under another name. Our approach identifies the author doing the renaming as the new owner of the file, where in truth she only did rename it. For that reason, renaming directories impacts the computation of code ownership in a way not desired.

\textbf{On the perspective of interpreting the \omap.} In our visualization we sought answers to questions regarding the developers and their behaviors. We analyzed the files from an author perspective point of view, and not from a file perspective of view. Thus the \omap tells the story of the developers and not of the files \eg concerning small commits: subsequent commits by different author to one file do not show up as a hotspot, while a commit by one author across multiple files does. The later being the pattern we termed \textit{Edit}.

Also from a project manager point of view the \omap can give valuable hints. Knowing whether a developer tends more to  Takeover or more to Familiarization is a good indicator to whom the responsibility of subsystem should be given. If a subsystem need rewrites and restructuring the Takeover type is a good choice, otherwise if a subsystem is a good base to be built up on the Familiarization type is a good choice.

But classifications of the authors have to be interpreted in their context. If a developer ly takes over subsystems this does not mean that he has an  character or that he will always tend to Takeovers. In our case study (autoref{fig:casestudy-outsight}) Green's Takeover of \id{S2} in \id{P7} must be seen in the context of the system history: Blue left the team and Green was the original developer of \id{S2}. He would have acted differently if Blue were still in the team.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper we aim to understand how the developers drove the evolution of the system. In particular we ask the following questions:
\begin{itemize}
\item How many authors developed the system?
\item Which author developed which part of the system?
\item What were the behaviors of the developers?
\end{itemize}

To answer them, we define the \omap visualization based on the notion of code ownership. In addition we semantically group files that have a similar \emph{commit signature} leading to a visualization that is not  based on alphabetical ordering of the files but on co-change relationships between the file histories. The \omap helps in answering which authors are knowledgeable in which part of the system and also reveal behavioral patterns. To show the usefulness we implemented the approach and applied it on several case studies. We reported some of the findings and we discussed the benefits and the limitations as we perceived them during the experiments.

In the future, we would like to investigate the application of the approach at other levels of abstractions besides files, and to take into consideration types of changes beyond just the change of a line of code.



\end{document}
