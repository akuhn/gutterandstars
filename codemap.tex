%!TEX root = gutter+stars.tex

%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%

\chapter{Software Cartography}
\label{the chapter on codemap}

\infobox
	{Code orientation in general}
	{Local codebase and possibly history of a system}
	{Spatial (established through lexical and structural information)}
	{Visual analytics of a cartographic visualization}



\asteriskasteriskasterisk

Lexical cues are by far the most common cue used by software engineers for code orientation. Examples of lexical cues are the name of an identifier used to refind some code, or a keyword used to search the web for documentation or code examples. Simple keyword search and regular expressions over the source code are of great help to follow up a lexical cue. The true power of lexical cues, however, is only unleashed when applying information retrieval and natural language processing. 

we use lexical information found in source code to establish a visualization that supports spatial code orientation.

we use lexical information to establish a spatial visualization that supports spatial code orientation by individuals or teams.

we report on a qualitative user study that evaluates a prototype implementation of the techniques introduced in the chapter above. 

Software visualizations can provide a concise overview of a complex software system.
Unfortunately, since software has no physical shape, there is no ``natural'' mapping of software to a two-dimensional space. As a consequence most visualizations tend to use a layout in which position and distance have no meaning, and consequently layout typically diverges from one visualization to another.
We propose an approach to consistent layout for software visualization, called \emSOCA, in which the position of a software artifact reflects its \emph{vocabulary}, and distance corresponds to similarity of vocabulary.
We use \LSI (LSI) to map software artifacts to a vector space, and then use \MDS (MDS) to map this vector space down to two dimensions.
The resulting consistent layout allows us to develop a variety of thematic \emph{software maps} that express very different aspects of software while making it easy to compare them.
The approach is especially suitable for comparing views of evolving software, since the vocabulary of software artifacts tends to be stable over time.
We present a prototype implementation of \SOCA, and illustrate its use with practical examples from numerous open source case studies.

Software visualization offers an attractive means to abstract from the complexity of large software systems.
A single graphic can convey a great deal of information about various aspects of a complex software system, such as its structure, the degree of coupling and cohesion, growth patterns, defect rates, and so on \cite{Dieh07a,Kien07a,Reis05a,Stor05a}.
Unfortunately, the great wealth of different visualizations that have been developed to abstract away from the complexity of software has led to yet another source of complexity: it is hard to compare different visualizations of the same software system and correlate the information they present.

We can contrast this situation with that of conventional thematic maps found in an atlas.
Different phenomena, ranging from population density to industry sectors, birth rate, or even flow of trade, are all displayed and expressed using \emph{the same consistent layout}.
It easy to correlate different kinds of information concerning the same geographical entities because they are generally presented using the same kind of layout.
This is possible because (i) there is a natural mapping of position and distance information to a two-dimensional layout\footnote{Even if we consider that the Earth is not flat on a global scale, there is still a natural mapping of position and distance to a two-dimensional layout; see the many types of cartographic projections (\eg the Mercator projection) used during centuries to do that. In fact, this is true for a large class of manifolds.}, and (ii) because by convention North is normally considered to be on the top.\footnote{The orientation of modern world maps, that is North on the top, has not always been the prevailing convention. On traditional Muslim world maps, for example, South used to be in the top. Hence, if Europe would have fallen to the Ottomans at the Battle of Vienna in 1683, all our maps might be drawn upside down by now \cite{Hite99a}.}

Software artifacts, on the other hand, have no natural layout since they have no physical location.
Distance and orientation also have no obvious meaning for software.
It is presumably for this reason that there are so many different and incomparable ways of visualizing software.
A cursory survey of recent \textsc{Softvis} and \textsc{Vissoft} publications shows that the majority of the presented visualizations feature arbitrary layout, the most common being based on alphabetical order and \emph{arbitrary hash-key order}.
(Hash-key order is what we get in most programming languages when iterating over the elements of a Set or Dictionary collection.)

Consistent layout for software would make it easier to compare visualizations of different kinds of information. But what should be the basis for positioning representations of software artifacts within a ``cartographic'' software map?
What we need is a semantically meaningful notion of position and distance for software artifacts, a spatial representation of software in a multi-dimensional space, which can then be mapped to consistent layout on the 2-dimensional visualization plane.

We propose to use \emph{vocabulary} as the most natural analogue of physical position for software artifacts, and to map these positions to a two-dimensional space as a way to achieve consistent layout for software maps.
Distance between software artifacts then corresponds to distance in their vocabulary.
Drawing from previous work \cite{Kuhn07a,Duca06c} we apply \LSI (LSI) \cite{Deer90a} to the vocabulary of a system to obtain $n$-dimensional locations, and we use \MDS (MDS) \cite{Borg05a} to obtain a consistent layout.
Finally we use cartography techniques (such as digital elevation, hill-shading and contour lines) to generate a landscape representing the frequency of topics. We call our approach \emSOCA, and call a series of visualizations \emph{Software Maps}, when they all use the same consistent layout created by our approach. 


Why should we adopt vocabulary as distance metric, and not some structural property?
First of all, vocabulary can effectively \emph{abstract} away from the technical details of source code \cite{Kuhn07a} by capturing the key domain concepts of source code. Software entities with similar vocabulary are conceptually and topically close. Lexical similarity has proven useful to detect high-level clones \cite{Marc01a} and cross-cutting concerns \cite{Pali08a} in software. Furthermore, it is known that over time vocabulary tends to be more stable than the structure of software \cite{Anto07a}, and tends to grow rather than to change \cite{Vasa07b}. Although refactorings may cause functionality to be renamed or moved, the overall vocabulary tends not to change, except as a side-effect of growth. This suggests that vocabulary will be relatively \emph{stable} in the face of change, except where significant growth occurs. As a consequence, vocabulary not only offers an intuitive notion of position that can be used to provide a consistent layout for different kinds of thematic maps, but it also provides a robust and consistent layout for mapping an evolving system. System growth can be clearly positioned with respect to old and more stable parts of the same system.

This chapter is an extension of previous work, in which we first proposed \emSOCA for consistent layout of software visualizations \cite{Kuhn08b}. The main contributions of the current chapter are:

\begin{itemize}
\item \emph{Improved algorithm.} In our previous work we presented a technique to create software maps given either a single release, or all releases of a system at once. In this chapter we propose an improved algorithm for incremental software maps that update as new changes appear.
\item \emph{Visual stability.} In our previous work we introduced \SOCA as an approach to achieve consistent layouts for software visualization. In this chapter we evaluate four open source case studies to investigate the visual stability of our approach over the evolution of a system. 
\item \emph{Desiderata for spatial representation.} We present a generalization of DeLine's desiderata for spatial software navigation~\cite{Deli05b} to spatial representation in general, and complete them with the desiderata that visual distance should have a meaningful interpretation.
\end{itemize}

\subsection{Desiderata for Spatial Representation of Software}

Robert DeLine's work on software navigation \cite{Deli05b,Deli06a} closely relates to \SOCA. His work is based on the observation that developers are consistently lost in code \cite{Deli05a} and that using textual landmarks only places a large burden on cognitive memory. He concludes the need for new visualization techniques that allow developers to use their spatial memory while navigating source code.

DeLine proposes four desiderata \cite{Deli05b} that should be satisfied by spatial software navigation: 1)~the display should show the entire program and be continuous, 2)~the display should contain visual landmarks such that developers can find parts of the program perceptually rather than relying on names, 3)~the display should remain visually stable during navigation [and evolution], and 4)~the display should be capable of showing global program information overlays other than navigation.

An ad-hoc algorithm that satisfies the first and fourth properties is presented in the same work. As distance metric between software entities (here, methods) an arbitrary chosen score is used.

Our work satisfies all above desiderata, and completes them with a fifth desideratum that visual distance should have a meaningful interpretation. The scope of \SOCA is broader than just navigation, it is also intended for reverse engineering and code comprehension in general. We can thus generalize the five desiderata for spatial representation of software as follows:

\begin{enumerate}
\item The visualization should show the entire program and be continuous.
\item The visualization should contain visualization landmarks that allow the developers to find parts of the system perceptually, rather than relying on name or other cognitive causes. 
\item The visualization should remain visually stable as the system evolves. 
\item The visualization should should be capable of showing global information overlays.
\item On the visualization, distance should have a meaningful interpretation. 
\end{enumerate}

% ===================================================================================
\section{Software Cartography}
\label{sec:techniques}

In this section we present the techniques that are used to achieve a consistent layout for software maps. We present two variations of \SOCA, an \emph{offline} algorithm that requires that all releases of a software system are available upfront, and an improved \emph{online} algorithm that updates the layout incrementally as new releases of the system appear. 

\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{fig/codemap-pipeline}
\end{center}
    \caption{\SOCA in a nutshell: left) the raw text of source files is parsed and indexed using \LSI, center) the high-dimensional term-document-matrix is mapped down to two dimensions using \MDS, and right) cartographic visualization techniques are used to render the final map.}
    \label{fig:soca}
\end{figure}

The general approach of \SOCA, as illustrated in \autoref{fig:soca}, is as follows:
\begin{enumerate}
\item We parse the vocabulary of source files into term-frequency histograms. All text found in raw source code is taken into account, including not only identifiers but also comments and literals.
\item We transform the term-frequency histograms using \LSI (LSI) \cite{Deer90a}, an information retrieval technique that resolves synonymy and polysemy. 
\item We use \MDS (MDS) \cite{Borg05a} to map the term-frequency histograms onto the 2D visualization pane. This preserves the lexical co-relation of source files as well as possible.
\item We use cartographic visualization techniques to render an aesthetically appealing landscape.
\end{enumerate}

Possible applications of \SOCA in the software development process are, \dots

\begin{itemize}
\item \dots to navigate within a software system, be it for development or analysis.
\item \dots to relate different metrics to each other, \eg search results and bug prediction.
\item \dots to stay in touch with other developers of your team, by showing open files of other developers.
\item \dots to understand a system’s domain upon first contact.
\item \dots to explore a system during reverse engineering.
\end{itemize}

\noindent We implemented a prototype of our approach, \CODEMAP, which is available as an open source project. \CODEMAP was originally programmed in Smalltalk, in the mean time development has been moved to Java. \CODEMAP is available as an Eclipse plug-in\footnote{\url{http://scg.unibe.ch/codemap}}.

\subsection{Iterative Online \SOCA}

In our previous work we presented a technique to create software maps given either a single release, or all releases of a system at once \cite{Kuhn08b}. In this chapter we propose an improved algorithm for incremental software maps that update as new changes appear. 

The offline scenario processes all releases in one pass. 

\begin{itemize}

\item The offline variation processes all releases of a software system at once, up to and until \MDS. Only then the location data is grouped by release, and an separate map for each release is rendered. That is both lexical similarity as well as position on the map anticipate all future evolvements from the first map on, since indexing and scaling take information of all releases into account. This scenario requires that information about all releases is available, which is given when performing post-mortem analysis of an existing system.

\item The online variation processes the input release by release (or, when integrated into a development environment, even change by change). For each release, the current source as well as information carried over from the previous \SOCA computation are used. In a first step, the source files of the current release and of previous releases are processed by \LSI. This allows removed and added topics to be detected. In the next step, the lexical similarity of all current source files is fed into \MDS---together with the positions on the previous map as starting points, thus leading to more visual stability.

\end{itemize}

Given a series of releases both variations yield a visually stable sequence of maps. Maps generated with the iterative algorithm are less stable over time compared to the post-mortem approach. The instability of the iterative approach decreases over time, since the amount of accumulated historical data increases with each release. In general, the iterative algorithm is less sensitive to the addition or removal of entire topics between releases, such changes are better observed by performing post-mortem analysis. Please refer to the evaluation in Section~\ref{sec:hausdorff} for more details.

%For example, the JUnit case-study in section  (Hausdorff distance $0.25 \pm 0.11$ versus $0.10 \pm 0.14$).

% -----------------------------------------------------------------------------------
\subsection{Lexical Similarity between Source Files}
\label{sec:lsi}

As motivated in the introduction, the distance between software entities on the map is based on the lexical similarity of source files. Lexical similarity is an Information Retrieval (IR) technique based on the vocabulary of text files. Formally, lexical similarity is defined as the cosine between the term frequency vectors of two text documents. That is, the more terms (\ie identifiers names and operators, but also words in comments) two source files share, the closer they are on the map. 

First, the raw source files are split into terms. Then a matrix is created, which lists for each document the occurrences of terms. Typically, the vocabulary of source code consists of 500--20'000 terms. In fact, studies have shown that the relation between term count and software size follows a power law \cite{Zhan08a}. For this work, we consider all text found in raw source files as terms. This includes class names, methods names, parameter names, local variables names, names of invoked methods, but also words found in comments and literal values. Identifiers are further preprocessed by splitting up the camel-case name convention which is predominantly used in Java source code. Note that since our approach is based on raw text, any programming language that uses textual source files might  be processed.

In a next step, \LSI \cite{Deer90a} is applied to reduce the rank of the term-document matrix to about 50 dimensions. LSI is able to resolve issues of synonymy and polysemy without the use of predefined dictionaries. This is advantageous for the vocabulary of source code which often deviates from common English usage. For more details on \LSI and lexical similarity, please refer to our previous work on software clustering \cite{Kuhn07a}.

% -----------------------------------------------------------------------------------
\subsection{Multi-dimensional Scaling}

In order to visualize the lexical similarity between software entities, we must find a mapping that places source files (or classes, or packages, depending in our definition of a document) on the visualization pane. The placement should reflect the lexical similarity between source files.

We use \MDS (MDS) in order to map from the previously established multi-dimensional term-document matrix down to two dimensions.
 \MDS tries to minimize a stress function while iteratively placing elements into a low-level space. \MDS yields the best approximation of a vector space's orientation, \ie it preserves the distance relation between elements as best as possible. This is good for data exploration problems.

Note that \MDS is not a force-based graph layout algorithm. MDS does not operate on a graph of vertices and edges. MDS maps elements from a high-dimensional metric space to a low-dimensional metric space. In this work, the high-dimensional space is a term-document matrix using Pearson-8 as metric\footnote{When computing the lexical similarity between text documents, it is important to use a cosine or Pearson distance metric. The standard Euclidian distance has no meaningful interpretation when applied to term-document vector space!} and the low-dimensional space a visualization pane with Euclidian metric.

\newlength{\figwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{fig/codemap-pipeline-too.pdf}
\end{center}
    \caption{Construction steps: left) MDS placement of files on the visualization pane, middle) circles around each file's location, based on class size in KLOC, right) digital elevation model with hill-shading and contour lines. Sidebox on digital elevation model) each file contributes a Gaussian shaped basis function to the elevation model according to its KLOC size. The contributions of all files are summed up to a digital elevation model.}
    \label{fig:steps}
\end{figure}

This work applies High-throughput MDS (Hit-MDS), which is an optimized implementation of MDS particularly well-suited for dealing with large data sets \cite{Stri05a}. The algorithm was originally designed for clustering multi-parallel gene expression probes. These data sets contain thousands of gene probes and the corresponding similarity matrix dimension reflects this huge data amount. The price paid for fast computation is less accurate approximation and a simplified distance metric.

% -----------------------------------------------------------------------------------
\subsection{Cartographic Visualization Techniques}

Eventually, we use hill-shading \cite{Burr98a} to render an aesthetically appealing landscape. \autoref{fig:steps} illustrates the final rendering steps of \SOCA. On the final map, each source file is rendered as a hill whose height corresponds to the entity's KLOC size.

Hill-shading uses a digital elevation model (DEM) to render the illumination of a landscape. The digital elevation model is a two-dimensional scalar field. Each each entity contributes a Gaussian shaped basis function to the elevation model. To avoid that closely located entities occlude each other, the contributions of all files are summed up as shown in \figref{steps}. 

\begin{figure*}
  \includegraphics[width=\linewidth]{fig/chronia-ludo-history}
  \caption{From left to right: each map shows an consecutieve iteration of the same software system. As all four views use the same layout, a user can build up a mental model of the system's spatial structure. For example, {\tt Board/LudoFactory} is on all four views located in the south-western quadrant. See also Figure~5 and 6 for more views of this system.}
  \label{fig:ludo}
\end{figure*}


A map without labels is of little use. On a software map, all entities are labeled with their name (class or file name). Labeling is a non-trivial problem, as an algorithm is needed to ensure that labels do not overlap. Also labels should not overlap important landmarks. Most labeling approaching are semi-automatic and need manual adjustment, an optimal labeling algorithm does not exist \cite{Sloc05a}. 

For locations that are near to each other it is difficult to place the labels so that they do not overlap and hide each other. For software maps it is even harder due to often long class names and clusters of closely related classes. This work uses a greedy brute-force algorithm for labeling. Labels are placed in order of hill size, \ie the name of the largest file is placed first, and so on. If a to-be placed label would overlap with an already placed label, the to-be placed label is omitted. Thus, the labels of smaller files are typically omitted in favor of the labels of larger files.

%\begin{figure}
%\begin{center}
%  \includegraphics[width=\linewidth]{figures/workflow2}
%\end{center}
%    \caption{\SOCA variations: (top) the offline approach processes all releases of a software system at once, and eventually renders a separate map for each release; (bottom) the iterative approach processes all source files up to the current release with LSI, and when applying MDS reuses the previous map as starting configuration.}
%    \label{fig:soca2}
%\end{figure}

% =============================================================================
\section{On the Choice of Vocabulary}
\label{sec:onvocabulary}

The decision to use a distance based on lexical similarity does, indeed, create a distribution of distances that should not change a lot in time. This is because programmers will not use a completely new set of lexical tokens in each new version of the software. In fact, it has been shown that over time vocabulary tends to be more stable than the structure of software \cite{Anto07a}. However, this also will create software maps that naturally only can show how items are similar from a lexical point of view.

The map layout as presented in this work can, of course, be used to see how items are related from the point of view of some other distance, such as considering structural similarity, similarity with regard to a complexity or testability metric. In that case, the distance may vary a lot over time during the evolution of a product, and this will create unstable layouts. The focus of this work, however, is the creation of maps that help programmers to establish a stable mental model of their software system under work. In any case, if maps  based on other metrics are ever to be used in conjunction with vocabulary-based \SOCA maps, we strongly recommend to visually distinguish them by using another rendering scheme. This helps to reduce the likeliness that programmers confuse the spatial layout of these other maps, with the mental model acquired through the use of \SOCA maps. 

As mentioned in the introduction, \SOCA is vocabulary-based because vocabulary can effectively \emph{abstract} away from the technical details of source code \cite{Kuhn07a} by capturing the key domain concepts of source code. The assumption being that software entities with similar vocabulary are conceptually and topically close. Consider, for example, programming languages and software where name overloading is applied. Even though overloaded methods differ in their implementation strategy, they will typically implement the same concept using the same vocabulary. In fact, lexical similarity has proven useful to detect high-level clones \cite{Marc01a} and cross-cutting concerns \cite{Pali08a} in software.  

Due to name scoping, semantically different scopes can have identical names with different meanings. Consider, for example, two large functions having mostly identifiers such as i, j, prev, next, end, stop, flag, \dots;  the one does some matrix computations, while the other is a hash-table implementation. Without the application of \LSI (\secref{lsi}) the two would be classified as being very similar, while this is clearly not true from a developer's perspective. \LSI, however, can identify words that have different meaning depending on their context. LSI has the ability to resolve certain synonymy and polysemy \cite{Deer90a}.


Although refactorings may cause functionality to be renamed or moved, the overall vocabulary tends not to change, except as a side-effect of growth \cite{Zhan08a,Vasa07b}. Consider the example of a rename refactoring. Two effects may occur. 
In the first case, all occurrences of a symbol are replaced with new symbol. This will not affect the map, since both lexical similarity and LSI are based on statistical analysis only. Replacing all occurrences of one term with a new term is, from the point of these IR technologies, a null operation. In the second case, some occurrences of a symbol are replaced with another symbol which is already used. This will indeed affect the layout of the map. Given that the new name was well chosen by the programmer, the new layout constitutes a better representation of the system. On the other hand, if the new name is a bad choice, the new layout is flawed. However, what constitutes bad naming is not merely a matter of taste. Approaches that combine vocabulary with structural information can indeed assess the quality of naming. Please refer to H\o{}st's recent work on debugging method names for further reading \cite{Hoes09a}.

Not considered in the present work is the relative weight of different lexical tokens. For example, it seems reasonable to weight local identifiers differently than identifiers in top-level namespaces. Also, one may treat names coming from library functions different from the ones coming from the actual user code. Given the absence of evaluation benchmarks, we decided to use equal weighting for all lexical token. Also, preliminary experiments with different weighting schemes indicate that relative weights below boost level, \ie below a factor of 10, do often not significantly affect the overall layout.

% =============================================================================
\section{Examples}
\label{sec:casestudy}

In this section we present examples of \SOCA. The first example visualizes the evolution of a small software system. The second example shows an overview of six open-source systems. As the third example, we provide two thematic overlays of the same software map.

% -----------------------------------------------------------------------------------
\subsection{The Evolution of Ludo}

Figure~\ref{fig:ludo} shows the complete history of the Ludo system, consisting of four iterations. Ludo is used in a first year programming course to teach iterative development (please mail the first author to get the sources). The 4th iteration is the largest with 30 classes and a total size of 3-4 KLOC. We selected Ludo because in each iteration, a crucial part of the final system is added. 

\begin{itemize}

\item The first map (\autoref{fig:ludo}, leftmost) shows the initial prototype. This iteration implements the board as a linked list of squares. Most classes are located in the south-western quadrant. The remaining space is occupied by ocean, nothing else has been implemented so far.

\item In the second iteration (\autoref{fig:ludo}, second to the left) the board class is extended with a factory class. In order to support players and stones, a few new classes and tests for future game rules are added. On the new map the test classes are positioned in the north-eastern quadrant, opposite to the other classes. This indicates that the newly added test classes implement a novel feature (\ie testing of the game's ``business rules'') and are thus not related to the factory's domain of board initialization. 

\item During the third iteration (\autoref{fig:ludo}, second to the right) the actual game rules are implemented. Most rules are implemented in the {\tt Square} and {\tt Ludo} class, thus their mountain rises. In the south-west, we can notice that, although the {\tt BoardFactory} has been renamed to {\tt LudoFactory}, its position on the map has not changed considerably. 

\item The fourth map (\autoref{fig:ludo}, rightmost) shows the last iteration. A user interface and a printer class have been added. Since both of them depend on most previous parts of the application they are located in the middle of the map. Since the new UI classes use vocabulary from all parts of the system, the islands are joined into a continent.

\end{itemize}

The layout of elements remains stable over all four iterations.  For example, {\tt Board/LudoFactory} is on all four views located in the south-western quadrant. This is due to LSI's robustness in the face of synonymy and polysemy; as a consequence most renamings do not significantly change the vocabulary of a software artifact \cite{Kuhn07a}.

%\setlength{\figwidth}{0.32\textwidth}
%\begin{figure*}
%\begin{center}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{apache-tomcat-bw}\\
%  Apache Tomcat
%\end{center}
%\end{minipage}~
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{columba-bw}\\
%  Columba
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{google-taglib-bw}\\
%  Google Taglib
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{j-fdp-bw}\\
%  JFtp
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{JoSQL-bw}\\
%  JoSQL
%\end{center}
%\end{minipage}
%\begin{minipage}{\figwidth}
%\begin{center}
%  \includegraphics[width=\figwidth]{figures-bw/jcgrid-bw}\\
%  JCGrid
%\end{center}
%\end{minipage}
%\end{center}
%\vspace{1ex}
%    \caption{Overview of the software maps of six open source systems. Each map reveals a distinct spatial structure. When consequently applied to every visualization, the consistent layout may soon turn into the system's iconic fingerprint. An engineer might \eg point to the top left map and say: ``Look, this huge {\tt Digester} peninsula in the north, that must be Tomcat. I know it from last year's code review.''.
%    }
%    \label{fig:fullpage}
%\end{figure*} 

% -----------------------------------------------------------------------------------
\subsection{Open-source examples}

We applied the \SOCA approach to all systems listed in the field study by Cabral and Marques \cite{Cabr07a}. They list 32 systems, including 4 of each type of application (Standalone, Server, Server Applications, Libraries) and selected programming language (Java, .NET).  

Figure \ref{fig:fullpage} shows the software map for six of these systems: Apache Tomcat, Columba, Google Taglib, JFtp, JCGrid and JoSQL. Each system reveals a distinct spatial structure. Some fall apart into many islands, like JFtp, whereas others cluster into one (or possibly two) large contents, like Columba and Apache Tomcat. The 36 case-studies raised interesting questions for future work regarding the correlation between a system's layout and code quality. For example, do large continents indicate bad modularizations? Or, do archipelagoes indicate low coupling?

%Each system's size in TLC and KLOC is listed in Table~\ref{tab:six}.

%\begin{table}[htdp]
%\begin{center}
%\begin{tabular}{l|rr}
%\textbf{System} & \textbf{\# Top-level} & \textbf{KLOC} \\
% & \textbf{classes} & \\
%\hline
%Apache Tomcat & 162 & 14'700 \\
%Columba & 1'549 & 53'500 \\
%Google Taglib & 20 & 940 \\ 
%JFtp & 78 & 3'470 \\
%JCGrid & 94 & 3'630 \\
%JoSQL & 83 & 6'480 \\
%\end{tabular}
%\end{center}
%\vspace{1ex}
%\caption{Statistics of the six systems in Figure~\ref{fig:fullpage}. \AK{Are you sure this is KLOC? Does Google Taglib really have 0.9 million(!) lines of code in 20 classes only?}
%}
%\label{tab:six}
%\end{table}

% -----------------------------------------------------------------------------------
\subsection{Thematic cartography examples}

\begin{figure}
\begin{center}
\begin{minipage}{1.4\figwidth}
\begin{center}
  \includegraphics[width=1.4\figwidth]{fig/chronia-ludo-testtubes}
\end{center}
\end{minipage}
\begin{minipage}{1.4\figwidth}
\begin{center}
  \includegraphics[width=1.4\figwidth]{fig/chronia-ludo-testexecution}
\end{center}
\end{minipage}
\end{center}
    \caption{Software maps with thematic overlay: (left) glyphs are drawn on top of the map, to display additional information. Each test tube glyph indicates the location of unit test case, (right) invocation edges are drawn on top of the map, showing the trace of executing the {\tt RuleThreeTest} test case.}
    \label{fig:mock}
\end{figure}

Software maps can be used as canvas for more specialized visualizations of the same system. In the following, we provide two thematic visualization of the Ludo system that might benefit from consistent layout. (The maps in this subsection are mockups, not yet fully supported by \CODEMAP.)

\begin{itemize}

\item Boccuzzo and Gall present a set of metaphors for the visual shape of entities \cite{Bocc07a}. They use simple and well-known graphical elements from daily life, such as houses and tables. However they use conventional albeit arbitrary layouts, where the distribution of glyphs often does not bear a meaningful interpretation. The first map in Figure~\ref{fig:mock} (on the left) employs their technique on top of a software map, using test tubes to indicate the distribution of test cases.

\item Greevy \etal present a three-dimensional variation of System Complexity View to visualize a System's dynamic runtime state \cite{Gree05d}. They connect classes with edges representing method invocation, and stack boxes on top of each other to represent a class's instances.
Since System Complexity Views do not capture any notion of position, the lengths of their invocation edges do not express any real sense of distance.

Figure~\ref{fig:mock} (on the right) employs their approach on top of a software map, drawing invocation edges in a two-dimensional plane.
Here the distances have an interpretation in terms of lexical distance, so the lengths of invocation edges are meaningful.
A short edge indicates that closely related artifacts are invoking each other, whereas long edges indicate a ``long-distance call'' to a lexically unrelated class.

\end{itemize}

%\section{Conclusion}
%
%This chapter presents \SOCA, a spatial representation of software. Our approach visualizes software entities using a consistent layout. Software maps present the entire program and are continuous. Software maps contain visual landmarks that allow developers to find parts of the system perceptually rather then relying on conceptual cues, \eg names. Since all software maps of a system use the same layout, maps with thematic overlays can be compared to each other.
%
%The layout of software maps is based on the lexical similarity of software entities. Our algorithm uses \LSI to position software entities in an multi-dimensional space, and \MDS to map these positions on a two-dimensional display. Software maps can be generated to depict evolution of a software system over time. We evaluated the visual stability of iteratively generated maps considering four open source case studies.
%
%In spite of the aesthetic appeal of hill shading and contour lines, the main contribution of this chapter is not the cartographic look of software maps. The main contribution of \SOCA is (i) that cartographic position reflects topical distance of software entities, and (ii) that consistent layout allows different software maps to be easily compared.
%In this way, software maps reflect world maps in an atlas that exploit the same consistent layout to depict various kinds of thematic information about geographical sites.
%
%We have presented several examples to illustrate the usefulness of software maps to depict the evolution of software systems, and to serve as a background for thematic visualizations.
%The examples have been produced using \CODEMAP, a proof-of-concept tool that implements our technique.
%
%As future work, we can identify the following promising directions:
%\begin{itemize}
%  \item Software maps at present are largely static.
%  We envision a more interactive environment in which the user can ``zoom and pan'' through the landscape to see features in closer detail, or navigate to other views of the software.
%  \item Selectively displaying features would make the environment more attractive for navigation. Instead of generating all the labels and thematic widgets up-front, users can annotate the map, adding comments and waymarks as they perform their tasks.
%  \item Orientation and layout are presently consistent for a single project only.
%  We would like to investigate the usefulness of conventions for establishing consistent layout and orientation (\ie ``testing'' is North-East) that will work across multiple projects, possibly within a reasonably well-defined domain.
%  \item We plan to perform an empirical user study to evaluate the application of \SOCA for software comprehension and reverse engineering, but also for source code navigation in development environments.
%\end{itemize}

%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%

\chapter{User Study on Software Cartography}
\label{the chapter on the codemap user study}

Current tool support for code orientation by spatial clues is \adhoc at best, most striking is the lack of spatial on-screen representations of source code. Without such a representation, developer are barely able to draw on the strong spatial capability of the human brain. 

In this chapter we provide a cartographic on-screen visualization so they can start using spatial clues for code orientation. Since software has no inherent spatial structure, we use lexical and structural information found in the source code to establish a spatial layout of the local code base. Code maps, as we call them, are stable over time and can be shared among members of a team to establish a common mental model of the system. 
%
We implemented our approach in a prototype, evaluated it in a user study, and found that it is most helpful for spatially exploring search results and call hierarchies.

Software visualization can be of great use for understanding and exploring a software system in an intuitive manner. Spatial representation of software is a promising approach of increasing interest. However, little is known about how developers interact with spatial visualizations that are embedded in the IDE. In this chapter, we present a pilot study that explores the use of \SOCA for program comprehension of an unknown system. We investigated whether developers establish a spatial memory of the system, whether clustering by topic offers a sound base layout, and how developers interact with maps. We report our results in the form of observations, hypotheses, and implications. Key findings are a) that developers made good use of the map to inspect search results and call graphs, and b) that developers found the base layout surprising and often confusing. We conclude with concrete advice for the design of embedded software maps.

Software visualization can be of great use for understanding and exploring a software system in an intuitive manner. In the past decade the software visualization community has developed a rich wealth of visualization approaches~\cite{Dieh07a} and provided evidence of their usefulness for expert tasks, such as reverse engineering, release management or dynamic analysis (\eg \cite{Tele08b,Pauw08a,Reis05b,Orso03a}). Typically, these visualization approaches had been implemented in interactive tools \cite{Sens08a}. However most of these tools are stand-alone prototypes that have never been integrated in an IDE (integrated development environment). Little is thus known about the benefits of software visualization for the ``end users'' in software engineering, that is for everyday programmers. What is lacking is how these techniques support the day to day activities of software developers \cite{Stor05a}. 

In this chapter, we report on a pilot study of a spatial software visualization that is embedded in the IDE. The spatial visualization is based on the \SOCA approach that has been presented and introduced in previous work \cite{Kuhn08a,Kuhn10b,Erni10a}. Spatial representation of software is a promising research field of increasing interest \cite{Wett08a,Deli10a,Brag10b,Stei10a,Mart08a,Noac05a}, however the respective tools are either not tightly integrated in an IDE or have not yet been evaluated in a user study. Spatial representation of software is supposed to support developers in establishing a long term, spatial memory of the software system. Developers may use spatial memory to recall the location of software artifacts, and to put thematic map overlays in relation with each other \cite{Kuhn10b}.

%\ewe{Maybe comment somewhere that the level of integration may vary. Distant integration would be the case of the visualization being available within the IDE, but remain rather an supplementary tools. Tight integration implies that the map reacts to other events in the IDE, e.g. file close, etc. or at least stays sync'ed with the project}

The scenario of our user study is first contact with an unknown closed-source system. Our main question was whether and how developers make use of the embedded visualization and if our initial assumptions made when designing the visualization (as for example the choice of lexical similarity as the map's base layout \cite[Sec 3]{Kuhn10b}) are based on a valid model of developer needs. Participants had 90 minutes to solve 5 exploratory tasks and to fix one bug report. We used the think-aloud protocol and recorded the voices of the participants together with a screen capture of their IDE interactions. We took manual notes of IDE interaction sequences and annotated the sequences with the recorded think-aloud transcripts. 

Results are mixed\,---\,some support and some challenge our assumptions on how developers would use the embedded visualization. Participants found the map most useful to explore search results and call graphs, but only rarely used the map for direct navigation as we would have expected.
%It became quickly apparent that we should revise our initial assumption that lexical similarity is a valid basis for the cartographic layout. Participants interpreted visual distance as a measure of structural dependencies---even though they were aware of the underlying lexical implementation.

Contributions of this chapter are as follows: 
%\nes{These aren't the contributions, they're summaries of your methodology, and promises of contributions.}

\begin{itemize}
\item We embedded the stand-alone \Codemap prototype in the Eclipse IDE, and added novel thematic overlays that support the most important development tasks with visual feedback (see \autoref{sec:tasks} and \autoref{sec:steps}).
\item We performed a think-aloud user study to evaluate the use of spatial visualization in the IDE. We discuss and comment on our results, and conclude with practical design implications (see \autoref{sec:results} and \autoref{sec:discussion}).
\item We provide suggestions on how to improve \SOCA and the \Codemap tool (see \autoref{sec:related}).
\end{itemize}

% ===================================================================================
\section{Software Cartography}
\label{sec:tasks}

\SOCA uses a spatial visualization of software systems to provide software development teams with a stable and shared mental model. The basic idea of cartographic visualization is to apply thematic cartography~\cite{Sloc05a} on software visualization. That is, to show thematic overlays on top of a stable, spatial base layout. Features on a thematic map are either point-based, arrow-based or continuous. For software this could be the dispersion of design flaws as visualized using icons; a call graph  is visualized as a flow map (as illustrated on \autoref{fig:awesome}); and test coverage is visualized as a choropleth map, \ie a heat map. 

\SOCA is most useful when it supports as many development tasks with spatial location awareness as possible. We therefore integrated our prototype into the Eclipse IDE so that a map of the software system may always be present. This helps developers to correlate as many development tasks as possible with their spatial location.

At the moment, the \Codemap plug-in for \eclipse supports the following tasks:\footnote{\url{http://scg.unibe.ch/codemap}}
% New features are added on a weekly base, please subscribe to \url{http://twitter.com/codemap} to receive latest news.}
% ON: The twitter feed is listed on the web page -- don't mention it in the chapter


\begin{itemize}
\item Navigation within a software system, be it for development or analysis. \Codemap is integrated with the package explorer and editor of \eclipse. The selection in the package explorer and the selection on the map are linked. Open files are marked with an icon on the map. Double clicking on the map opens the closest file in the editor. When using heat map mode, recently visited classes are highlighted on the map.

\item Comparing software metrics to each other, \eg to compare bug density with code coverage. The map displays search results, compiler errors, and (given the Eclemma plug-in is installed) test coverage information. More information can be added through an plug-in extension point.

\item Social awareness of collaboration in the development team. \Codemap can connect two or more \eclipse instances to show open files of other developers. Colored icons are used to show the currently open files of all developers. Icons are colored by user and updated in real time.

\item Understand a software system's domain. The layout of \Codemap is based on clustering software by topic~\cite{Kuhn07a}, as it has been shown that, over time, the lexicon of source code is more stable than its structure~\cite{Anto07a}. Labels on the map are not limited to class names, but include automatically retrieved keywords and topics.


\item Exploring a system during reverse engineering. \Codemap is integrated with \eclipse's structural navigation features, such as search for callers, implementers, and references. Arrows are shown for search results. We apply the \textsc{Flow Map} algorithm \cite{Phan05a} to  avoid visual clutter by merging parallel arrow edges. \autoref{fig:awesome} shows the result of searching for calls to the {\tt \#getSettingOrDefault} method in the {\tt MenuAction} class .
\end{itemize}

\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{fig/codemap-example}
\end{center}
    \caption{\emph{Thematic codemap of a software system. Here the \Codemap tool itself is shown. Arrow edges show incoming calls to the {\tt \#getSettingOrDefault} method in the {\tt MenuAction} class, which is currently active in the editor and thus labeled with a pop-up.}}
    \label{fig:awesome}
\end{figure}

% ===================================================================================
\section{Methodology}
\label{sec:method}

We evaluated our approach in a pilot study with professional developers and students. The scenario investigated by the experiment is first contact with an unknown software system. Participants have 90 minutes to solve 5 program comprehension tasks and to fix one bug report. After the experiment, participants are asked to sketch a drawing of their mental map of the system. 

Our goal for the present pilot study was to learn about the usability of \Codemap for program comprehension. We have been seeking to answer several questions. 
How can we support developers in establishing a spatial memory of software systems? How do we best support the developers spatial memory using software visualization? How to best embed spatial software visualization in the IDE? When provided with spatial representation of search results and call graphs, how do developers make use of them?  

Not covered in this study, and thus open for future user studies, are the shared team awareness and long term memory claims of the \SOCA approach \cite{Kuhn10b}.

%\AK{More to come here.}
%\todo{Need more background about the design rational of the study, about what we cover from our claims (ie that codemap helps to explore unknown software and that codemap helps to build up a spatial model) and which we do not cover (that the model is stable over months or years, that multiple team members share the same model, etc) and why we've chosen to cover those. Also tell why we do an exploratory study instead of going with the herd and doing one of those controlled experiments ... Cite interview with Andy Ko ftw! Also tell here that we would address other stake holders, like managers and architects that are supposed to share the same long term, spatial memory.}
%\on{this one? \url{http://doi.ieeecomputersociety.org/10.1109/MS.2009.122} -- does not seem right}
%
%
%\ewe{What I do lack so far is a clear explanation of your assumption about the map's role: (1) do you expect the map to become a central tool in development, or an auxiliary help (2) what's your position regarding "there is more than one way to do one thing", and how do you deal with it in the study. E.g. a dev uses the map to locate search result another not. Is it positive or negative? (3) what was the rationale to design these feature in the map, e.g. we thought that   synchronizing the map with the open files would benefit development because xzy. Then you can validate or invalidate each hypothesis}

\subsection{Design of the Study}

The study consists of six programming tasks. The training task introduced the participants to the \Codemap plug-in. The first five tasks were program comprehension tasks, starting with general questions and then going into more and more detailed questions. Eventually, the last task was to fix an actual bug in the system. Participants were asked to use the map whenever they saw fit, but otherwise they were free to use any other feature of Eclipse they wanted.

\paragraph{Task 1, Domain and Collaborators} \emph{``Find the purpose of the given application and identify the main collaborators. Explore the system, determine its domain, and fulfil the following tasks: a) describe the domain, b) list the main collaborators, c) draw a simple collaboration diagram, d) identify the main feature of the application.''}

\paragraph{Task 2, Technologies} \emph{``In this task we are interested in the technologies used in the application. List the main technologies, such as for example Ajax, XML, or unit testing.''}

\paragraph{Task 3, Architecture}  \emph{``In this task we are going to take a look at the architecture of the application. Reverse engineer the architecture by answering the following questions: a) which architectural paradigm is used (as for example pipes and filters, layers, big ball of mud, etc)? b) what are the main architectural components? c) how are those components related to one another? d) draw a UML diagram at the level of components.''}

\paragraph{Task 4, Feature Location} \emph{``In this task we are interested in classes that collaborate in a given feature. Please locate the following features: a) Interactive users are reminded after some months, and eventually deleted if they do not log in after a certain number of months, b) Depending on the kind of user, a user can see and edit more or less data. There are permission settings for each kind of user that are checked whenever data is accesses, and c) Active search: the system compares the curriculum vitae of the users with stored searches of the companies and mails new matches to the companies.''}

\paragraph{Task 5, Code Assessment} \emph{``In this task we want to assess the code quality of the application. Please answer the following questions: a) what is the degree of test coverage? b) are there any god classes? c) are the classes organized in their proper packages? Should certain classes be moved to other packages? Please list two to three examples.''} 

We provided a code coverage plug-in with the experiment, as well as a definition of what constitutes a god class \cite{Lanz06a}.

\paragraph{Task 6, Bug Fixing} In this task we provided an actual bug report and asked \emph{``Describe how you would handle the bug report, that is how and where you would change the system and which classes are involved in the bug fix. You are not asked to actually fix the bug, but just to describe how you would fix it.''}

\subsection{Participant Selection}

Participants were selected through an open call for participation on Twitter\footnote{\url{http://twitter.com/codemap}} as well as through flyers distributed at a local Eclipse event. Subjects were required to be medium level Java programmers with at least one year of experience with both Java and Eclipse programming. The six tasks had been designed so that the participants did not need to be knowledgeable with the provided application, but rather that they explore it as they go along. Seven participants took part in the experiment: 4 graduate students and 3 professional developers from industry. None of the participants was familiar with the provided application or with the Codemap plugin; even though some had attended a 15 minute presentation about the Codemap plugin at the Eclipse event mentioned above. 

\subsection{Study Setting}

The study consisted of three main parts. The first part was the training task in which the participants were given a short presentation of Codemap and a tutorial document that explained all features of the Codemap plug-in. The tutorial explained all features mentioned in \autoref{sec:tasks} using walk-through descriptions of their use. The participants were given 20 minutes to explore a small example program using the Codemap plug-in. When they felt ready, we started part two of the experiment.

The second part consisted of the actual programming tasks. A fixed amount of time was allotted to each task. Participants were asked to spend no more than 15 minutes on each task. All subjects had access to the Codemap plugin as our aim was to explore their use of the plugin rather than to compare a controlled parameter against the baseline.

Eventually, in a third part we held a debriefing session. We asked participants to draw a map (with any layout or diagram language whatsoever) of how they would explain the system under study to another developer. We asked the participants for feedback regarding their use of the Codemap plugin and how the plugin could be improved.

% ===================================================================================
\section{Data Collection}
\label{sec:analysis}

We asked the participants to think aloud, and recorded their voice together with a captured video of their computer screen using the Camtasia software\footnote{\url{http://www.techsmith.com/camtasia}}. We reminded the participants to think aloud whenever they fell  silent: we told them to imagine a junior programmer sitting beside them to whom they are to explain their actions (Master/Apprentice \cite{Hugh97a}).  The participants were asked to respond to a survey while performing the study. The survey consisted of their answers to the tasks, as well as the perceived difficulty of the tasks and whether they found the Codemap plugin useful for the task at hand. We used a combination of semantic differential statements and Likert scales with a 5 point scale.

We measured whether or not subjects were successful in completing a programming task. We used three success levels to measure the success and failure of tasks: a task could be a success, a partial success or a failure. We further subdivided tasks 4 and 5 into three subtasks and recorded success levels for each individual subtask. We asked one of the original authors of the system to assess the success levels. As this was a think-aloud study, we did not measure time, but alloted a fixed 15 minute slot to each task.

Our main interest was focused on how the participants used the IDE to solve the tasks, independent of their success level. To do this, we transcribed important quotes from the recorded participant voices and screen captures and took notes of the actions that the participants did during the tasks. For each task we tracked the use of the following IDE elements:

\begin{itemize}
\item Browsing the system using the \emph{Package Explorer} and \emph{Outline} view. This includes both drill-down as well as linear browsing of package, class and method names.
\item Browsing the system using the spatial visualization of the Codemap plugin. This includes both opening single classes, selecting a whole cluster of classes on the map, as well as reading class name labels on the map.
\item Reading source code in the editor pane, including documentation in the comments of class and method headers.
\item Navigating the structure of the system using the \emph{Type Hierarchy} and \emph{Call Hierarchy} view. We tracked whether they explored the results of these searches in Eclipse's tabular result view or using the flow-map arrows displayed on  the spatial visualization of Codemap.
\item Searching the structure of the system with either the \emph{Open Type} or \emph{Java Search} dialog. This allows users to search for specific structural elements such as classes, methods or fields. Again, we tracked whether they explored the results in Eclipse's result view or on the visualization of Codemap.
\item Searching the system with the unstructured text search, either through the \emph{Java Search} dialog or the immediate search bar of the Codemap plugin. Also here, we tracked whether they explored the results in Eclipse's result view or on the visualization of Codemap.
\end{itemize}

Replicability: the raw data of our analysis is available on the \Codemap website at \url{http://scg.unibe.ch/codemap}.

%For each task we recorded a three-level usage of the above IDE elements, that is the main means of solving the task, used among other means, or not used at all (marked with $xx$, $x$, or empty cell in \autoref{fig:table}). In addition, we took notes of particular usage sequences that we annotated with the think-aloud transcriptions. 

%========================================================
\section{Results}
\label{sec:results}

After analyzing our data, we observed different degrees of interaction with the \Codemap plug-in. We focused our analysis on interaction sequences that included interaction with the \Codemap plug-in, but also on those interaction sequences that challenged our assumptions about how developers would make use of the plug-in.

The presentation of results is structured as follows. First, we briefly cover how each task was solved. Then present an in-depth analysis of our observations, structured by triples of \emph{observation}, \emph{hypothesis}, and \emph{implication}. Implications are directed at improving the design and usability of spatial visualizations that are embedded in an IDE. %Eventually, in \autoref{sec:discussion} we discuss observations regarding the different performance of students and professional participants that are reported in the task performance subsection just below.

\subsection{Task Performance}

\paragraph{Task 1, Domain and Collaborators} Participants used an approach best described as a ``reverse Booch method''~\cite{Booc94a}. Given a two-sentence description of the system that we've provided, they searched for nouns and verbs using Eclipse's full text search. Most participants used \Codemap to assess quantity and dispersion of search results, and also to directly select and inspect large classes. Then they looked at the class names of the matches to learn about the domain and collaborators of the system. Students also read source code, whereas professional participants limited their investigation to using the package explorer and class outline.

\paragraph{Task 2, Technologies} This task showed the most uniform behavior from both student and professional participants. They inspected the build path node and opened all included JAR libraries. Professional developers typically raised the concern that possibly not all of these libraries were (still) used and started to explore whether they were used. Typically they would carry out a search to do so, but one developer showed a very interesting pattern: He would remove the libary ``on purpose'' and then look for compile errors as an indicator of its use. Students seems to implicitly assume that all libraries were actually used, at least they never raised such a concern. We interpret this as a sign that professionals are more cautious \cite{Ko04a} and thus more aware of the typical decay caused by software evolution, which may include dead libraries.

\paragraph{Task 3, Architecture}
Typically participants drilled-down with the package explorer and read all package names. All professionals started out by formulating the hypothesis of a layered three-tier architecture, and then start fitting the packages to the different layers. Most participants used \Codemap to look at the dispersion of a package's classes (when selecting a package in the package explorer, the contained classes are highlighted on the map).

To learn about the architectural constraints, professionals, for the first time in the experiment, started reading source code. They also did so quite differently from the way that students did. Whereas students typically read code line by line, trying to understand what it does, the professionals rather used the scroll-wheel to skim over the code as it flies by on the screen, thereby looking for ``landmarks'' such as constructor calls, method signatures and field definitions. Professionals made much more use of ``open call hierarchy'' and ``open type hierarchy''. Interestingly enough, only one participant opened a type hierarchy of the whole project. 

%\nes{Here's a strange thing with your chapter. You sort of drift here into a HCI discussion that is interesting, but has little to do with your map. Of course, if this is where you take your research, you have to quote the relevant papers! Also, this line of finding sort of dominates the "real" contributions, but isn't even mentioned in the abstract or introduction.}\ewe{Was anybody navigating by ctrl-click on the types?}

\paragraph{Task 4, Feature Location}

\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{fig/codemap-userstudy2010-T-fat-arrow-1937}
\end{center}
    \caption{\emph{Screen capture of ``Aha moment'' as encountered by participant~T during task 4-b (location of security features): Upon opening the call hierarchy of \texttt{Grant}'s constructor, a huge call-arrow appeared on the map: indicating dozens of individual calls that connect the security-related archipelago in the south-west with the \texttt{TreeFactory} island in the east. Given the visual evidence of this arrow, participant~T solved the task without further investigation.}}
    \label{fig:fatarrow}
\end{figure}

For this task, participants made most frequent and more interesting use of \Codemap than for any other task. Same of for task 1, participants used a reversal of the Booch method. They searched for nouns and verbs found in the feature description. Again, they used the map to assess quantity and dispersion of search results. Also two participants used the map to select and inspect search matches based on their context in the map.

Participants now began to read more source code than before. In particular, when they found a promising search result they used the ``open call hierarchy'' feature to locate related classes. All participants reported that \Codemap flow-map overlay helped them to work with the call graph. For some developers there was an actual ``Aha moment'' where one glance at the \Codemap helped them to solve the current subtask immediately without further investigation. \autoref{fig:fatarrow} illustrates one particular moment as encountered by participant~T during the location of the security feature.

\paragraph{Task 5, Code Assesment} This set of tasks made it most obvious that \Codemap's layout was not based on package structure. Participants reported that they had a hard time to interpret the thematic maps as they could not map locations on the map to packages. In particular the professional participants expressed concerns regarding the use of KLOC for hill size. They expressed concerns that this might be misleading since lines of code is not always an indicator of importance or centrality on the system's design.

\paragraph{Task 6, Bug Fixing} Participants mainly used the same approach as for the feature location tasks. They first located the implementation of the feature in which the bug occurs, and then fixed the bug. Professional participants did so successfully, whereas student participants did not manage to find the correct location in the source code.

\paragraph{Wrap-up session} In general, participants reported that \Codemap was most useful when it displayed search results, callers, implementers, and references. A participant reported: \emph{``I found it very helpful that you get a visual cue of quantity and distribution of your search results''}. In fact, we observed that that participants rarely used the map for direct navigation but often for search and reverse engineering tasks.

Another observation was that inexperienced developers (\ie students) are more likely to find the map useful than professional developers. This might be explained by the hypothesis that to power users \emph{any} new way of using the IDE is likely to slow them down, and conversely to beginners \emph{any} way of using the IDE is  novel. The only exception to this observation was \Codemap's search bar, a one-click interface to \eclipse's native search, that was appreciated and used by all participants but one that preferred to use the search dialog.

One Participant also provided us feedback comparing his experience with \Codemap to that with the Moose analysis tool \cite{Nier05c}. He uses Moose at work after having attended a tutorial by a consultant. He said he prefers the immediate feedback of Codemap, and reported that \emph{``the gap between Moose and IDE is just too large, not to mention the struggle of importing Java code. Moose helps you to \eg find god-classes but this is typically not new to developers that know a system. Codemap seems more interesting as it integrates with what you actually do in the IDE as you program.''}

\subsection{Observations, Hypotheses, Implications}

In this section, we present an in-depth analysis of our observations, structured by triples of \emph{observation}, \emph{hypothesis}, and \emph{implication}. Implications are directed at improving the design and usability of spatial visualizations that are embedded in an IDE.
% ---- ----- ----- ----- ----- ---- ---
\paragraph{Observation 6.1: When thinking aloud, developers did speak of the system's architecture in spatial terms} 

%\begin{figure}
%\begin{center}
%  \includegraphics[draft,width=\linewidth]{fig/wrapup-map}
%\end{center}
%    \caption{\emph{Spatial map of the system, drawn by one after the participants in the the wrap-up session. All utility packages are located in the upper left corner, separated by a jagged line.}}
%    \label{fig:wrapupmap}
%\end{figure}

The think-aloud protocol revealed that participants refer to the system's architecture in spatial terms. Professional participants referred to packages as being above, below, or at the some level as one another. Some of them even did so before recovering the system's 3-tier architecture in task \#3. Most professionals referred to utility packages a being spatially beside or outside the layered architecture. 

For example, participant~T located all utility packages in the upper left corner, separated by a jagged line. While doing so, he made a gesture as if pushing the utility classes away and stated, \emph{``I am putting them up here because to me they are somehow beside the system.''}

Students on the other hand made much fewer references to the system's architecture, both spatial as well as in general. They were typically reasoning about the system at the level of classes and source lines, rather than in architectural terms. The maps drawn by students in the wrap-up phase, however, showed similar spatial structure to those of the professionals. It remains thus open whether students established a genuine spatial model while working with the code (as we observed for professionals) or only because they were asked to draw the wrap-up maps.

\paragraph{Hypothesis 6.1: Professional developers do establish a spatial mental model of the system's architecture}

Based on above observations there is evidence to assume that professional developers establish a spatial mental model of the system's architecture as they work with code. Furthermore, they do so even without visual aids, since they use spatial terms and thinking even before being asked to draw a diagram of the system's architecture.

%\ewe{A few more questions: do devs cluster the things in similar ways, but positions the clusters relatively to one other in a different way? Or is the clustering different among people? Is there a natural tendency to position clusters one way or the other, e.g. UI on the left, Data on the right, Utility at the bottom? If you can say something about that it would be great}

\paragraph{Implication 6.1: Developers should be able to arrange the layout according to their mental model}

This has implications on the design of a system's spatial visualization. Developers should be able to arrange the layout according to their mental model. Developers should be able to drag and move parts of the map around as they wish, rather than having to stick with the automatically established layout. Code Canvas \cite{Deli10a} and Code Bubbles \cite{Brag10b} both already address this implication. In those tools, the user may drag individual elements around and arrange them according to his mental model. 

We observed that developers referred to architectural components, but not classes, in spatial terms. The needs of developers might thus be even better served by providing them more high-level means of arranging the map. Our next prototype will use \emph{anchored multidimensional scaling} such that developers may initialize the map to their mental model. Anchored MDS allows the developer to define anchors which influence the layout of the map \cite[Sec 4.4]{Buja08a}. Any software artifact can be used as an anchor (as long as we can compute a its distance to artifacts on the map), even for example external libraries. In this way, developers might \eg arrange the database layer in the south and the UI layer in the north using the respective libraries as anchors.

\paragraph{Observation 6.2: Participants used Codemap to assess quantity and dispersion of search results and call graphs}

The feature of Codemap that was used most often, by both professionals and students, was the illustration of search results and call graphs. Participants reported that they liked the search-result support of the map, explaining that it gives them much faster initial feedback than Eclipse's tabular presentation of search results. Many participants reported that it was \emph{``as if you could feel the search results,''} and that \emph{``you get an immediate estimate how much was found, whether it is all one place or scattered all over the place.''}

\autoref{fig:fatarrow} illustrates one particular ``Aha moment'' as encountered by participant~T during task 4-b, \ie location of security features: Upon opening the call hierarchy, a huge call-arrow appeared on the map: indicating dozens of individual calls that connect the security-related archipelago in the south-west with the \texttt{TreeFactory} island in the east. Given the visual evidence of this arrow, the participant solved the task immediately without further investigation of the system.

\paragraph{Hypothesis 6.2: Intuitive visualization to show quantity and dispersion of search results (as well as call graphs) address an important need of developers}

%\ewe{I would maybe generalize it and say: Intuitive visualization to show the quantity and dispersion.... I could imagine that the package explorer could be enriched with a smart way to show search result. E.g. packages with many hits would be red, while packages with few hits would be gray. Or a smart algorithm that would unfold the package to show the relevant class hierarchies. Or something else}

Given the above observation it seems clear that developers have urgent needs for better representation of search results than tabular lists. We found that both students and professionals used the map to get an immediate estimation of search results. This is most interesting since otherwise their use of the tabular search results differed: Professionals glanced at the results, inspected one or maybe two results, and then either accepted or rejected their hypothesis about the system, while students would resort to a linear search through all search results, not daring to reject a hypothesis on the grounds of one or two inspected results only.

Given the map's illustration of search results however, the behavior of both groups changed. Students dared to take quick decisions from a mere glance at the map, whereas professionals were more likely to inspect several results. One professional reported that he \emph{``inspected more results than usual, because the map shows them in their context and that this helps him to take a more informed choice on which results are worth inspection and which ones not.''}

\paragraph{Implication 6.2: Tools should put search results into a meaningful context, so developers can take both quicker and better-informed decisions}

The need for better presentation of search results has implications beyond the design of spatial visualizations. Work on presentation of search results goes beyond spatial maps \cite{Hear09a}, for example results can be presented as a graph. Poshyvanyk and Marcus \cite{Posh07a} have taken one such approach (representing search results as a lattice) and applied it to source code search with promising results. 
%\ewe{Can't agree more}

For our next prototype we plan to integrate search results into the package explorer view, just as is already done with compile errors (which are, from this point of view, just like the search results of a complex query that is run to find syntax errors). This planned feature addresses another implication of our study as well, as we have found that some developers establish a spatial memory of the package explorer view. It therefore makes sense to mark search results both on our map as well as in the explorer view. 
%\ewe{Can't agree more}

\paragraph{Observation 6.3: When interacting with the map, participants were attracted to isolated elements, rather than exploring clusters of closely related elements}
 
We found that participants are more likely to inspect easily discernible elements on the map. They are more likely to notice and interact with an isolated island rather than with elements that are part of a larger continent. Unfortunately, it is exactly dense and strongly correlated clusters that contain the most interesting parts of the system! When investigating this issue, participants answered that \emph{``those (isolated) elements looked more important as they visually stick out of the rest of the map.''} 
 
Also, when working with another system that had (unlike the present study) a large cluster in the middle surrounded by archipelagos on the periphery, we found that users started their exploration with isolated hills in the periphery, only then working their way towards the more dense cluster in the middle. 

\paragraph{Hypothesis 6.3/a: Developers avoided clusters of closely relates elements because they are difficult to identify and select on the map}

All participants had difficulties to open files by clicking on the map. They had difficulties to select classes on the map when they are in a crowded cluster. They would click in the middle of a label, but often the labels are not centered, which is an unavoidable artifact of any labeling algorithm, and thus the clicks would open a different (unlabeled) class.

Codemap does provide tooltips, however participants did not use them. From observing their work it was obvious why: both students and professionals were working at such a speed that waiting for a tooltip to appear would have totally taken them out of their workflow. 

\paragraph{Observation 6.3/b: Participants rarely used Codemap to return to previously visited locations, instead using package explorer and ``Open Type'' to do so}

Contrary to our assumptions, participants did not use the map to return to previously visited locations by recalling them from spatial memory. Some would use the map, but only for exposed classes that are easily recognizable and clickable. This observation is related to the previous one.

We found however some participants established a spatial memory of the package explore\,---\,and did so \emph{in addition to their spatial model of the system's architecture!} For example, participant~S would drill down with the explorer saying ``let's open that class down there'' or ``there was this class up here.'' Over the course of the experiment he got quicker at navigating back to previously visited classes in the package explorer. Other participants, as for example participant~T, relied on lexical cues and made extensive use of Eclipse's ``Open Type'' dialog to find their way back to previously visited classes.

Usability glitches will of course worsen the effect of (or might even be the main cause of) not using the map for navigation and revisiting classes. From this it follows that:

\paragraph{Implication 6.3: The map's layout should be such that all elements are easily discernable and easy to click}

Real estate on a computer screen is limited, and even more so in an IDE with all its views and panels. As tool builders we have limited space available for an embedded visualization. Given our goal of establishing a global layout we face the challenge of having to visualize all elements of a system in that very limited space. 

The current implementation of Codemap has one level of scale only, which may yield crowded clusters where elements are placed just pixels apart. A zoomable map as provided by Code Canvas \cite{Deli10a} addresses this issue. 

The fact that we are attracted by elements that visually detach from other has two impacts: one is that we tend to look at isolated elements as being of low significance, the other being that it is hard to identify elements in the cluster. These impacts are very different, but can both be addressed in a common way.
% Both are very different, but might be consolidated.
For instance, a threshold could be used to not show isolated elements at all, but only significant clusters. Alternatively, colors may be used to display isolated elements so that they do not draw our attention so readily.

%\ewe{I'm a bit puzzled with the formulation of 6.3. The fact that we are attracted by elements that visually detach from other has two impacts: (1) one is the we tend to look at isolated element of low significance, the other (2) that it's hard to identify elements in the cluster. Both are very different. The hypothesis  is more related to (1) while hypothesis and implication relates more to (2). The argumentation should be consolidated, IMHO. For instance, a threshold could be used to not show isolated elements at all, but only significant clusters. That would address (1), but not (2). Or the usage of colors, to display isolated element so that they don't attract our look }

%\paragraph{Observation 6.4: Participants rarely used Codemap to return to previously visited locations, instead using package explorer and ``Open Type'' to do so}
% 
%\paragraph{Hypothesis 6.4:  Developers failed to establish a spatial memory of the map not due to its layout but also due to missing visual cues}
%
%In general, the issue of usability is orthogonal to the map's layout. For example, offering ``search as you type'' might help to raise map interaction for those developers that mainly rely on lexical cues, no matter which base layout is used. It was our impression that any exploratory user study of embedded software visualization will be dominated just as by usability issues as by technical factors, such as your choice of layout algorithm. \ewe{I don't get this one}
%
%\paragraph{Implication 6.4: It should be possible to bookmark classes as ``top landmarks,'' both manually and automatically based on usage statistics}
% 
%What might help as well to ease the retrieval of previously visited classes is a ``top landmarks'' feature where you can manually (but also automatically based on visits) set markers on the map as starting points for further activities. We plan to work on this for our next prototype. \ewe{The apparent ability to remember position in the package explorer but not in the map may be due to other aspects. I'm wondering for instance if providing a grid in the map with row A-F, and column 0-16 would help to remember the position in the map. Maybe yes, maybe not. In the real-world analogy, maps have latitude and longitude. With never remember them, but help us to slice the map. We maybe need some rigidity to be able to anchor position in our mental model. The package explorer is too rigid, and the map is to flexible/free.} \ewe{All that to say that I don't quite get the link form observation 6.4 to implication 6.4}
%
%\ewe{Actually, I'm wondering if dropping 6.4 altogether wouldn't make the argumentation stronger. To me the main points are 6.1, 6.2, 6.3, 6.5}.

\paragraph{Observation 6.5: Participants used Codemap as if its layout were based on package structure\,---\,even though they were aware of the underlying topic-based layout}

Developers assume that packages are a valid decomposition of the system and expect that the layout of the spatial visualization corresponds to the package structure. We found that clustering classes by topic rather than packages violates the ``principle of least surprise.'' We observed that participants tended to interpret visual distance as a measure of structural dependencies\,---\,even though they were aware of the underlying lexical implementation!
 
Participants expected the layout to reflect at least some structural property. Most of them reacted surprised or confused when for example the classes of a package were not mostly in the same place. For example, Participant~S reported in the wrap-up, \emph{``this is a very useful tool but the layout does not make sense".} Another participant stated during task 3 (\ie the architecture recovery) with confusion that \emph{``the classes contained in packages are scattered on the map, it is not obvious what their spatial connection is.''}
 
\paragraph{Hypothesis 6.5: From the developers view, the predominant mental decomposition of a system is package structure}

Given our reverse engineering background  \cite{Nier05c,Kuhn07a,Duca09c} we had come to distrust package decomposition, however it seems that developers like to rely on the packaging that other developers have made when designing the system.

One problem raised by research in re-packaging legacy systems is that packages play too many roles: as distribution units, as units of namespacing, as  working sets, as topics, as unit of architectural components, etc. However, as an opposing point of view, we can relate packaging to the folksonomies of the Web 2.0, where users label elements with unstructured tags that are then exploited by other users to search for elements. In the same way, we could say that putting trust into a given package structure is a way of collaborative filtering. Developers assume that other developers had made the same choice as they would when packaging the system. 

\paragraph{Implication 6.5: The map layout should be based on code structure rather than latent topics only. However, non-structural data should be used to enrich the layout}

When running the user study, it became quickly apparent that we should revise our initial assumption that lexical similarity is a valid dissimilarity metric for the spatial layout. This was the strongest feedback, and as is often the case in exploratory user studies, already obvious from watching the first professional participant for five minutes only. From all participants we got the feedback that they expect the layout to be structural and that our clustering by topics kept surprising them even after working with the map for almost two hours.
 
Still we think that spatial layouts that go beyond package structure are worthwhile. Therefore, we propose to enrich structure-based layout with non-structural data, such as design flaws. For future work, we are about to refine our layout algorithm based on that conclusion. The new layout is based on both lexical similarity and the ideal structural proximity proposed by the ``Law of Demeter'' (LOD). This is a design guideline that states that each method should only talk to its friends, which are defined as its class's fields, its local variables and its method arguments. Based on this we can defined an {\itshape idealized} call-based distance between software artifacts. Given a LOD-based layout, software artifacts are close to one another if they are supposed to call one another and far apart if they better should not call one another. Thus we get the desired property that visualizing call-graphs conveys meaningful arrow distances. On a LOD-based map, any long-distance-call has a diagnostic interpretation that helps developers to take actions: Long flow-map arrows indicate calls that possibly violate the ``Law of Demeter''.

%\begin{table*}
%\begin{center}
%  \includegraphics[width=\linewidth]{fig/codemap2010-userstudy}
%\end{center}
%    \caption{\emph{Collected data of three selected participants. From top to bottom, categories are: success rate, perceived difficultly, perceived usefulness of the Codemap plug-in, and observed use of IDE features (a single cross indicates use of that feature, and a double cross indicates that the given feature was used the most).}}
%    \label{fig:table}
%\end{table*}

%=========================================================
\section{Threats to Validity}
\label{sec:discussion}

This section summarizes threats to validity. The study had a small sample size (3 students, 4 professionals) and might thus not be representative. We manually evaluated the data, results might thus be biased. Nevertheless, results are promising and running a pilot think-aloud study with a small user group is a state-of-the-art technique in usability engineering to learn learn about the reactions of users. Such pilot studies are typically used as feedback for further iteration of the tool and to assess the usefulness of its application \cite{Niel03a}.

In this section, we discuss observations regarding the different performance of students and professional participants that are reported in the task performance subsection just below. We found no correlation between codemap interaction and success rates, rather success was completely dominated by the participant's programming experience. Professional participants succeeded in all tasks, where as students failed more than half of all tasks. 
%
Professional developers are very focused, \chg{the} they mainly used keyboard shortcuts only and \chg{goes} go very fast, he seems to ignore not only the map but most parts of the IDE as he goes along, our impression is that he is very focused and always knows exactly where to look next. Professional developers only used a limited set of IDE means to achieve a task, whereas students would typically use all available means (of which they know of, we found that some features where unknown to the students, whereas on the other hand we learned new usages of Eclipse from the professionals. This is a serious challenge for researches, imagine for example that you'd ask your control group to solve a taks in Excel but you are unaware of for example \chg{the} of the Pivot table feature and thus etc).
%
Professional developers would formulate hypotheses first and very quickly and either accepts or rejects them with typically looking at one or two search results only, almost all their conclusions are correct but if they are not they are just as quick to abandon them and move on to the next hypothesis. Where as students, when facing an incorrect hypothesis would stick with it and turn to exhaustive linear search through the system looking for the missing evidence. Similar but not as critical they would often look for more than one prove of a correct hypothesis. \cite{HolmesorFritz}
%
As most researchers in computer science do have a back ground as software engineers, we are often tempted to assume that we proficient software engineers ourselves. Watching the think-aloud footage of the professional participants has though as a better, their use of the IDE showed a proficiency and efficiency that was magnitudes beyond ours and challenged many of our assumptions regarding the use of software maps in an IDE. 
%
It is a known problem to recruit professional participants for user studies, but given our experience with the think-aloud protocol we think it is safe to assume that a study with even few professionals has more predictive power than any study with dozens of students. The needs of seasoned developers are just so different from those of students that still struggling to learn their profession. 
%
One of the most striking difference between students and professionals was that a professional would never do any click or action in the IDE without having a clearly stated hypothesis of what is going on, while students soon turned into repeated ``linear search'' patterns when facing problemens they could not understand. 
%
On the other hand, the students shows a much better adoption rate of the software visualization the the professionals. That was not unexpected, since to power users any new way of using the IDE is likely to slow them down, and conversely to beginners any way of using the IDE is  novel. This raises the question of incremental versus revolutionary improvements: is it possible to prove the usefulness of a revolutionary improvement in a user study? Clearly, for well-defined incremental improvements we can measure the benefit with controlled and repeatable experiments. However, some revolutionary improvements are likely to be rejected by professionals for being not enough in line with their habits but as well as are likely to show no measurable improvement for students, since students are still struggling to learn their profession rather than being able to perform controlled tasks.
%
%\ewe{Need to be proof-read}

%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%

\chapter{User Study on API Learning}
\label{the chapter on the MSR user study}

Modern software development requires a large investment in learning application programming interfaces (APIs).
%
Recent research found that the learning materials themselves are often inadequate: developers struggle to find answers beyond simple usage scenarios.
%
Solving these problems requires a large investment in tool and search engine development.
%
To understand where further investment would be the most useful, we ran a study with 19 professional developers to understand what a solution might look like, free of technical constraints. In this chapter, we report on design implications of tools for API learning, grounded in the reality of the professional developers themselves. 
The reoccurring themes in the participants' feedback were trustworthiness, confidentiality, information overload and the need for code examples as first-class documentation artifacts.

Modern software development requires a large investment in learning application programming interfaces (APIs), which allows developers to reuse existing components. API learning is a continuous process. Even when a developer makes a large initial investment in learning the API, 
for example, by reading books or going through online tutorials, 
the developer will continue to consume online material 
about the API 
throughout the development process. These materials include reference documentation from the API provider, sample code, blog posts, and forum questions and answers. 

Indeed, seeking online API information has become such a pervasive part of modern programming that emerging research tools blend the experiences of the browser and the development environment. For example, Codetrail automatically links source code and the web pages viewed while writing the code \cite{gm09}. Blueprint allows a developer to launch a web query from the development environment and incorporate code examples from the resulting web pages \cite{bdwk10}. While these new tools help reduce the cost of (re)finding relevant pages and incorporating information from them, this covers only a portion of developers' frustrations. 
%
In a recent study of API learning obstacles among professional developers, Robillard found that the learning materials themselves are often inadequate~\cite{robillard09}. Bajracharya and Lopes analysed a year's worth of search queries and found that current code search engines address only a subset of developers needs~\cite{Bajracharya2009a}.   
For example, developers struggled to find code examples beyond simple usage scenarios, to understand which parts of an API support which programming tasks, and to infer the intent behind the API's design. 
Solving these systematic problems requires a large investment, either in the API provider's official documentation, the API users' community-based documentation, or in the search engines that unite the two \cite{bdwk10,Hoffmann2007a}. 
%Where should such an investment be made? Should API providers create more extensive documentation on complex usage scenarios and design intent? Should the API users push for mass organization of content based on tagging or wikis? Should search engines create more sophisticated indices of API web pages, e.g. by parsing embedded code snippets? 
Any of these changes is difficult and expensive.

% NOTE: I originally anonymized "Microsoft" and "Silverlight" but it seemed a bit silly.

To understand where further investment would be the most useful, we ran a study with 19 professional developers from Microsoft Corporation, with the goal of understanding what an ``ideal'' solution might look like, free from technical constraints. We invited randomly chosen members of a corporate email list of  Silverlight users to participate in one-hour sessions for small gratuities. Silverlight is a large API for creating web applications, with hundreds of classes for data persistance, data presentation, and multimedia. All participants were male with an average of $12.2$ years of professional experience. 

Borrowing from participatory design, we asked the participants to act as our partners in designing a new user experience for learning Silverlight. We ran two types of sessions.  In the first, we interviewed participants to learn their common learning materials and  most challenging learning tasks and then asked them to sketch a design for a new learning portal. We compiled these ideas into five exploratory designs. In the second type of session, we ran focus groups to get feedback on our descriptions of their learning tasks and the five designs. 

This chapter's main contributions are a compilation of design implications for API learning tools, grounded in the reality of the professional developers themselves. We report on the recurring themes in the participants' feedback: trustworthiness, confidentiality, information overload and the need for code examples as first-class documentation artifacts.

\begin{figure*}
    \includegraphics[width=\linewidth]{fig/api-learning-five-design}
    \caption{The five solution designs as narrated to the participants (from left to right): \ZoomableUML, \ConceptMap, \FacettedSearch, and on the last panel \RichIntellisense~(above) and \CloudREPL~(below). Pen color has been used to distinguish the designs (green) from the participant's input (red). The stick notes are the participant's votes.}
    \label{fivedesigns}
\end{figure*} 

\moarsauce
\section{First Study: Current Practice}

In the first type of study session, we met singly with nine participants, and ran each through three activities. First, we asked the participant to describe all the  materials he used for learning Silverlight, as we recorded them on the whiteboard. Next, we asked him to consider this as a set of ``ingredients'' and to sketch a design for a learning portal that presents some or all of these ingredients to help developers learn Silverlight. Finally, we asked him to review the design by comparing the experience of learning Silverlight by using the design versus his own experience learning Silverlight.

\moarsauce
\subsection{Learning Sources}
We asked the participant to describe all the  materials he used for learning Silverlight, as we recorded them on the whiteboard. Some of the learning sources are obvious and readily reported by participants, such as books and web search. To learn about non-obvious learning sources, we asked developers ``did you ever find an answer to a technological question that is not listed here,'' which led to answers like reverse engineering or social networking. Their reported learning sources are the following:

\begin{description}
\item[``Off the top of my head''] is by far the most common way developers find answers on the job. Most participants reported that they set aside dedicated time for learning. Typical off-the-job learning sources are: lurking on mailing lists and forums, watching videos and reading books. Most knowledge however is based on experience and acquired through learning-by-doing on the job. One participant refers to this a \emph{``growing your own folklore.''}

\item[Web search] was reported by all participants as the first place to go when they have an information need. Among the search results participants are typically looking for are: blog posts, discussion forums, official reference documentation, mailing list archives, bug reports and source repositories (listed in order of typical access patterns). 
Participants prefer results with code examples over results without code examples, which is supported by existing research on API learning barriers \cite{robillard09}.

\item[Intellisense] (\ie auto-completion of identifiers) was reported as a tool for the discovery of unknown APIs by all developers. One participant called this \emph{``digging through the namespaces.''} Discovering unknown APIs is an appropriation of auto-completion, originally conceived to help recall names from familiar APIs. 

\item[Prototyping,] reverse engineering and many more forms of tinkering were reported by all participants as a last resort when all above sources failed to provide an answer. 
Some participants even resort to reverse engineering when documentation is available, as they prefer source code over natural language documentation. 
Developers typically use prototyping both as an explorative tool and to verify hypotheses about the APIs. All participants reported that having to \emph{``get your hands dirty''} is an integral part of their learning experience. 

\item[Asking another person] was reported by most participants as a last resort. Developers follow a ``due diligence'' process before asking another person for an answer. It is important to them to have put enough personal effort into finding an answer before asking on a mailing list or reaching out to a person from their social network. 
Also, they reported to prefer immediate results, such as those provided by web search, over waiting for the reply of asynchronous communication such as email and discussion forums.

\end{description}

These findings are consistent with Robillard's study of learning obstacles~\cite{robillard09}, but provide a more complete catalog of learning materials. Both studies found that developers strive to stay within the programming patterns and use cases that the API provider intends (even when that intent is undocumented) and that developers typically lack documentation when using an API for a less common task. Somewhat surprisingly, we found that developers prefer the community-based learning materials on the web, like blogs, forum posts, and tutorials, over more ``authoritative'' learning material, like books and reference documentation. Developers also prefer active but potentially time-consuming information seeking,~like iterative web search and reverse engineering, to waiting on answers from others, because they perceive the answers as more immediate.

\moarsauce
\subsection{Learning Categories}

% renamed this to learning categories, since "task categories" might be confused by reviewers with tasks in a suer study (at least Niko was clearly confused when reading this draft!) 

Based on the design sketches that participants produced, we elicited three broad categories of learning tasks:

\begin{description}
\item[Technology selection] is about learning about an API's fundamental capabilities (\emph{``Can Silverlight play video in this codec?''}) and about comparing capabilities (\emph{``Is DirectX or Silverlight better for my game?''}). Sometimes the selection decision is about growing skills rather than project requirements. 

\item[Mapping task to code] includes both discovery of unfamiliar APIs as well as remembering relevant identifier names in previously learned APIs. Getting an answer to this type of questions typically falls in two phases. Initially developers search based on natural language task descriptions (e.g. \emph{``how to implement a zoomable canvas''}) and skim through many search results to stumble on relevant identifier names. Once they have a concrete identifier, their search behavior becomes more focused and may be as simple as looking up the identifier's reference documentation.

\item[Going from code to better code] is a major concern of professional developers. All participants reported that they spend considerable effort getting answers to performance questions. Other use cases are robustness and idiomatic, \ie intended, use of an API, in particular with regard to breaking changes of newly released API versions or different target platforms.

\end{description}

The kind of learning categories impacts the preferred learning strategies of developers. For technology selection, participants sometimes use web search to learn about available technologies, but eventually prefer personal recommendations from their social network.  For mapping task to code, participants strongly prefer search results with code examples over such without code examples. For getting to better code however, such as troubleshooting a performance problem, participants prefer solving the problem themselves (including reverse engineering) but sometimes ask others to double-check the answers they find.

\moarsauce
\section{Second Study: Solution Elements}

For the second type of session, we compiled the user feedback from the first sessions into five exploratory designs. We ran 10 participants in three focus groups (with three, three, and four members) and asked them to provide feedback on the five designs. In each session, we drew each design on its own whiteboard and encouraged participants to ask questions, provide feedback, and to add their own ideas as we explained the design. 

Figure~\ref{fivedesigns} shows a photograph of the whiteboards with the five designs, taken at the end of a focus group's session. In the following the designs a described on the order they were presented to the participants in that session:

\moarsauce
\paragraph{Design: \ZoomableUML} 
This design draws from the spatial software representation of CodeCanvas~\cite{Deline2010a} and addresses answering complex reachability questions~\cite{Latoza2010a} as you code. The design extends the IDE with a zoomable UML diagram. The diagram opens zoomed on locally reachable types of the API and shows their dependencies and interaction. The user can zoom out to get a larger picture of the API, up to the level of namespaces.

\moarsauce
\paragraph{Design: \ConceptMap} 
The API is presented as a zoomable map, organized around programming domain concepts (e.g. ``controls'', ``media content''). As the user zooms in, the concepts become more refined (e.g. ``streaming video''). At the lowest zoom level, the map shows web-based content about that concept, including blogs, forum posts, tutorials, and the people who author these.
The map is searchable and keeps track of user interaction as well as the user's learning progress. Users can bookmark locations and share their bookmarks. Documentation editors can use the same feature to share tutorials as ``sight-seeing tours.''

\moarsauce
\paragraph{Design: \FacettedSearch}
This design unifies web search and asking people questions. The user types a question into a textbox. As she types, related  search results are pulled in from various sources (web sites, bug reports, code examples, mailing list archives, etc). Search results are grouped by facets, such as type of sources, type of content or semantic concepts. Besides the results, a tag cloud appears with extracted identifier names. Search results are summarized using code examples, if possible. In addition, the results include suggested people and mailing lists that are experts on the topic of the questions, to which the question can be posted.

\moarsauce
\paragraph{Design: \RichIntellisense} 
This design extends auto-completion of identifiers with search results that are automatically pulled from the world wide web. The results are ``localized'' to the current context of the IDE, such as imported libraries and reachable types \cite{Holmes2005}. Results are shown in the same pop-up windows as the auto-completion suggestions. If possible search results are displayed as code examples, ready for incorporating into the code, as in Brandt et al \cite{bdwk10}.

\moarsauce
\paragraph{Design: \CloudREPL} 
This design attaches an execution context to code examples on the web. Code examples include hidden meta-information with all context that is required to execute. Examples are editable, debuggable and can be executed live in the browser. With a single click, users can download examples into their IDEs. Similarly, users can upload the code in their IDE as runnable examples on the web, for inclusion in blogs or discussion forums.

\moarsauce
\section{Feedback}

After we explained all five designs, we then handed each participant a pen and sticky notes and gave them 10 minutes to annotate the designs, either with a blank sticky note to mean ``I like this part'' or with their own comments (typical ones were smiley faces, frowny faces, ``NO'', etc). 

The votes are summarized in Table~\ref{thetable}: the most popular design are ``\FacettedSearch`` and for learning activities the ``\ConceptMap`` design. Participants downvoted the ``\ZoomableUML'' and ``\RichIntellisense'' due to concerns about information overload, the same happened with ``\CloudREPL`` due to concerns about missing confidentiality.

\begin{table}[h]
\begin{center}
\begin{tabular}{l|ll}
\textbf{Design} & Up-Votes & Down-Votes \\
\hline
\ZoomableUML & $\star\star\star\star~$ & $\ast\ast\ast\ast\ast~$ \\
\ConceptMap & $\star\star\star\star\star\star~$ & $\ast\ast\ast~$ \\
\FacettedSearch & $\star\star\star\star\star\star\star\star\star~$ & $\ast~$ \\
\RichIntellisense & $\star\star\star~$ & $\ast\ast\ast\ast\ast\ast\ast~$ \\
\CloudREPL & $\star\star\star~$ & $\ast~$
\end{tabular}
\end{center}
\caption{At the end of the second type of sessions, participants voted with sticky notes for the designs. \FacettedSearch{} received the most up votes, \RichIntellisense{} the most down votes.}
\label{thetable}
\end{table}%

There were several recurring themes in our participants' feedback which cut across the various designs. The four top most recurring themes are discussed and summarized as design implications for tool builders in the following:

\moarsauce
\subsection{Code Examples}

We got very positive feedback on the emphasis on code examples and identifier names in the ``\FacettedSearch'' design. Participants prefer results with code examples over results without code examples, which is supported by existing research on API learning barriers \cite{robillard09}. When mapping a task to code, developers typically use web search and linearly go through all results until they find one with a code example or an identifier; often repeating this process a dozen times until they find a working answer. Participants liked about the facetted search design that it extracts code examples and identifiers from top search results. One participant even said that the summary tag cloud with identifiers, by itself, would be reason to use it.

\emph{Implication for tool builders:} Developers need the heterogeneous learning materials that web search provides, but want it to be more targeted and organized. Search engines for API learning should extract code examples and identifiers found in natural text documents, and present them to the developers in a more accessible way. This implication is supported related work on code examples \cite{bdwk10,Hoffmann2007a,Holmes2005}.

\moarsauce
\subsection{Credibility}

Credibility of web sources appeared as a major concern with all designs that included content taken from the web. For the participants, credibility is mostly a function of where the information comes from. For example, participants reported that search results from blogs are often more relevant, but typically less credible than official reference documentation. They also rely on the social reputation of its source rather than technical factors, which supports existing research \cite{Gysin2010a}. In particular with the ``\FacettedSearch'' design, which automatically summarizes search results, participants emphasized the importance of seeing the information source to judge credibility.  

\emph{Implication for tool builders:} Tools should show both credibility and relevance when presenting search results, such that the developers can make an informed decision when using API information and code examples from the web. To asses the credibility of API information tools should prefer social factors, such as the credibility of the information's author, rather than technical statistics, such as code metrics.

\moarsauce
\subsection{Confidentiality}

Confidentiality appeared as a major concern with all designs that share local information with a global audience. In particular with the ``\CloudREPL'' design, which publishes an example's execution context on the web, participants were concerned with leaking proprietary information, like the use of certain libraries. One participant was also concerned that publicly inquiring about technologies could accidentally reveal business strategies.

\emph{Implication for tool builders:} When automatically sharing local information with the web, tools must be careful about protecting proprietary information, such as not showing confidential code, nor libraries being used. Tools should give developers full control over shared information, for example by letting them review the list of automatically included terms before issuing the search query. Or alternatively, only sharing information that is on a user controlled white list.

\moarsauce
\subsection{Information Overload} 

Information overload was the major reason why participants rejected the ``\ZoomableUML'' and the ``\RichIntellisense'' designs. We got strong feedback that pulling more information into the IDE is not welcome unless it is highly task- and context specific information. Participants were also concerned that adding more features to Intellisense's popup will use too much screen real estate and slow down the IDE. 

\emph{Implication for tool builders:} Any tool that pulls additional information into the IDE must be highly selective and should only show information that is specific to the developer's current task and context. The ability to further filter down the information is crucial, as well as not slowing down the IDE and using screen real estate sparingly.

%\subsection{Visual Representation}

%\todo{Maybe some words on the visual representation of code, ie about the participants  doubt that visual languages such as UML are any better than tree views. And also about the difficulty to represent learning material on a visual map?}

%\subsection{Sharing of Answers}

%\todo{Maybe some words on social media and the participants strong division in two groups with regard to that. But also that we found that participants tend take question to a less visible channel when helping to answer them, like taking them off the mailinglist (in order to avoid other attendees information overload) but then never share the final answer to the initial audience. And that participants said that they would share content, such as their bookmarks, on the conceptual map.}

\moarsauce
\subsection{Threats to validity}

We selected all participants from the same corporation, whose common hiring practices and corporate culture may bias the results. In particular, the participants all work for the same company that produces the Silverlight API, which gives the participants unique access to the API creators. 
Therefore, the participants may not be representative of all professional developers. 
Nonetheless, participants mostly accessed public learning outside the company and many expressed hesitation about asking questions of fellow employees for fear of harming their reputation. 
The study is also based on a single API. While this choice allowed us to compare participants' experiences and gave them common ground during the focus groups, there may be issues in learning Silverlight that do not generalize to other APIs. 

\moarsauce
\section{Conclusion}

% Split by task type?

%\todo{Typically there should be no new ideas in a conclusion, so maybe should move all new ideas from here to a discussion part... or just rename this section. Folksonomies for example are not discussed before, and it would need discussion to motivate why they are an alternative to expensive tool building, so maybe this should be highlighted/hinted at through out the chapter. Just the same for the duality of ``heterogeneous learning materials from the web,'' I already tried to do this but we could emphasize this more and make it a central them, like: this is very useful information but credibility and confidentality are crucial when reaching our from local to global sources / audiences. Maybe also we can add some words in the introduction that the heterogenous global content was on available like 10 years ago and that this an all new and exciting situation and thus called for that research to learn about how developers use content on the web to find answers about API questions.}

Web search is the predominant form of information seeking, but in many cases is frustrating and error-prone. Developers need the heterogeneous learning materials that web search provides, but want it to be more targeted and organized. 
Therefore, API learning tools that bring web search and development environments closer together 
	1)~should leverage examples and identifiers found in natural text documents as first-class results,
	2)~should communicate the credibility of aggregated results,
	3)~most not shared confidential information without the user's consent,
	and 4)~should filter search results by task and context to avoid information overload. 

%Doing this requires additional metadata per web page, such as the API name, version, target environment, fully qualified names of the members in code snippets, etc. The most cost-effective way to do this is by having a search engine infer this information per page. However, without this, the API user community could take a folkonomy approach.


